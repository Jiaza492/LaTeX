\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Artificial Intelligence Note}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\maketitle

\setlength{\parindent}{0in}

\section*{Intelligent Agents}

1. An {\bf agent} is anything that can be viewed as perceiving environment throught sensors and acting upon that evnironment throught actuators. \\

2. Property of task environments

\begin{itemize}
  \item Fully observable vs. partially observable
  \item Signle angent vs. multiagent
  \item Deterministic vs. stochastic
  \item Episodic vs. sequential
  \item Static vs. dynamic
  \item Discrete vs. continuous
  \item Known vs. unknown
\end{itemize}

\section*{Solving Problems by Searching}

1. Uniformed search strategies

\begin{itemize}
  \item Breadth-first search
  \item Uniform-cost search ($g(n)$)
  \item Depth-first search
  \item Depth-limited search
  \item Iterative deepening depth-first search
  \item Bidirectional search
\end{itemize}

2. Infromed (heuristic) search strategies

\begin{itemize}
  \item Greedy best-first search ($h(n)$)
  \item $A^*$ search

    Admissibility: An admissible heuristic is one that never overestimates the cost to reach the goal.

    Consistency (monotonicity): $h(n) \le c(n,a,n') + h(n')$.

    * Every consistent heuristic is also admissible.
  \item Iterative-deepening $A^*$
  \item Recursive best-first search (RBFS)

    backed-up value: the best $f$-value of its children
\end{itemize}

\section*{Beyond Classical Search}

1. Local search algorithms \\

Local search algorithms operate using a single current node (rather than multiple paths) and generally move only to neighbors of that node. \\

Local search algorithms are suitable for problems in which all that matters is the solution state, not the path cost to reach it. \\

A complete local search algorithm always finds a goal if one exists; an optimal algorithm always finds a global minimum/maximum.

\begin{itemize} 
  \item Hill-climbing (greedy local search)

    Stuck reasons: local maxima, ridges, plateaux.

    Alternatives: stochastic hill climbing, first-choice hill climbing, random restart hill climbing.
  \item Simulated annealing
  \item Local beam search

    In a local beam search, useful information is passed among the parallel search threads.

    Alternatives: stochastic beam search
  \item Genetic algorithms
\end{itemize}

2. Searching with nondeterministic actions \\

AND-OR search trees \\

OR nodes: In a deterministic environment, the only branching is introduced by the agent's own choices in each state. \\

AND nodes: In a nondeterministic environment, branching is also introduced by the environment's choice of outcome for each action.

\section*{Adversarial Search}

1. Zero-sum (constant-sum) games: a game where the total payoff to all players is the same for every instance of the game. \\

2. The minimax algorithm \\

The minimax algorithm performs a complete depth-first exploration of the game tree. \\

3. Alpha-beta pruning \\

$\alpha =$ the value of the best (i.e., highest-value) choice we have found so far at any choice point along the path for MAX. \\

$\beta =$ the value of the best (i.e., lowest-value) choice we have found so far at any choice point along the path for MIN. \\

4. Imperfect real-time decisions: cut-off search

\section*{Logical Agents}

1. Knowledge base \\

A knowledge base is a set of sentences. Each sentence is expressed in a language called a knowledge representation language and represents some assertion about the world. A knowledge base, KB, may initially contain some background knowledge. \\

2. Entailment: a sentence follows logically from another sentence.

\begin{equation*}
  \alpha \models \beta \text{ if and only if } M(\alpha) \subseteq M(\beta)
\end{equation*}

(Note the direction of $\subseteq$ here: if $\alpha \models \beta$, then $\alpha$ is a stronger assertion than $\beta$: it rules out more possible worlds.) \\

Sound: an inference algorithm that derives only entailed sentences is called sound or truth-preserving. \\

Complete: an inference algorithm is complete if it can derive any sentence that is entailed. \\

3. Deduction theorem
\begin{eqnarray*}
  \alpha &\models& \beta \text{ if and only if the sentence } (\alpha \Rightarrow \beta) \text{ is valid.} \\
  \alpha &\models& \beta \text{ if and only if the sentence } (\alpha \lor \lnot \beta) \text{ is unsatisfiable.}
\end{eqnarray*}

4. Modus Ponens

\begin{equation*}
  \frac {\alpha \Rightarrow \beta, \text{   } \alpha}{\beta}
\end{equation*}

5. And-Elimination

\begin{equation*}
  \frac {\alpha \land \beta}{\alpha} 
\end{equation*}

6. Unit resolution inference rule

\begin{equation*}
  \frac {\ell_1 \lor \cdots \lor \ell_k, \text{   } m}
        {\ell_1 \lor \cdots \lor \ell_{i-1} \lor \ell_{i+1} \lor \cdots \lor \ell_k}
\end{equation*}

where each $\ell$ is a literal and $\ell_i$ and $m$ are complementary literals. \\

7. Conjunctive normal form \\

(Disjunct) conjunct (Disjunct) \\

8. Ground resolution theorem \\

If a set of clauses is unsatisfiable, then the resolution closure of those clauses contains the empty clause. \\

9. Horn clause and definite clauses \\

Definite clauses: a disjunction of literals of which exactly one is positive. For example, the clause $(\lnot L_{1,1} \lor \lnot Breeze \lor B_{1,1})$. \\

Horn clause: a disjunction of literals of which at most one is positive. \\

Horn clauses are closed under resolution: if you resolve two Horn clauses, you get back a Horn clause.

\section*{First-order Logic}

1. Symbols and interpretations

\begin{itemize}
  \item Terms

    A term is a logical expression that refers to an object. For example,
    \begin{equation*}
      LeftLeg(John)
    \end{equation*}

    A term with no variables is called a ground term.
  \item Atomic sentences

    An atomic sentence is formed from a predicate symbol optionally followed by a parenthesized list of terms, such as
    \begin{equation*}
      Brother(Richard, John)
    \end{equation*}
  \item Complex sentences

    We can use logical connectives to construct more complex sentences, with the same syntax and semantics as in proportional calculus.
  \item Quantifiers
    \begin{itemize}
      \item Universal quantification ($\forall$)

        Common mistake: Use conjunction ($land$) instead of implication ($\Rightarrow$) with $\forall$ will lead to an overly strong statement.
      \item Existential quantification ($\exists$)

        Common mistake: Use implication ($\Rightarrow$) instead of conjunction ($\land$) with $\exists$ will lead to a very weak statement.
    \end{itemize}
  \item Equality
    \begin{equation*}
      Father(John) = Henry
    \end{equation*}
\end{itemize}

2. Universal instantation (UI) \\

Every instantiation of a univerally quantified sentence is entailed by it:

\begin{equation*}
  \frac {\forall v \alpha}{Subst(\{v/g\}, \alpha)}
\end{equation*}

for any variable $v$ and ground term $g$. \\

3. Exisential instantation (EI) \\

For any sentence $\alpha$, variable $v$, and constant symbol $k$ that does not appear elsewhere in the knowledge base:

\begin{equation*}
  \frac {\exists v \alpha}{Subst(\{v/k\}, \alpha)}
\end{equation*}

E.g., $\exists x Crown(x) \land OnHead(x, John)$ yields:

\begin{equation*}
  Crown(C_1) \land OnHead(C_1, John)
\end{equation*}

provided $C_1$ is a new constant symbol, called a Skolem constant.

\section*{Quantifying Uncertainty}

1. Trying to use logic to cope with a domain like medical diagnosis fails for three main reasons: laziness, theoretical ignorance, practical ignorance. \\

2. Decision theory = probability theory + utility theory \\

3. Maximum expected utility (MEU): An agent is rational if and only if it chooses the action that yields the highest expected utility, averaged over all the possible outcomes of the action. \\

4. Sample space: the set of all possible worlds (possible worlds are mutally exclusive). \\

5. Normalization

\begin{equation*}
\boldsymbol{P}(X|\boldsymbol{e}) =
\alpha \boldsymbol{P}(X, \boldsymbol{e}) = \alpha \sum_y \boldsymbol{P}(X, \boldsymbol{e}, \boldsymbol{y})
\end{equation*}

6. Independence

\begin{equation*}
P(X,Y) = P(X)P(Y)
\end{equation*}

7. Bayes' rule

\begin{equation*}
P(b|a) = \frac {P(a|b)P(b)}{P(a)}
\end{equation*}

8. Naive Bayes model

\begin{equation*}
P(Cause, Effect_1,..., Effect_n) =
P(Cause) \prod_iP(Effect_i|Cause)
\end{equation*}

\section*{Probabilistic Reasoning}

1. CPT: conditional probability table. \\

2. Representation of conditional distribution

\begin{itemize}
  \item Probit distribution
    \begin{eqnarray*}
      \Phi(x) &=& \int^x_{-\infty} N(0,1)(x)dx \\
      P(buys | Cost = c) &=& \Phi((-c + \mu) / \sigma)
    \end{eqnarray*}
  \item Logit distribution
    \begin{eqnarray*}
      P(buys | Cost = c) &=&
      \frac {1}{1 + \exp(-2 \frac {-c + \mu}{\sigma})}
    \end{eqnarray*}
\end{itemize}

The two distributions look similar, but the logit distribution actually has much longer "tails''. The probit is often better fit to real situations, but the logit is sometimes easier to deal with mathematically. It is widely used in neural networks. \\

3. Representing the full joint distribution

\begin{equation*}
P(x_1,...,x_n) = \prod^n_{i=1} \theta(x_i | parents(X_i))
\end{equation*}

4. The chain rule

\begin{equation*}
P(x_1,...,x_n) = \prod^n_{i=1} P(x_i | x_{i-1},...,x_1)
\end{equation*}

5. Locally structured (sparse) system \\

In a locally structured system, each subcomponent interacts directly with only a bounded number of other components, regardless of the total number of components. Local structure is usually associated with linear rather than exponential growth in complexity. \\

6. Inference by enumeration (p523)
\begin{eqnarray*}
\boldsymbol{P}(B|j,m)
&=& \alpha \boldsymbol{P}(B,j,m)
= \alpha \sum_e \sum_a \boldsymbol{P}(B,j,m,e,a) \\
P(b|j,m)
&=& \alpha \sum_e \sum_a P(b) P(e) P(a|b,e) P(j|a) P(m|a) \\
P(b|j,m)
&=& \alpha P(b) \sum_e P(e) \sum_a P(a|b,e) P(j|a) P(m|a)
\end{eqnarray*}

7. The complexity of exact inference \\

Singly connected netowrks (polytrees): The time and space complexity of exact inference in polytrees is linear in the size of the network. \\

Multiply connected networks: \#P-hard. \\

8. Direct sampling methods \\

Suppose there are $N$ total samples, and let $N_{PS}(x_1,...,x_n)$ be the number of times the specific event $x_1,...,x_n$ occurs in the set of samples. Then
\begin{equation*}
  \lim_{N\rightarrow \infty} \frac {N_{PS}(x_1,...,x_n)}{N}
  = S_{PS}(x_1,...,x_n) = P(x_1,...,x_n)
\end{equation*}

Whenever we use an approximate equality (``$approx$") in what follows, we mean it in exactly this sense--that the estimated probability becomes exact in the large-sample limit. Such an estimate is called consistent.
\begin{equation*}
  P(x_1,...,x_m) \approx N_{PS}(x_1,...,x_m)/N
\end{equation*}

9. Rejection sampling \\

Rejection sampling is a general method for producing samples from a hard-to-sample distribution given an easy-to-sample distribution.
\begin{eqnarray*}
  \hat{\boldsymbol{P}}(X|\boldsymbol{e})
  &=& \alpha \boldsymbol{N}_{PS}(X, \boldsymbol{e})
  = \frac {\boldsymbol{N}_{PS}(X, \boldsymbol{e})}
          {N_{PS}(\boldsymbol{e})} \\
  \hat{\boldsymbol{P}}(X|\boldsymbol{e})
  &\approx& \frac {\boldsymbol{P}(X, \boldsymbol{e})}
                  {P(\boldsymbol{e})}
  = \boldsymbol{P}(X | \boldsymbol{e})
\end{eqnarray*}

10. Likelihood weighting \\

Likelihood weighting avoids the inefficiency of rejection sampling by generating only events that are consistent with the evidence {\bf e}. It is a particular instance of the general statistical technique of importance sampling, tailored for inference in Bayesian networks.
\begin{eqnarray*}
S_{WS}(\boldsymbol{z}, \boldsymbol{e})
&=& \prod^l_{i=1} P(z_i | parents(Z_i)) \\
w(\boldsymbol{z}, \boldsymbol{e})
&=& \prod^m_{i=1} P(e_i | parents(E_i)) \\
S_{WS}(\boldsymbol{z}, \boldsymbol{e}) w(\boldsymbol{z}, \boldsymbol{e})
&=& \prod^l_{i=1} P(z_i | parents(Z_i)) \prod^m_{i=1} P(e_i | parents(E_i)) \\
&=& P(\boldsymbol{z}, \boldsymbol{e})
\end{eqnarray*}

The likelihood weighting estimates are consistent.
\begin{eqnarray*}
\hat{P}(x | \boldsymbol{e})
&=& \alpha \sum_{\boldsymbol{y}} N_{WS}(x, \boldsymbol{y}, \boldsymbol{e})
    w(x, \boldsymbol{\boldsymbol{y}}, \boldsymbol{e}) \\
&\approx& \alpha' \sum_{\boldsymbol{y}} S_{WS}(x, \boldsymbol{y}, \boldsymbol{e})
          w(x, \boldsymbol{y}, \boldsymbol{e}) \\
&=& \alpha' \sum_{\boldsymbol{y}} P(x, \boldsymbol{y}, \boldsymbol{e}) \\
&=& \alpha' P(x, \boldsymbol{e}) = P(x | \boldsymbol{e})
\end{eqnarray*}

11. Markov blanket \\

The Markov blanket of a variable consists of its parents, children, and children's parents. \\

12. Gibbs sampling in Bayesian networks
\begin{eqnarray*}
\pi_{t+1}(\boldsymbol{x'})
= \sum_{\boldsymbol{x}} \pi_t(\boldsymbol{x}) q(\boldsymbol{x} 
  \rightarrow \boldsymbol{x'})
\end{eqnarray*}

We say that the chain has reached its stationary distribution if $\pi_t = \pi_{t+1}$. Let us call this stationary distribution $\pi$; its defining equation is therefore
\begin{eqnarray*}
\pi(\boldsymbol{x'})
= \sum_{\boldsymbol{x}} \pi_t(\boldsymbol{x}) q(\boldsymbol{x}
  \rightarrow \boldsymbol{x'})
\end{eqnarray*}

Provided the transition probability distribution q is ergodic--that is, every state is reachable from every other and there are no strictly periodic cycles--there is exactly one distribution $\pi$ satisfying this equation for any given $q$. \\

We say that $q(\boldsymbol{x} \rightarrow \boldsymbol{x'})$ is in detailed balance with $\pi(\boldsymbol{x})$ if 
\begin{eqnarray*}
\pi(\boldsymbol{x}) q(\boldsymbol{x} \rightarrow \boldsymbol{x'})
= \pi(\boldsymbol{x'}) q(\boldsymbol{x'} \rightarrow \boldsymbol{x})
\end{eqnarray*}

Detailed balance implies stationarity.
\begin{eqnarray*}
\sum_{\boldsymbol{x}} \pi(\boldsymbol{x}) q(\boldsymbol{x} \rightarrow \boldsymbol{x'}) = \sum_{\boldsymbol{x}} \pi(\boldsymbol{x'}) q(\boldsymbol{x'} \rightarrow \boldsymbol{x}) = \pi(\boldsymbol{x'}) \sum_{\boldsymbol{x}} q(\boldsymbol{x'} \rightarrow \boldsymbol{x}) =\pi(\boldsymbol{x'})
\end{eqnarray*}

Define $\overline{\boldsymbol{X_i}}$ to be these other variables (except the evidence variables); their values in the current state are $\overline{\boldsymbol{x_i}}$. We have
\begin{eqnarray*}
q_i(\boldsymbol{x} \rightarrow \boldsymbol{x'})
= q_i((x_i, \overline{\boldsymbol{x_i}}) \rightarrow 
  (x'_i, \overline{\boldsymbol{x_i}}))
= P(x'_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})
\end{eqnarray*}

Now we show that the transition probability for each step of the Gibbs sampler is in detailed balance with the true posterior:
\begin{eqnarray*}
\pi(\boldsymbol{x}) q_i(\boldsymbol{x} \rightarrow \boldsymbol{x'})
&=& P(\boldsymbol{x}|\boldsymbol{e})
    P(x'_i| \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& P(x_i, \overline{\boldsymbol{x_i}} | \boldsymbol{e})
    P(x'_i| \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& P(x_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})
    P(\overline{\boldsymbol{x_i}} | \boldsymbol{e})
    P(x'_i | \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& P(x_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})
    P(x'_i, \overline{\boldsymbol{x_i}} 
      | \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& \pi(\boldsymbol{x'}) q_i(\boldsymbol{x'} \rightarrow \boldsymbol{x})
\end{eqnarray*}

Sampling $X_i$ from $\boldsymbol{P}(X_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})$ in a Bayesian network. \\

Since a varible is indenpendent of all other variables given its Markov blanket, we have
\begin{eqnarray*}
P(x'_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e}) = P(x'_i | mb(X_i))
\end{eqnarray*}

where $mb(X_i)$ denotes the values of variables in $X_i$'s Markov blanket, $MB(X_i)$. As the probability of a variable given its Markov blanket is proportional to the probability of the variable given its parents times the probability of each child given its respective parents:
\begin{eqnarray*}
P(x'_i|mb(X_i))
= \alpha P(x'_i | parents(X_i)) \times
  \prod_{Y_j \in Children(X_i)} P(y_j | parents(Y_j))
\end{eqnarray*}

Hence, to flip each variable $X_i$ conditioned on its Markov blanket, the number of multiplications required is equal to the number of $X_i$'s children.

\section*{Probabilistic Reasoning Over Time}

1. States and observations
\begin{itemize}
\item $\boldsymbol{X}_t$: the set of state variables at time t, which are assumed to be unobservable.
\item $\boldsymbol{X}_{a:b}$: the set of variables from $\boldsymbol{X}_a$ to $\boldsymbol{X}_b$.
\item $\boldsymbol{E}_t$: the set of observable evidence variables. The observation at time t is $\boldsymbol{E}_t = \boldsymbol{e}_t$ for some set of values $\boldsymbol{e}_t$.
\end{itemize}

2. Markov assumption: the current state depends on only a finite fixed number of previous states. \\

3. First order Markov process: the current state depends only on the previous state and not on any earlier state.
\begin{equation*}
\boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{X}_{0:t-1})
= \boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{X}_{t-1})
\end{equation*}

4. Sensor Markov assumption
\begin{equation*}
\boldsymbol{P}(\boldsymbol{E}_t | \boldsymbol{X}_{0:t}, \boldsymbol{E}_{0:t-1})
= \boldsymbol{P}(\boldsymbol{E}_t | \boldsymbol{X}_t)
\end{equation*}

5. Hidden Markov Models
\begin{equation*}
\boldsymbol{P}(\boldsymbol{X}_{0:t} | \boldsymbol{E}_{1:t})
= \boldsymbol{P}(\boldsymbol{X}_0)
  \prod^t_{i=1} \boldsymbol{P}(\boldsymbol{X}_i | \boldsymbol{X}_{i-1})
  \boldsymbol{P}(\boldsymbol{E}_i | \boldsymbol{X}_i)
\end{equation*}

6. Inference in temporal models
\begin{itemize}
\item Filtering (state estimation): The task of computing the belief state--the posterior distribution over the most recent state--given all evidence to date. $\boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{e}_{1:t})$. \\
The probability of rain today, given all the observations of the umbrella carrier made so far.
\item Prediction: The task of computing the posterior distribution over the future state, give all evidence to date. $\boldsymbol{P}(\boldsymbol{X}_{t+k} | \boldsymbol{e}_{1:t})$ for some $k>0$. \\
The probability of rain three days from now, given all the observations to date.
\item Smoothing: The task of computing the posterior distribution over a past state, given all evidence up to the present. $\boldsymbol{P}(\boldsymbol{X}_k | \boldsymbol{e}_{1:t})$ for some $0 \le k < t$. \\
The probability that it rained last Wednesday, given all the observations of the umbrella carrier made up to today.
\item Most likely explanation: Given a sequence of observations, try to find the sequence of states that is most likely to have generated those observations. \\
$\argmax_{\boldsymbol{X}_{1:t}}\boldsymbol{P}(\boldsymbol{X}_{1:t} | \boldsymbol{e}_{1:t})$ for some $0 \le k < t$.
\item Learning: The transition and sensor models, if not yet known, can be learned from observations. Inference provides an estimate of what transitions actually occurred and of what states generated the sensor readings, and these estimates can be used to update the models.
\end{itemize}

7. Filtering

\begin{equation*}
\boldsymbol{P}(\boldsymbol{X}_{t+1} | \boldsymbol{e}_{1:t+1})
= f(\boldsymbol{e}_{t+1}, 
    \boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{e}_{1:t}))
\end{equation*}

8. Forward backward equations
\begin{eqnarray*}
\boldsymbol{f}_{1:t+1}
&=& \alpha \text{FORWARD}(\boldsymbol{f}_{1:t}, \boldsymbol{e}_{t+1}) \\
\boldsymbol{b}_{k+1:t}
&=& \alpha \text{BACKWARD}(\boldsymbol{b}_{k+2:t}, \boldsymbol{e}_{k+1})
\end{eqnarray*}

9. Forward backward equations for HMM
\begin{eqnarray*}
\boldsymbol{f}_{1:t+1}
&=& \alpha \boldsymbol{O}_{t+1} \boldsymbol{T}^T \boldsymbol{f}_{1:t} \\
\boldsymbol{b}_{k+1:t}
&=& \alpha \boldsymbol{T} \boldsymbol{O}_{k+1} \boldsymbol{b}_{k+2:t}
\end{eqnarray*}

\section*{Making Simple Decisions}

1. MEU \\

The agent's preferences are captured by a utility function, $U(s)$, which assigns a single number to express the desirability of a state. The expected utility of an action given the evidence, $EU(a | \boldsymbol{e})$, is just the average utility value that maximizes the agent's expected utility:
\begin{equation*}
  EU(a | \boldsymbol{e}) 
  = \sum_{s'} P(\text{RESULT}(a) = s'|a,\boldsymbol{e})U(s')
\end{equation*}

The principle of {\bf maximum expected utility} (MEU) says that a rational agent should choose the action that maximizes the agent's expected utility:
\begin{equation*}
  \text{action} = \argmax_a EU(a|\boldsymbol{e})
\end{equation*}

2. Constraints on rational preference \\

Notation to describe an agent's preferences:
\begin{itemize}
  \item $A \succ B$ the agent prefers $A$ over $B$
  \item $A \sim B$ the agent is indifferent between $A$ and $B$
  \item $A \succcurlyeq B$ the agent prefers $A$ over $B$ or is indifferent between them
\end{itemize}

A lottery $L$ with pssible outcomes $S_1,...,S_n$ that occur with probabilities $p_1,...,p_n$ is written
\begin{equation*}
  L = [p_1, S_1; p_2, S_2; ..., p_n,S_n]
\end{equation*}

Six constraints that any reasonable preference relation should obey

\begin{itemize}
  \item Orderability
    \begin{equation*}
      \text{Exactly one of } (A \succ B), (B \succ A), or  (A \sim B)
      \text{ holds.}
    \end{equation*}
  \item Transitivity
    \begin{equation*}
      (A \succ B) \land (B \succ C) \Rightarrow (A \succ C)
    \end{equation*}
  \item Continuity
    \begin{equation*}
      (A \succ B \succ C) \Rightarrow \exists p [p, A; 1-p,C] \sim B.
    \end{equation*}
  \item Substitutability
    \begin{equation*}
      A \sim B \Rightarrow [p,A; 1-p,C] \sim [p,V; 1-p,C].
    \end{equation*}
    This also holds if we substitute $\succ$ for $\sim$ in this axiom.
  \item Montonicity
    \begin{equation*}
      A \succ B \Rightarrow
      (p > q \Leftrightarrow [p,A;1-p,B] \succ [q,A;1-q,B]).
    \end{equation*}
  \item Decomposability
    \begin{equation*}
      [p, A; 1-p, [q, B; 1-q, C]] \sim [p, A; (1-p)q, B; (1-p)(1-q), C].
    \end{equation*}
\end{itemize}

3. Preferences lead to utility

\begin{itemize}
  \item Existence of Utility Function
    \begin{eqnarray*}
      U(A) > U(B) \Rightarrow A \succ B \\
      U(A) = U(B) \Rightarrow A \sim B
    \end{eqnarray*}
  \item Expected Utility of a Lottery
    \begin{eqnarray*}
      U([p_1,S_1;...;p_n,S_n]) = \sum_i p_i U(S_i).
    \end{eqnarray*}
\end{itemize}

The preceding theorems establish that a utility function exists for any rational agent, but they do not establish tat it is unique.
\begin{equation*}
  U'(S) = aU(S) + b,
\end{equation*}

where $a$ and $b$ are constants and $a>0$. \\

As in game-playing, in a deterministic environment an agent just needs a preference ranking on states-the numbers don't matter. This is called a value function or ordinal utility function.

\section*{Making Complex Decisions}

1. Markov decision process (MDP) \\

MDP is a sequential decision problem for a fully observable, stochastic environment with a Markovian transition model and additive reward. \\

It consists of a set of states (with initial state $s_0$); a set ACTIONS($s$) of actions in each state; a transition model $P(s'| s, a)$; and a reward function $R(s)$. \\

We assume the transitions are Markovian in the sense that the probability of reaching $s'$ from $s$ depends only on $s$ and not on the history of earlier states. \\

2. Policy \\

A solution is called a policy. $\pi(s)$ is the action recommended by the policy $\pi$ for state $s$. \\

An optimal policy is a policy that yields the highest expected utility. A proper policy is a policy that is guaranteed to reach a terminal state. \\

3. Finite horizon \\

There is a fixed time $N$ after which nothing matters--the game is over. Thus $U_h([s_0, s_1,...,s_{N+k}]) = U_h([s_0, s_1,...,s_{N+k}])$ for all $k>0$.  With a finite horizon, the optimal action in a given state could change over time (nonstationary). \\

4. Rewards
\begin{itemize}
\item Additive rewards: The utility of a state sequence is
  \begin{equation*}
    U_h([s_0,s_1,s_2,...]) = R(s_0)+R(s_1)+R(s_2)+...
  \end{equation*}
\item Discounted rewards: The utility of a state sequence is
  \begin{equation*}
    U_h([s_0,s_1,s_2,...])
    = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2)+...
  \end{equation*}
  where the discount factor $\gamma$ is a number between 0 and 1. The discount factor describes the preference of an agent for current rewards over future rewards. A discount factor of $gamma$ is equivalent to an interest rate of $(1 / \gamma -1)$.
\end{itemize}

5. Optimal policies and the utilities of states \\

We assume the agent is in some initial state $s$ and define $S_t$ (a random variable) to be the state the agent reaches at time $t$ when executing a particular policy $\pi$.  The probability distribution over state sequences $S_1, S_2,...,$ is determined by the initial state $s$, the policy $\pi$, and the transition model for the environment. \\

The expected utility obtained by executing $\pi$ starting in $s$ is given by
\begin{equation*}
U^{\pi}(s) = E \left[ \sum^{\infty}_{t=0} \gamma^t R(S_t) \right]
\end{equation*}

where the expectation is with respect to the probability distribution over state sequences determined by $s$ and $\pi$. We'll use $\pi^*_S$ to denote:
\begin{equation*}
\pi^*(s) = \argmax_{a \in A(s)} U^{\pi}(s')
\end{equation*}

A remarkable consequence of using discounted utilities with infinite horizons is that the optimal policy is independent of the starting state. \\

The utility function $U(s)$ allows the agent to select actions by using the principle of maximum expected utility:
\begin{equation*}
\pi^*(s) = \argmax_{a \in A(s)} \sum_{s'} P(s'|s,a) U(s')
\end{equation*}

6. The value iteration algorithm \\

The basic idea is to calculate the utility of each state and then use the state utilities to select an optimal action in each state. \\

7. The Bellman equation for utilities
\begin{eqnarray*}
U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s' | s, a) U(s')
\end{eqnarray*}

8. The Bellman update
\begin{eqnarray*}
U_{i+1}(s) \leftarrow
R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s' | s, a) U_i(s')
\end{eqnarray*}

where the update is assumed to be applied simultaneously to all the states at each iteration. \\

9. The policy iteration algorithm \\

The policy iteration algorithm alternates the following two steps, beginning from some initial policy $\pi_0$:
\begin{itemize}
\item Policy evaluation: Given a policy $\pi_i$, calculate $U_i = U^{\pi}$, the utility of each state if $\pi_i$ were to be executed.
\item Policy improvement: Calculate a new MEU policy $\pi_{i+1}$, using one-step look-ahead based on $U_i$.
\end{itemize}

The algorithm terminates when the policy improvement step yields no change in the utilities. \\

For small state spaces, policy evaluation using exact solution methods is often the most efficient approach. For large state spaces, $O(n^3)$ time might be prohibitive. \\

10. The simplified Bellman update
\begin{eqnarray*}
U_{i+1}(s) = R(s) + \gamma \sum_{s'} P(s' | s, \pi(s)) U_i(s')
\end{eqnarray*}

The resulting algorithm is called modified policy iteration. It is often more efficient than standard policy iteration or value iteration.

\section*{Learning From Examples}

1. Learning \\

An agent is learning if it improves its performance on future tasks after making observations about the world. \\

2. Forms of learning \\

Any component of an agent can be improved by learning from data. The improvements, and the techniques used to make them, depend on four major factors:
\begin{itemize}
  \item Which component is to be improved.
  \item What prior knowledge that agent already has.
  \item What representation is used for the data and the component.

    Factored representation: a vector of attribute values.
  \item What feedback is available to learn from.
    \begin{itemize}
      \item Unsupervised learning

        In unsupervised learning the agent learns patterns in the input even though no explicit feedback is supplied. The most common unsupervised learning task is clustering: detecting potentially useful clusters of input examples.
      \item Reinforcement learning

        In reinforcement learning the agent learns from a series of reinforcement--reward or punishments.
      \item Supervised learning

        In supervised learning the agent observes some example input-output pairs and learns a function that maps from input to output.
      \item Semi-supervised learning*

        In semi-supervised learning we are given a few labeled examples and must make what we can of a large collection of unlabeled examples.
    \end{itemize}
\end{itemize}

3. Learning decision trees \\

A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true if and only if the input attributes satisfy one of the paths leading to a leaf with value true:
\begin{equation*}
  Goal \Leftrightarrow (Path_1 \lor Path_2 \lor ...).
\end{equation*}

{\bf Entropy} is a measure of the uncertainty of a random variable; acquisition of information corresponds to a reduction in entropy.
\begin{equation*}
  Entropy: \; H(V) = \sum_k P(v_k) \log_2{\frac{1}{P(v_k)}}
                   = -\sum_k P(V_k) \log_2{P(v_k)}
\end{equation*}

Define $B(q)$ as the entrop of a Boolean random variable that is true with probability $q$:
\begin{equation*}
  B(q) = -(q \log_2 {q} + (1-q) \log_2 (1-q)).
\end{equation*}

If a traning set contains $p$ positive examples and $n$ negative examples, then the entrop of the goal attribute on the whole set is
\begin{equation*}
  H(Goal) = B(\frac{p}{p+n}).
\end{equation*}

An attribute $A$ with $d$ distinct values divides the training set $E$ into subsets $E_1,...,E_d$. Each subset $E_k$ has $p_k$ positive examples and $n_k$ negative examples, then the expected entropy remaining after testing attribute $A$ is
\begin{equation*}
  Remainder(A) = \sum^d_{k=1} \frac{p_k+n_k}{p+n}B(\frac{p_k}{p_k+n_k}).
\end{equation*}

The {\bf inforamtion gain} from the attribute test on $A$ is the expected reduction in entropy:
\begin{equation*}
  Gain(A) = B(\frac{p}{p+n}) - Remainder(A).
\end{equation*}

4. From error rates to loss \\

The {\bf loss function} is defined as the amount of utility lost by predictiong $h(x) = \hat {y}$ when the correct answer is $f(x) = y$:
\begin{eqnarray*}
  L(x, y, \hat{y})
  &=& Utility(\text {result of using $y$ given an input $x$}) \\
  &-& Utility(\text {result of using $\hat{y}$ given an input $x$})
\end{eqnarray*}

In general, small errors are better than large ones; two functions that implement that idea are the absolute value of the difference (called $L_1$ loss), and the square of the difference (called the $L_2$ loss), If we are content with the idea of minimizing error rate, we can use the $L_{0/1}$ loss function, which has a loss of 1 for an incorrect answer and is appropriate for discrete-valued outputs:

\begin{itemize}
  \item Absolute value loss: $L_1(y, \hat{y}) = |y - \hat{y}|$
  \item Squared error loss: $L_2(y, \hat{y}) = (y - \hat{y})^2$
  \item Absolute value loss: $L_{0/1}=0$ if $y = \hat{y}$, else 1
\end{itemize}

Let $\varepsilon$ be the set of all possible input-output examples. Then the expected generalization loss for a hypothesis $h$ (with respect to loss function $L$) is
\begin{equation*}
  GenLoss_L(h) = \sum_{(x,y) \in \varepsilon} L(y,h(x)) P(x,y),
\end{equation*}

and the best hypothesis, $h^*$, is the one with the minimum expected generalization loss:
\begin{equation*}
  h^* = \argmin_{h \in \mathcal{H}} GenLoss_L (h).
\end{equation*}

Because $P(x,y)$ is not known, the learning agnet can only estimate generalization loss with empirical loss on a set of examples, $E$:
\begin{equation*}
  EmpLoss_L(h) = \frac{1}{N} \sum_{(x,y) \in E} L(y,h(x)) P(x,y).
\end{equation*}

The estimated best hypothesis, $\hat{h}^*$, is the one with the minimum expected generalization loss:
\begin{equation*}
  \hat{h}^* = \argmin_{h \in \mathcal{H}} GenLoss_L (h).
\end{equation*}

The process of explicityly penalizing complex hypotheses is called {\bf regularization} (because it looks for a function that is more regular, or less complex):
\begin{eqnarray*}
  Cost(h) &=& EmpLoss(h) + \lambda Complexity(h) \\
  \hat{h}^* &=& \argmin_{h \in \mathcal{H}} Cost(h).
\end{eqnarray*}

5. Artificial neural networks \\

Neural networks are composed of nodes or units connected by directed links. A link from unit $i$ to unit $j$ serves to propagate the activation $a_i$ from $i$ to $j$. Each link also has a numeric weight $w_{i,j}$ associated with it, which determines the strength and sign of the connection. Just as an linear regression models, each unit has a dummy input $a_0=1$ with an associated weight $w_{0,j}$. Each unit $j$ first computes a weighted sum of its input:
\begin{equation*}
  in_j = \sum^n_{i=0} w_{i,j} a_i.
\end{equation*}

Then it applies an activation function $g$ to this sum derive the output:
\begin{equation*}
  a_j = g(in_j) = g \left( \sum^n_{i=0} w_{i,j}a_i \right).
\end{equation*}

The activation function $g$ is typically either a hard threshold, in which case the unit is called a {\bf perceptron}, or a logistic function, in which case the term {\bf sigmoid perceptron} is sometimes used. Both of these nonlinear activation function ensure the important property that the entire network of units can represent a nonlinear function. \\

A feed-forward network has connections only in one direction--that is, it forms a directed acyclic graph. A recurrent network, on the other hand, feeds its outputs back into its own inputs. \\

6. Support vector machine \\

Instead of minimizing expected empirical loss on the training data, SVMs attempt to minimize expected generalization loss. \\

Notations: SVMs use the convention that class labels are +1 and -1.The separator is defined as the set of points $\{\boldsymbol{x}: \boldsymbol{w} \cdot \boldsymbol{x} + b = 0 \}$. \\

SVMs can be represented in the form of dual representation, in which the optimal solution is found by solving
\begin{equation*}
  \argmax_{\alpha} \sum_j \alpha_j - \frac{1}{2} \sum_{j,k} \alpha_j \alpha_k y_j y_k (\boldsymbol{x}_j, \boldsymbol{x}_k)
\end{equation*}

subject to the constraints $\alpha_j \ge 0$ and $\sum_j \alpha_j y_j = 0$. This is a quadratic programming optimization problem. Once we have found the vector $\alpha$ we can get back to $\boldsymbol{w}$ with the equation $\boldsymbol{w} = \sum_j \alpha_j \boldsymbol{x}_j$, or we can stay in the dual representation. Once the optimal $\alpha_j$ have been calculated, it is
\begin{equation*}
  h(\boldsymbol{x})
  = sign \left( \sum_j \alpha_j y_j (\boldsymbol{x \cdot x_j}) - b \right).
\end{equation*}

Note that the weights $\alpha_j$ associated with each data point are zero expect for {\bf support vectors}--the points closet to the separator. (They are called ``support'' vectors because they ``hold up'' the separating plane). \\

The expression $(\boldsymbol{x_j \cdot x_k})^2$ is called a {\bf kernel function}, and is usually written as $K(\boldsymbol{x_j \cdot x_k})$.

\end{document}
