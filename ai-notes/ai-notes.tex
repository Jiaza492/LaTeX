\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}

\title{Artificial Intelligence Note}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\maketitle

\setlength{\parindent}{0in}

\section{Intelligent Agents}

1. An {\bf agent} is anything that can be viewed as perceiving environment throught sensors and acting upon that evnironment throught actuators. \\

2. Property of task environments

\begin{itemize}
  \item Fully observable vs. partially observable
  \item Signle angent vs. multiagent
  \item Deterministic vs. stochastic
  \item Episodic vs. sequential
  \item Static vs. dynamic
  \item Discrete vs. continuous
  \item Known vs. unknown
\end{itemize}

\section{Solving Problems by Searching}

1. Uniformed search strategies

\begin{enumerate}
  \item Breadth-first search
  \item Uniform-cost search ($g(n)$)
  \item Depth-first search
  \item Depth-limited search
  \item Iterative deepening depth-first search
  \item Bidirectional search
\end{enumerate}

2. Infromed (heuristic) search strategies

\begin{enumerate}
  \item Greedy best-first search ($h(n)$)
  \item $A^*$ search \\
    Admissibility: An admissible heuristic is one that never overestimates the cost to reach the goal. \\
    Consistency (monotonicity): $h(n) \le c(n,a,n') + h(n')$. \\
    * Every consistent heuristic is also admissible.
  \item Iterative-deepening $A^*$
  \item Recursive best-first search (RBFS) \\
    backed-up value: the best $f$-value of its children
\end{enumerate}

\section{Beyond Classical Search}

1. Local Search Algorithms \\

If the path to the goal does not matter... \\

Local search algorithms operate using a single current node (rather than multiple paths) and generally move only to neighbors of that node.

\begin{enumerate} 
  \item Hill-climbing (greedy local search) \\
    Stuck reasons: local maxima, ridges, plateaux. \\
    Alternatives: stochastic hill climbing, first-choice hill climbing, random restart hill climbing.
  \item Simulated annealing
  \item Local beam search \\
    In a local beam search, useful information is passed among the parallel search threads. \\
    Alternatives: stochastic beam search
  \item Genetic algorithms
\end{enumerate}

2. Searching with nondeterministic actions \\

AND-OR search trees \\

OR nodes: In a deterministic environment, the only branching is introduced by the agent's own choices in each state. \\
AND nodes: In a nondeterministic environment, branching is also introduced by the environment's choice of outcome for each action. \\

\section{Adverserial Search}

1. Zero-sum (constant-sum) games: a game where the total payoff to all players is the same for every instance of the game. \\

2. The minimax algorithm \\

The minimax algorithm performs a complete depth-first exploration of the game tree. \\

3. Alpha-beta pruning \\

$\alpha =$ the value of the best (i.e., highest-value) choice we have found so far at any choice point along the path for MAX. \\
$\beta =$ the value of the best (i.e., lowest-value) choice we have found so far at any choice point along the path for MIN. \\

4. Imperfect real-time decisions: cut-off search

\section{Logical Agents}

1. A knowledge base is a set of sentences. Each sentence is expressed in a language called a knowledge representation language and represents some assertion about the world. A knowledge base, KB, may initially contain some background knowledge. \\

2. Entailment: a sentence follows logically from another sentence.

\begin{equation*}
  \alpha \models \beta \text{ if and only if } M(\alpha) \subseteq M(\beta)
\end{equation*}

if $\alpha \models \beta$, then $\alpha$ is a stronger assertion than $\beta$: it rules out more possible worlds. \\

Sound: an inference algorithm that derives only entailed sentences is called sound or truth-preserving. \\

Complete: an inference algorithm is complete if it can derive any sentence that is entailed. \\

3. Deduction theorem

\begin{equation*}
  \alpha \models \beta \text{ if and only if the sentence } (\alpha \Rightarrow \beta) \text{ is valid.}
\end{equation*}

\begin{equation*}
  \alpha \models \beta \text{ if and only if the sentence } (\alpha \lor \lnot \beta) \text{ is unsatisfiable.}
\end{equation*}

4. Modus Ponens

\begin{equation*}
  \frac {\alpha \Rightarrow \beta, \text{   } \alpha}{\beta}
\end{equation*}

5. And-Elimination

\begin{equation*}
  \frac {\alpha \land \beta}{\alpha} 
\end{equation*}

6. Unit resolution inference rule

\begin{equation*}
  \frac {\ell_1 \lor \cdots \lor \ell_k, \text{   } m}
        {\ell_1 \lor \cdots \lor \ell_{i-1} \lor \ell_{i+1} \lor \cdots \lor \ell_k}
\end{equation*}

where each $\ell$ is a literal and $\ell_i$ and $m$ are complementary literals. \\

7. Conjunctive normal form \\

(Disjunct) conjunct (Disjunct) \\

8. Ground resolution theorem \\

If a set of clauses is unsatisfiable, then the resolution closure of those clauses contains the empty clause. \\

9. Horn clause and definite clauses \\

Definite clauses: a disjunction of literals of which exactly one is positive. For example, the clause $(\lnot L_{1,1} \lor \lnot Breeze \lor B_{1,1})$. \\

Horn clause: a disjunction of literals of which at most one is positive. \\

Horn clauses are closed under resolution: if you resolve two Horn clauses, you get back a Horn clause.

\section{First-order Logic}

1. Terms: A term is a logical expression that refers to an object. For example,

\begin{equation*}
  LeftLeg(John)
\end{equation*}

A term with no variables is called a ground term. \\

2. Atomic sentences: an atomic sentence is formed from a predicate symbol optionally followed by a parenthesized list of terms, such as

\begin{equation*}
  Brother(Richard, John)
\end{equation*}

3. Complex sentences \\

We can use logical connectives to construct more complex sentences, with the same syntax and semantics as in proportional calculus. \\

4. Quantifiers \\

Uuniversial quantification ($\forall$) \\
Common mistake: use conjunction instead of implication with $\forall$ will lead to an overly strong statement.  \\

Existential quantification ($\exists$) \\
Common mistake: use implication instead of conjunction with $\exists$ will lead to a very weak statement. \\

5. Equality

\begin{equation*}
  Father(John) = Henry
\end{equation*}

6. Universal instantation (UI) \\

Every instantiation of a univerally quantified sentence is entailed by it:

\begin{equation*}
  \frac {\forall v \alpha}{Subst(\{v/g\}, \alpha)}
\end{equation*}

for any variable $v$ and ground term $g$. \\

7. Exisential instantation (EI) \\

For any sentence $\alpha$, variable $v$, and constant symbol $k$ that does not appear elsewhere in the knowledge base:

\begin{equation*}
  \frac {\exists v \alpha}{Subst(\{v/k\}, \alpha)}
\end{equation*}

E.g., $\exists x Crown(x) \land OnHead(x, John)$ yields:

\begin{equation*}
  Crown(C_1) \land OnHead(C_1, John)
\end{equation*}

provided $C_1$ is a new constant symbol, called a Skolem constant.

\section{Quantifying Uncertainty}

1. Trying to use logic to cope with a domain like medical diagnosis fails for three main reasons: laziness, theoretical ignorance, practical ignorance. \\

2. Decision theory = probability theory + utility theory \\

3. Maximum expected utility (MEU): An agent is rational if and only if it chooses the action that yields the highest expected utility, averaged over all the possible outcomes of the action. \\

4. Sample space: the set of all possible worlds (possible worlds are mutally exclusive). \\

5. Normalization

\begin{equation*}
\boldsymbol{P}(X|\boldsymbol{e}) =
\alpha \boldsymbol{P}(X, \boldsymbol{e}) = \alpha \sum_y \boldsymbol{P}(X, \boldsymbol{e}, \boldsymbol{y})
\end{equation*}

6. Independence

\begin{equation*}
P(X,Y) = P(X)P(Y)
\end{equation*}

7. Bayes' rule

\begin{equation*}
P(b|a) = \frac {P(a|b)P(b)}{P(a)}
\end{equation*}

8. Naive Bayes model

\begin{equation*}
P(Cause, Effect_1,..., Effect_n) =
P(Cause) \prod_iP(Effect_i|Cause)
\end{equation*}

\section{Probabilistic Reasoning}

1. CPT: conditional probability table. \\

2. Representation of conditional distribution

\begin{itemize}
  \item Probit distribution
    \begin{eqnarray*}
      \Phi(x) &=& \int^x_{-\infty} N(0,1)(x)dx \\
      P(buys | Cost = c) &=& \Phi((-c + \mu) / \sigma)
    \end{eqnarray*}
  \item Logit distribution
    \begin{eqnarray*}
      P(buys | Cost = c) &=&
      \frac {1}{1 + \exp(-2 \frac {-c + \mu}{\sigma})}
    \end{eqnarray*}
\end{itemize}

The two distributions look similar, but the logit distribution actually has much longer "tails''. The probit is often better fit to real situations, but the logit is sometimes easier to deal with mathematically. It is widely used in neural networks. \\

3. Representing the full joint distribution

\begin{equation*}
P(x_1,...,x_n) = \prod^n_{i=1} \theta(x_i | parents(X_i))
\end{equation*}

4. The chain rule

\begin{equation*}
P(x_1,...,x_n) = \prod^n_{i=1} P(x_i | x_{i-1},...,x_1)
\end{equation*}

5. Locally structured (sparse) system \\

In a locally structured system, each subcomponent interacts directly with only a bounded number of other components, regardless of the total number of components. Local structure is usually associated with linear rather than exponential growth in complexity. \\

6. Inference by enumeration (p523)
\begin{eqnarray*}
\boldsymbol{P}(B|j,m)
&=& \alpha \boldsymbol{P}(B,j,m)
= \alpha \sum_e \sum_a \boldsymbol{P}(B,j,m,e,a) \\
P(b|j,m)
&=& \alpha \sum_e \sum_a P(b) P(e) P(a|b,e) P(j|a) P(m|a) \\
P(b|j,m)
&=& \alpha P(b) \sum_e P(e) \sum_a P(a|b,e) P(j|a) P(m|a)
\end{eqnarray*}

7. The complexity of exact inference \\

Singly connected netowrks (polytrees): The time and space complexity of exact inference in polytrees is linear in the size of the network. \\

Multiply connected networks: \#P-hard. \\

8. Direct sampling methods \\

Suppose there are $N$ total samples, and let $N_{PS}(x_1,...,x_n)$ be the number of times the specific event $x_1,...,x_n$ occurs in the set of samples. Then
\begin{equation*}
  \lim_{N\rightarrow \infty} \frac {N_{PS}(x_1,...,x_n)}{N}
  = S_{PS}(x_1,...,x_n) = P(x_1,...,x_n)
\end{equation*}

Whenever we use an approximate equality (``$approx$") in what follows, we mean it in exactly this sense--that the estimated probability becomes exact in the large-sample limit. Such an estimate is called consistent.
\begin{equation*}
  P(x_1,...,x_m) \approx N_{PS}(x_1,...,x_m)/N
\end{equation*}

9. Rejection sampling \\

Rejection sampling is a general method for producing samples from a hard-to-sample distribution given an easy-to-sample distribution.
\begin{eqnarray*}
  \hat{\boldsymbol{P}}(X|\boldsymbol{e})
  &=& \alpha \boldsymbol{N}_{PS}(X, \boldsymbol{e})
  = \frac {\boldsymbol{N}_{PS}(X, \boldsymbol{e})}
          {N_{PS}(\boldsymbol{e})} \\
  \hat{\boldsymbol{P}}(X|\boldsymbol{e})
  &\approx& \frac {\boldsymbol{P}(X, \boldsymbol{e})}
                  {P(\boldsymbol{e})}
  = \boldsymbol{P}(X | \boldsymbol{e})
\end{eqnarray*}

10. Likelihood weighting \\

Likelihood weighting avoids the inefficiency of rejection sampling by generating only events that are consistent with the evidence {\bf e}. It is a particular instance of the general statistical technique of importance sampling, tailored for inference in Bayesian networks.
\begin{eqnarray*}
S_{WS}(\boldsymbol{z}, \boldsymbol{e})
&=& \prod^l_{i=1} P(z_i | parents(Z_i)) \\
w(\boldsymbol{z}, \boldsymbol{e})
&=& \prod^m_{i=1} P(e_i | parents(E_i)) \\
S_{WS}(\boldsymbol{z}, \boldsymbol{e}) w(\boldsymbol{z}, \boldsymbol{e})
&=& \prod^l_{i=1} P(z_i | parents(Z_i)) \prod^m_{i=1} P(e_i | parents(E_i)) \\
&=& P(\boldsymbol{z}, \boldsymbol{e})
\end{eqnarray*}

The likelihood weighting estimates are consistent.
\begin{eqnarray*}
\hat{P}(x | \boldsymbol{e})
&=& \alpha \sum_{\boldsymbol{y}} N_{WS}(x, \boldsymbol{y}, \boldsymbol{e})
    w(x, \boldsymbol{\boldsymbol{y}}, \boldsymbol{e}) \\
&\approx& \alpha' \sum_{\boldsymbol{y}} S_{WS}(x, \boldsymbol{y}, \boldsymbol{e})
          w(x, \boldsymbol{y}, \boldsymbol{e}) \\
&=& \alpha' \sum_{\boldsymbol{y}} P(x, \boldsymbol{y}, \boldsymbol{e}) \\
&=& \alpha' P(x, \boldsymbol{e}) = P(x | \boldsymbol{e})
\end{eqnarray*}

11. Markov blanket \\

The Markov blanket of a variable consists of its parents, children, and children's parents. \\

12. Gibbs sampling in Bayesian networks
\begin{eqnarray*}
\pi_{t+1}(\boldsymbol{x'})
= \sum_{\boldsymbol{x}} \pi_t(\boldsymbol{x}) q(\boldsymbol{x} 
  \rightarrow \boldsymbol{x'})
\end{eqnarray*}

We say that the chain has reached its stationary distribution if $\pi_t = \pi_{t+1}$. Let us call this stationary distribution $\pi$; its defining equation is therefore
\begin{eqnarray*}
\pi(\boldsymbol{x'})
= \sum_{\boldsymbol{x}} \pi_t(\boldsymbol{x}) q(\boldsymbol{x}
  \rightarrow \boldsymbol{x'})
\end{eqnarray*}

Provided the transition probability distribution q is ergodic--that is, every state is reachable from every other and there are no strictly periodic cycles--there is exactly one distribution $\pi$ satisfying this equation for any given $q$. \\

We say that $q(\boldsymbol{x} \rightarrow \boldsymbol{x'})$ is in detailed balance with $\pi(\boldsymbol{x})$ if 
\begin{eqnarray*}
\pi(\boldsymbol{x}) q(\boldsymbol{x} \rightarrow \boldsymbol{x'})
= \pi(\boldsymbol{x'}) q(\boldsymbol{x'} \rightarrow \boldsymbol{x})
\end{eqnarray*}

Detailed balance implies stationarity.
\begin{eqnarray*}
\sum_{\boldsymbol{x}} \pi(\boldsymbol{x}) q(\boldsymbol{x} \rightarrow \boldsymbol{x'}) = \sum_{\boldsymbol{x}} \pi(\boldsymbol{x'}) q(\boldsymbol{x'} \rightarrow \boldsymbol{x}) = \pi(\boldsymbol{x'}) \sum_{\boldsymbol{x}} q(\boldsymbol{x'} \rightarrow \boldsymbol{x}) =\pi(\boldsymbol{x'})
\end{eqnarray*}

Define $\overline{\boldsymbol{X_i}}$ to be these other variables (except the evidence variables); their values in the current state are $\overline{\boldsymbol{x_i}}$. We have
\begin{eqnarray*}
q_i(\boldsymbol{x} \rightarrow \boldsymbol{x'})
= q_i((x_i, \overline{\boldsymbol{x_i}}) \rightarrow 
  (x'_i, \overline{\boldsymbol{x_i}}))
= P(x'_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})
\end{eqnarray*}

Now we show that the transition probability for each step of the Gibbs sampler is in detailed balance with the true posterior:
\begin{eqnarray*}
\pi(\boldsymbol{x}) q_i(\boldsymbol{x} \rightarrow \boldsymbol{x'})
&=& P(\boldsymbol{x}|\boldsymbol{e})
    P(x'_i| \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& P(x_i, \overline{\boldsymbol{x_i}} | \boldsymbol{e})
    P(x'_i| \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& P(x_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})
    P(\overline{\boldsymbol{x_i}} | \boldsymbol{e})
    P(x'_i | \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& P(x_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})
    P(x'_i, \overline{\boldsymbol{x_i}} 
      | \overline{\boldsymbol{x_i}},\boldsymbol{e}) \\
&=& \pi(\boldsymbol{x'}) q_i(\boldsymbol{x'} \rightarrow \boldsymbol{x})
\end{eqnarray*}

Sampling $X_i$ from $\boldsymbol{P}(X_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e})$ in a Bayesian network. \\

Since a varible is indenpendent of all other variables given its Markov blanket, we have
\begin{eqnarray*}
P(x'_i | \overline{\boldsymbol{x_i}}, \boldsymbol{e}) = P(x'_i | mb(X_i))
\end{eqnarray*}

where $mb(X_i)$ denotes the values of variables in $X_i$'s Markov blanket, $MB(X_i)$. As the probability of a variable given its Markov blanket is proportional to the probability of the variable given its parents times the probability of each child given its respective parents:
\begin{eqnarray*}
P(x'_i|mb(X_i))
= \alpha P(x'_i | parents(X_i)) \times
  \prod_{Y_j \in Children(X_i)} P(y_j | parents(Y_j))
\end{eqnarray*}

Hence, to flip each variable $X_i$ conditioned on its Markov blanket, the number of multiplications required is equal to the number of $X_i$'s children.

\section{Probabilistic Reasoning Over Time}

1. States and observations
\begin{itemize}
\item $\boldsymbol{X}_t$: the set of state variables at time t, which are assumed to be unobservable.
\item $\boldsymbol{X}_{a:b}$: the set of variables from $\boldsymbol{X}_a$ to $\boldsymbol{X}_b$.
\item $\boldsymbol{E}_t$: the set of observable evidence variables. The observation at time t is $\boldsymbol{E}_t = \boldsymbol{e}_t$ for some set of values $\boldsymbol{e}_t$.
\end{itemize}

2. Markov assumption: the current state depends on only a finite fixed number of previous states. \\

3. First order Markov process: the current state depends only on the previous state and not on any earlier state.
\begin{equation*}
\boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{X}_{0:t-1})
= \boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{X}_{t-1})
\end{equation*}

4. Sensor Markov assumption
\begin{equation*}
\boldsymbol{P}(\boldsymbol{E}_t | \boldsymbol{X}_{0:t}, \boldsymbol{E}_{0:t-1})
= \boldsymbol{P}(\boldsymbol{E}_t | \boldsymbol{X}_t)
\end{equation*}

5.
\begin{equation*}
\boldsymbol{P}(\boldsymbol{X}_{0:t} | \boldsymbol{E}_{1:t})
= \boldsymbol{P}(\boldsymbol{X}_0)
  \prod^t_{i=1} \boldsymbol{P}(\boldsymbol{X}_i | \boldsymbol{X}_{i-1})
  \boldsymbol{P}(\boldsymbol{E}_i | \boldsymbol{X}_i)
\end{equation*}

6. Inference in temporal models
\begin{itemize}
\item Filtering (state estimation): The task of computing the belief state--the posterior distribution over the most recent state--given all evidence to date. $\boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{e}_{1:t})$. \\
The probability of rain today, given all the observations of the umbrella carrier made so far.
\item Prediction: The task of computing the posterior distribution over the future state, give all evidence to date. $\boldsymbol{P}(\boldsymbol{X}_{t+k} | \boldsymbol{e}_{1:t})$ for some $k>0$. \\
The probability of rain three days from now, given all the observations to date.
\item Smoothing: The task of computing the posterior distribution over a past state, given all evidence up to the present. $\boldsymbol{P}(\boldsymbol{X}_k | \boldsymbol{e}_{1:t})$ for some $0 \le k < t$. \\
The probability that it rained last Wednesday, given all the observations of the umbrella carrier made up to today.
\item Most likely explanation: Given a sequence of observations, try to find the sequence of states that is most likely to have generated those observations. \\
$\argmax_{\boldsymbol{X}_{1:t}}\boldsymbol{P}(\boldsymbol{X}_{1:t} | \boldsymbol{e}_{1:t})$ for some $0 \le k < t$.
\item Learning: The transition and sensor models, if not yet known, can be learned from observations. Inference provides an estimate of what transitions actually occurred and of what states generated the sensor readings, and these estimates can be used to update the models.
\end{itemize}

7. Filtering

\begin{equation*}
\boldsymbol{P}(\boldsymbol{X}_{t+1} | \boldsymbol{e}_{1:t+1})
= f(\boldsymbol{e}_{t+1}, 
    \boldsymbol{P}(\boldsymbol{X}_t | \boldsymbol{e}_{1:t}))
\end{equation*}

8.
\begin{eqnarray*}
\boldsymbol{f}_{1:t+1}
&=& \alpha \text{FORWARD}(\boldsymbol{f}_{1:t}, \boldsymbol{e}_{t+1}) \\
\boldsymbol{b}_{k+1:t}
&=& \alpha \text{BACKWARD}(\boldsymbol{b}_{k+2:t}, \boldsymbol{e}_{k+1})
\end{eqnarray*}

9. Hidden Markov Models
\begin{eqnarray*}
\boldsymbol{f}_{1:t+1}
&=& \alpha \boldsymbol{O}_{t+1} \boldsymbol{T}^T \boldsymbol{f}_{1:t} \\
\boldsymbol{b}_{k+1:t}
&=& \alpha \boldsymbol{T} \boldsymbol{O}_{k+1} \boldsymbol{b}_{k+2:t}
\end{eqnarray*}

\section{Making Simple Decisions}

1. MEU \\

The agent's preferences are captured by a utility function, $U(s)$, which assigns a single number to express the desirability of a state. The expected utility of an action given the evidence, $EU(a | \boldsymbol{e})$, is just the average utility value that maximizes the agent's expected utility:
\begin{equation*}
  EU(a | \boldsymbol{e}) 
  = \sum_{s'} P(\text{RESULT}(a) = s'|a,\boldsymbol{e})U(s')
\end{equation*}

The principle of {\bf maximum expected utility} (MEU) says that a rational agent should choose the action that maximizes the agent's expected utility:
\begin{equation*}
  \text{action} = \argmax_a EU(a|\boldsymbol{e})
\end{equation*}

2. Constraints on rational preference \\

Notation to describe an agent's preferences:
\begin{itemize}
  \item $A \succ B$ the agent prefers $A$ over $B$
  \item $A \sim B$ the agent is indifferent between $A$ and $B$
  \item $A \succcurlyeq B$ the agent prefers $A$ over $B$ or is indifferent between them
\end{itemize}

A lottery $L$ with pssible outcomes $S_1,...,S_n$ that occur with probabilities $p_1,...,p_n$ is written
\begin{equation*}
  L = [p_1, S_1; p_2, S_2; ..., p_n,S_n]
\end{equation*}

Six constraints that any reasonable preference relation should obey

\begin{itemize}
  \item Orderability
    \begin{equation*}
      \text{Exactly one of } (A \succ B), (B \succ A), or  (A \sim B)
      \text{ holds.}
    \end{equation*}
  \item Transitivity
    \begin{equation*}
      (A \succ B) \land (B \succ C) \Rightarrow (A \succ C)
    \end{equation*}
  \item Continuity
    \begin{equation*}
      (A \succ B \succ C) \Rightarrow \exists p [p, A; 1-p,C] \sim B.
    \end{equation*}
  \item Substitutability
    \begin{equation*}
      A \sim B \Rightarrow [p,A; 1-p,C] \sim [p,V; 1-p,C].
    \end{equation*}
    This also holds if we substitute $\succ$ for $\sim$ in this axiom.
  \item Montonicity
    \begin{equation*}
      A \succ B \Rightarrow
      (p > q \Leftrightarrow [p,A;1-p,B] \succ [q,A;1-q,B]).
    \end{equation*}
  \item Decomposability
    \begin{equation*}
      [p, A; 1-p, [q, B; 1-q, C]] \sim [p, A; (1-p)q, B; (1-p)(1-q), C].
    \end{equation*}
\end{itemize}

3. Preferences lead to utility

\begin{itemize}
  \item Existence of Utility Function
    \begin{eqnarray*}
      U(A) > U(B) \Rightarrow A \succ B \\
      U(A) = U(B) \Rightarrow A \sim B
    \end{eqnarray*}
  \item Expected Utility of a Lottery
    \begin{eqnarray*}
      U([p_1,S_1;...;p_n,S_n]) = \sum_i p_i U(S_i).
    \end{eqnarray*}
\end{itemize}

The preceding theorems establish that a utility function exists for any rational agent, but they do not establish tat it is unique.
\begin{equation*}
  U'(S) = aU(S) + b,
\end{equation*}

where $a$ and $b$ are constants and $a>0$. \\

As in game-playing, in a deterministic environment an agent just needs a preference ranking on states-the numbers don't matter. This is called a value function or ordinal utility function.

\section{Making Complex Decisions}

1. Markov decision process (MDP) \\

MDP is a sequential decision problem for a fully observable, stochastic environment with a Markovian transition model and additive reward. \\

It consists of a set of states (with initial state $s_0$); a set ACTIONS($s$) of actions in each state; a transition model $P(s'| s, a)$; and a reward function $R(s)$. \\

2. Policy \\

A solution is called a policy. $\pi(s)$ is the action recommended by the policy $\pi$ for state $s$. \\

An optimal policy is a policy that yields the highest expected utility. A proper policy is a policy that is guaranteed to reach a terminal state. \\

3. Finite horizon \\

There is a fixed time $N$ after which nothing matters--the game is over. Thus $U_h([s_0, s_1,...,s_{N+k}]) = U_h([s_0, s_1,...,s_{N+k}])$ for all $k>0$.  With a finite horizon, the optimal action in a given state could change over time (nonstationary). \\

4. Rewards
\begin{itemize}
\item Additive rewards: The utility of a state sequence is
  \begin{equation*}
    U_h([s_0,s_1,s_2,...]) = R(s_0)+R(s_1)+R(s_2)+...
  \end{equation*}
\item Discounted rewards: The utility of a state sequence is
  \begin{equation*}
    U_h([s_0,s_1,s_2,...])
    = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2)+...
  \end{equation*}
  where the discount factor $\gamma$ is a number between 0 and 1. The discount factor describes the preference of an agent for current rewards over future rewards. A discount factor of $gamma$ is equivalent to an interest rate of $(1 / \gamma -1)$.
\end{itemize}

5. Optimal policies and the utilities of states \\

We assume the agent is in some initial state $s$ and define $S_t$ (a random variable) to be the state the agent reaches at time $t$ when executing a particular policy $\pi$.  The probability distribution over state sequences $S_1, S_2,...,$ is determined by the initial state $s$, the policy $\pi$, and the transition model for the environment. \\

The expected utility obtained by executing $\pi$ starting in $s$ is given by
\begin{equation*}
U^{\pi}(s) = E \left[ \sum^{\infty}_{t=0} \gamma^t R(S_t) \right]
\end{equation*}

where the expectation is with respect to the probability distribution over state sequences determined by $s$ and $\pi$. We'll use $\pi^*_S$ to denote:
\begin{equation*}
\pi^*(s) = \argmax_{a \in A(s)} U^{\pi}(s')
\end{equation*}

A remarkable consequence of using discounted utilities with infinite horizons is that the optimal policy is independent of the starting state. \\

The utility function $U(s)$ allows the agent to select actions by using the principle of maximum expected utility:
\begin{equation*}
\pi^*(s) = \argmax_{a \in A(s)} \sum_{s'} P(s'|s,a) U(s')
\end{equation*}

6. The value iteration algorithm \\

The basic idea is to calculate the utility of each state and then use the state utilities to select an optimal action in each state. \\

7. The Bellman equation for utilities
\begin{eqnarray*}
U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s' | s, a) U(s')
\end{eqnarray*}

8. The Bellman update
\begin{eqnarray*}
U_{i+1}(s) \leftarrow
R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s' | s, a) U_i(s')
\end{eqnarray*}

where the update is assumed to be applied simultaneously to all the states at each iteration. \\

9. The policy iteration algorithm \\

The policy iteration algorithm alternates the following two steps, beginning from some initial policy $\pi_0$:
\begin{itemize}
\item Policy evaluation: Given a policy $\pi_i$, calculate $U_i = U^{\pi}$, the utility of each state if $\pi_i$ were to be executed.
\item Policy improvement: Calculate a new MEU policy $\pi_{i+1}$, using one-step look-ahead based on $U_i$.
\end{itemize}

The algorithm terminates when the policy improvement step yields no change in the utilities. \\

For small state spaces, policy evaluation using exact solution methods is often the most efficient approach. For large state spaces, $O(n^3)$ time might be prohibitive. \\

10. The simplified Bellman update
\begin{eqnarray*}
U_{i+1}(s) = R(s) + \gamma \sum_{s'} P(s' | s, \pi(s)) U_i(s')
\end{eqnarray*}

The resulting algorithm is called modified policy iteration. It is often more efficient than standard policy iteration or value iteration.

\end{document}
