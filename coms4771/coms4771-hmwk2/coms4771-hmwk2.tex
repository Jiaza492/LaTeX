\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage[dvips]{graphicx}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Machine Learning Homework 2}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\maketitle

\setlength{\parindent}{0in}

\section*{Introduction}

For how to run the MATLAB code, please refer to file ``Shell''.

\section*{Problem 1}

The VC dimension is 4. \\

In Fig-\ref{fig:p1}, we draw an axis-aligned rectangle such that for each edge, there is one and only one point is in that edge. For each point, if the output for this point is 1, we move the corresponding edge outwards to make the point inside the rectangle; if the output is 0, then we move the corresponding edge inwards to make the point outside the rectangle. Note that this does not affect the rest of the points. As a result, using this method, we can always find one rectangle that satisfy the output of the four points. Fig-\ref{fig:p1_2} shows the rectangle to satisfy situation with only $p_2$ is 1. \\

However, axis aligned rectangle can not shatter 5 points. Because one rectangle only has 4 edges. Though we can satisfy any output of 4 points, there is no way we can satisfy the last point, which is in the middle of the previous 4 points. But if we are able to choose whether the inside and outside can be positive or negative, then it means that we can satisfy the last point in the middle. That is, when the last point is +1, we declare the inside is positive, and then adjust the four edges to satisfy the other 4 points. When the last point is -1, we declare the inside is negative, and then adjust the four edges to satisfy the other 4 points. In this way, we can satisfy at least 5 points.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7 \textwidth]{p1}
  \caption{P1 original rectangle \label{fig:p1}}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7 \textwidth]{p1_2}
  \caption{P1 Example \label{fig:p1_2}}
\end{figure}

\section*{Problem 2}

We first prove Mercer's theorem in one direction. Assume $\phi(x) \in R^d$, and let $(\phi(x))_k$ denote the $k$th element of vector $\phi(x)$. We have
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \phi^T(x_i) \phi(x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \left(
  c_i c_j \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k 
  \right) \\
  &=& \sum_{k=1}^d \left( \sum_{i=1}^n c_i (\phi(x_i))_k
  \sum_{j=1}^n c_j (\phi(x_j))_k \right) \\
  &=& \sum_{k=1}^d \left( \sum_{i=1}^n c_i (\phi(x_i))_k \right)^2 \ge 0
\end{eqnarray*}

So the kernel matrix \textbf{K} is positive semi-definite. \\

a) $k(x, \tilde{x}) = \alpha k_1(x, \tilde{x}) + \beta k_2(x, \tilde{x})$
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( \alpha k_1(x_i,x_j) + \beta k_2(x_i,x_j) \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \alpha k_1(x_i,x_j)
  + \sum_{i=1}^n \sum_{j=1}^n c_i c_j \beta k_2(x_i,x_j) \\
  &=& \alpha \textbf{c}^T \textbf{K}_1 \textbf{c}
  + \beta \textbf{c}^T \textbf{K}_2 \textbf{c} \ge 0
\end{eqnarray*}

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel. \\

b) $k(x, \tilde{x}) = k_1(x, \tilde{x}) \times k_2(x, \tilde{x})$
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( k_1(x_i,x_j) \times k_2(x_i,x_j) \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( \phi_1^T(x_i) \phi_1(x_j) \times \phi_2^T(x_i) \phi_2(x_j) \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( \sum_{p=1}^{d_1} (\phi_1(x_i))_p (\phi_1(x_j))_p 
  \times \sum_{q=1}^{d_2} (\phi_2(x_i))_q (\phi_2(x_j))_q \right) \\
  &=& \sum_{p=1}^{d_1} \sum_{q=1}^{d_2} \left(
  \sum_{i=1}^n c_i (\phi_1(x_i))_p (\phi_2(x_i))_q
  \times \sum_{j=1}^n c_j (\phi_1(x_j))_p (\phi_2(x_j))_q
  \right) \\
  &=& \sum_{p=1}^{d_1} \sum_{q=1}^{d_2}
  \left( \sum_{i=1}^n c_i (\phi_1(x_i))_p (\phi_2(x_i))_q \right)^2 \ge 0
\end{eqnarray*}

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel. \\

c) $k(x, \tilde{x}) = f(k_1(x, \tilde{x}))$ \\

Assume $f(x) = a_0 + a_1 + a_2 x^2 + \dots + a_d x^d$, where $a_i > 0, \; i = 0,1,2,\dots, d$.
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j f(k_1(x_i,x_j)) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \left( c_i c_j \sum_{m = 1}^d a_m
  \left( k_1(x_i,x_j) \right)^m \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d c_i c_j a_m
  \left( \phi_1^T(x_i) \phi_1(x_j) \right)^m \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d c_i c_j a_m
  \left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right)^m \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d c_i c_j a_m
  \left( \sum_{k_1=1}^d (\phi(x_i))_{k_1} (\phi(x_j))_{k_1} \right) \dots
  \left( \sum_{k_m=1}^d (\phi(x_i))_{k_m} (\phi(x_j))_{k_m} \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d
  \left( \sum_{k_1=1}^d \dots \sum_{k_m=1}^d
  \left( c_i c_j a_m (\phi(x_i))_{k_1} (\phi(x_j))_{k_1} 
  \dots (\phi(x_i))_{k_m} (\phi(x_j))_{k_m} \right) \right) \\
  &=& \sum_{m = 1}^d a_m \left( \sum_{k_1=1}^d \dots \sum_{k_m=1}^d 
  \left( \sum_{i=1}^n c_i (\phi(x_i))_{k_1} \dots (\phi(x_i))_{k_m} \right)
  \left( \sum_{j=1}^n c_j (\phi(x_j))_{k_1} \dots (\phi(x_j))_{k_m} \right)
  \right) \\
  &=& \sum_{m = 1}^d a_m \left( \sum_{k_1=1}^d \dots \sum_{k_m=1}^d 
  \left( \sum_{i=1}^n c_i (\phi(x_i))_{k_1} \dots (\phi(x_i))_{k_m} \right)^2
  \right) \ge 0
\end{eqnarray*}

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel. \\

d) $k(x, \tilde{x}) = \exp (k_1(x, \tilde{x}))$ \\

\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \exp (k_1(x_i,x_j)) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \exp (\phi_1^T(x_i) \phi_1(x_j)) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j
  \exp \left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \sum_{m=0}^{+\infty}
  \frac {\left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right)^m}{m!} \\
  &=& \sum_{m=0}^{+\infty} \frac {1}{m!} \sum_{i=1}^n \sum_{j=1}^n c_i c_j
  \left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right)^m \\
  &=& \sum_{m=0}^{+\infty} \frac {1}{m!} \left(
  \sum_{k_1=1}^d \dots \sum_{k_m=1}^d \left(
  \sum_{i=1}^n c_i (\phi(x_i))_{k_1} \dots (\phi(x_i))_{k_m}
  \right)^2 \right) \ge 0 \\
\end{eqnarray*}

Note that the last equation uses the result from part $(c)$. \\

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel.

\section*{Problem 3}

Let
\begin{equation*}
  \textbf{X} =
  \begin{bmatrix}
    - \phi^T(x_1)- \\
    - \phi^T(x_2)- \\
    \vdots \\
    -\phi^T(x_n)- 
  \end{bmatrix}
\end{equation*}

Rewrite $L(\textbf{w})$
\begin{eqnarray*}
  L(\textbf{w})
  &=& \sum_{i=1}^n (\textbf{w}^T \phi(\textbf{x}_i) - y_i)^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \sum_{i=1}^n (y_i - \textbf{w}^T \phi(\textbf{x}_i))^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert 
  \begin{matrix}
    \begin{bmatrix}
      y_1 \\
      y_2 \\
      \vdots \\
      y_n
    \end{bmatrix}
    -
    \begin{bmatrix}
    - \phi^T(\textbf{x}_1)- \\
    - \phi^T(\textbf{x}_2)- \\
    \vdots \\
    -\phi^T(\textbf{x}_n)- 
    \end{bmatrix}
    \textbf{w}
  \end{matrix}
  \right\rVert^2 + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert \textbf{y} - \textbf{Xw} \right\rVert^2
  + \lambda \textbf{w}^T \textbf{w}
\end{eqnarray*}
Solving gradient $=0$
\begin{eqnarray*}
  \nabla_{\textbf{w}} {L} &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left\lVert \textbf{y} - \textbf{Xw} \right\rVert^2 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  \nabla_{\textbf{w}} {\left( \left(\textbf{y} - \textbf{Xw} \right)^T
      \left(\textbf{y} - \textbf{Xw} \right) 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left(\textbf{y}^T - \textbf{w}^T \textbf{X}^T \right)
      \left(\textbf{y} - \textbf{Xw} \right) 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \textbf{y}^T \textbf{y} - 2 \textbf{y}^T \textbf{X} \textbf{w}
      + \textbf{w}^T \textbf{X}^T \textbf{Xw} 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  - 2 \textbf{X}^T \textbf{y} + 2 \textbf{X}^T \textbf{Xw} 
  + 2 \lambda \textbf{w} &=& 0 \\
  \textbf{X}^T \textbf{Xw} + \lambda \textbf{w}
  &=& \textbf{X}^T \textbf{y} \\
  \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right) \textbf{w}
  &=& \textbf{X}^T \textbf{y} \\
  \textbf{w} &=&
  \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right)^{-1}
  \textbf{X}^T \textbf{y}
\end{eqnarray*}
Since
\begin{eqnarray*}
  \textbf{X}^T \textbf{y} &=&
  \begin{bmatrix}
    | & | & \dots  & | \\
    \phi(\textbf{x}_1) & \phi(\textbf{x}_2) & \dots & \phi(\textbf{x}_n) \\
    | & | & \dots  & |
  \end{bmatrix}
  \begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
  \end{bmatrix} \\
  &=& \sum_{i=1}^n \phi(x_i) y_i
\end{eqnarray*}
We have
\begin{eqnarray*}
  \textbf{w}
  &=& \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right)^{-1}
  \textbf{X}^T \textbf{y} \\
  &=& \sum_{i=1}^n
  \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right)^{-1}
  \phi(\textbf{x}_i) y_i \\
  &=& \sum_{i=1}^n y_i
  \left( \left( \textbf{X} \textbf{X}^T \right)^T 
  + \lambda \textbf{I} \right)^{-1} \phi(\textbf{x}_i) \\
  &=& \sum_{i=1}^n y_i 
  \left( \textbf{K}^T + \lambda \textbf{I} \right)^{-1} \phi(\textbf{x}_i) \\ 
  &=& \sum_{i=1}^n y_i 
  \left( \textbf{K} + \lambda \textbf{I} \right)^{-1} \phi(\textbf{x}_i)
\end{eqnarray*}

Intuitively, this shows that \textbf{w} lives in the span of feature maps: $\textbf{w} = \sum_{i=1}^n \alpha_i \phi(\textbf{x}_i)$. \\

Let
\begin{eqnarray*}
  \alpha &=&
  \begin{bmatrix}
    \alpha_1 \\
    \alpha_2 \\
    \vdots \\
    \alpha_n
  \end{bmatrix}
\end{eqnarray*}
Then
\begin{eqnarray*}
  \textbf{w}
  &=& \sum_{i=1}^n \alpha_i \phi(\textbf{x}_i) \\
  &=& \textbf{X}^T \alpha
\end{eqnarray*}
We have
\begin{eqnarray*}
  L(\textbf{w})
  &=& \sum_{i=1}^n \left(\textbf{w}^T \phi(\textbf{x}_i) - y_i \right)^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \sum_{i=1}^n (y_i - \textbf{w}^T \phi(\textbf{x}_i))^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert 
  \begin{matrix}
    \begin{bmatrix}
      y_1 \\
      y_2 \\
      \vdots \\
      y_n
    \end{bmatrix}
    -
    \begin{bmatrix}
    - \phi^T(\textbf{x}_1)- \\
    - \phi^T(\textbf{x}_2)- \\
    \vdots \\
    -\phi^T(\textbf{x}_n)- 
    \end{bmatrix}
    \textbf{w}
  \end{matrix}
  \right\rVert^2 + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert \textbf{y} - \textbf{Xw} \right\rVert^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert \textbf{y} - \textbf{X} \textbf{X}^T \alpha \right\rVert^2
  + \lambda \alpha^T \textbf{X}^T \textbf{X} \alpha \\
  &=& \left\lVert \textbf{y} - \textbf{K} \alpha \right\rVert^2
  + \lambda \alpha^T \textbf{K}^T \alpha
\end{eqnarray*}
Solving gradient $=0$
\begin{eqnarray*}
  \nabla_{\textbf{w}} {L}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left\lVert \textbf{y} - \textbf{K} \alpha \right\rVert^2 
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( \left(\textbf{y} - \textbf{K} \alpha \right)^T
      \left(\textbf{y} - \textbf{K} \alpha \right)
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left(\textbf{y}^T - \alpha^T \textbf{K}^T \right)
      \left(\textbf{y} - \textbf{K} \alpha \right) 
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \textbf{y}^T \textbf{y} - 2 \textbf{y}^T \textbf{K} \alpha
      + \alpha^T \textbf{K}^T \textbf{K} \alpha 
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  - 2 \textbf{K}^T \textbf{y} + 2 \textbf{K}^T \textbf{K} \alpha 
  + 2 \lambda \textbf{K}^T \alpha
  &=& 0 \\
  \textbf{K}^T \textbf{K} \alpha + \lambda \textbf{K} \alpha
  &=& \textbf{K}^T \textbf{y} \\
  \textbf{K}^{-T} \textbf{K}^T \textbf{K} \alpha
  + \lambda \textbf{K}^{-T} \textbf{K} \alpha
  &=& \textbf{K}^{-T} \textbf{K}^T \textbf{y} \\
  \textbf{K} \alpha + \lambda \alpha
  &=& \textbf{y} \\
  \left( \textbf{K} + \lambda \textbf{I} \right) \alpha
  &=& \textbf{y} \\
  \alpha &=& \left( \textbf{K} + \lambda \textbf{I} \right)^{-1} \textbf{y}
\end{eqnarray*}

\section*{Problem 4}

I use the first half of the data to train the SVM and the other half of the data to test performance. Since the training set and the test set for every run is the same, this guarantees the results are comparable with each other. \\

Having run several experiments, I found that linear, polynomial and RBF kernels can all achieve 0 classification error given specific parameters. But other performance indicators, like margins, number of support vectors, varies. \\

For linear kernel, I tried several runs with different C parameters. The results in shown in Table-\ref{tab:linear}. Based on Table-\ref{tab:linear}, I find that as parameter $C$ decreases, the margin and the number of support vectors increase, but the error tends to increase. When $C$ is set to $0.005$, there are 2 examples being mis-classified. This indicates that a  higher upper bound $C$, which means more freedom to solve the optimization problem, yields a better result. Also, a large number of support vectors indicates a bad fit. \\

\begin{table}[ht!]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      $C$	  &  0.005 &  0.05  &   0.5  &    1   &  10    \\ \hline
      Error       &    2   &   0    &    0   &    0   &   0    \\ \hline
      Margins     & 5.4551 & 3.0136 & 2.7453 & 2.7453 & 2.7453 \\ \hline
      Support Vectors & 45 &  18    &   13   &   13   &   13   \\ \hline
    \end{tabular}
  \end{center}
  \caption{Results of Linear Kernels with different $C$ value 
    \label{tab:linear}}
\end{table}

For polynomial kernel, I tried several runs with different degrees. The results in shown in Table-\ref{tab:poly}. As we can see, when the degree is greater or equal to 3, the number of support vectors is 0. This is because all the Lagrange multipliers are less than the tolerance $\epsilon$. This may indicate overfitting. \\

\begin{table}[ht!]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      max. degree &    1   &   2    &    3   &    4   &   5    \\ \hline
      Error       &    0   &   0    &    0   &    0   &   0    \\ \hline
      Margins     & 2.7031 & 49.549 & 1079.3 & 19114  & 250679 \\ \hline
      Support Vectors & 14 &  16    &    0   &    0   &   0    \\ \hline
    \end{tabular}
  \end{center}
  \caption{Results of polynomial kernels with different degree 
    \label{tab:poly}}
\end{table}

For RBF kernel, I tried several runs with different C parameters. The results in shown in Table-\ref{tab:rbf}. When $C=6$, we achieve the minimum number of support vectors. This indicate $C=6$ is a better model for models with other $C$ values shown in Table-\ref{tab:rbf}. \\

\begin{table}[ht!]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      $C$	  &    1   &   5    &    6   &   10   &  100   \\ \hline
      Error       &    0   &   0    &    0   &    0   &   43   \\ \hline
      Margins     & 0.3326 & 0.5313 & 0.4849 & 0.3780 & 1.2687 \\ \hline
      Support Vectors & 72 &  36    &   29   &   38   &   72   \\ \hline
    \end{tabular}
  \end{center}
  \caption{Results of RBF kernels with different $C$ value 
    \label{tab:rbf}}
\end{table}

To sum up, the number of support vectors is a good indicator for the goodness of fitness. When the number of support vectors is too large, it indicates the model suffers from under-fitting. When the number of support vectors is too small, the model may be overfitting.

\end{document}
