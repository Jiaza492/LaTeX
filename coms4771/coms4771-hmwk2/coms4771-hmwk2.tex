\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage[dvips]{graphicx}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Machine Learning Homework 2}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\maketitle

\setlength{\parindent}{0in}

\section*{Problem 1}

The VC dimension is 4. \\

In Fig-\ref{fig:p1}, we draw an axis-aligned rectangle such that for each edge, there is one and only one point is in that edge. For each point, if the output for this point is 1, we move the corresponding edge outwards to make the point inside the rectangle; if the output is 0, then we move the corresponding edge inwards to make the point outside the rectangle. Note that this does not affect the rest of the points. As a result, using this method, we can always find one rectangle that satisfy the output of the four points. Fig-\ref{fig:p1_2} shows the rectangle to satisfy situation with only $p_2$ is 1. \\

However, axis aligned rectangle can not shatter 5 points. Because one rectangle only has 4 edges. Though we can satisfy any output of 4 points, there is no way we can satisfy the last 1 point.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7 \textwidth]{p1}
  \caption{P1 original rectangle \label{fig:p1}}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7 \textwidth]{p1_2}
  \caption{P1 Example \label{fig:p1_2}}
\end{figure}

\section*{Problem 2}

We first prove Mercer's theorem in one direction. Assume $\phi(x) \in R^d$, and let $(\phi(x))_k$ denote the $k$th element of vector $\phi(x)$. We have
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \phi^T(x_i) \phi(x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \left(
  c_i c_j \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k 
  \right) \\
  &=& \sum_{k=1}^d \left( \sum_{i=1}^n c_i (\phi(x_i))_k
  \sum_{j=1}^n c_j (\phi(x_j))_k \right) \\
  &=& \sum_{k=1}^d \left( \sum_{i=1}^n c_i (\phi(x_i))_k \right)^2 \ge 0
\end{eqnarray*}

So the kernel matrix \textbf{K} is positive semi-definite. \\

a) $k(x, \tilde{x}) = \alpha k_1(x, \tilde{x}) + \beta k_2(x, \tilde{x})$
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( \alpha k_1(x_i,x_j) + \beta k_2(x_i,x_j) \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \alpha k_1(x_i,x_j)
  + \sum_{i=1}^n \sum_{j=1}^n c_i c_j \beta k_2(x_i,x_j) \\
  &=& \alpha \textbf{c}^T \textbf{K}_1 \textbf{c}
  + \beta \textbf{c}^T \textbf{K}_2 \textbf{c} \ge 0
\end{eqnarray*}

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel. \\

b) $k(x, \tilde{x}) = k_1(x, \tilde{x}) \times k_2(x, \tilde{x})$
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( k_1(x_i,x_j) \times k_2(x_i,x_j) \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( \phi_1^T(x_i) \phi_1(x_j) \times \phi_2^T(x_i) \phi_2(x_j) \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j 
  \left( \sum_{p=1}^{d_1} (\phi_1(x_i))_p (\phi_1(x_j))_p 
  \times \sum_{q=1}^{d_2} (\phi_2(x_i))_q (\phi_2(x_j))_q \right) \\
  &=& \sum_{p=1}^{d_1} \sum_{q=1}^{d_2} \left(
  \sum_{i=1}^n c_i (\phi_1(x_i))_p (\phi_2(x_i))_q
  \times \sum_{j=1}^n c_j (\phi_1(x_j))_p (\phi_2(x_j))_q
  \right) \\
  &=& \sum_{p=1}^{d_1} \sum_{q=1}^{d_2}
  \left( \sum_{i=1}^n c_i (\phi_1(x_i))_p (\phi_2(x_i))_q \right)^2 \ge 0
\end{eqnarray*}

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel. \\

c) $k(x, \tilde{x}) = f(k_1(x, \tilde{x}))$ \\

Assume $f(x) = a_0 + a_1 + a_2 x^2 + \dots + a_d x^d$, where $a_i > 0, \; i = 0,1,2,\dots, d$.
\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j f(k_1(x_i,x_j)) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \left( c_i c_j \sum_{m = 1}^d a_m
  \left( k_1(x_i,x_j) \right)^m \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d c_i c_j a_m
  \left( \phi_1^T(x_i) \phi_1(x_j) \right)^m \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d c_i c_j a_m
  \left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right)^m \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d c_i c_j a_m
  \left( \sum_{k_1=1}^d (\phi(x_i))_{k_1} (\phi(x_j))_{k_1} \right) \dots
  \left( \sum_{k_m=1}^d (\phi(x_i))_{k_m} (\phi(x_j))_{k_m} \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{m = 1}^d
  \left( \sum_{k_1=1}^d \dots \sum_{k_m=1}^d
  \left( c_i c_j a_m (\phi(x_i))_{k_1} (\phi(x_j))_{k_1} 
  \dots (\phi(x_i))_{k_m} (\phi(x_j))_{k_m} \right) \right) \\
  &=& \sum_{m = 1}^d a_m \left( \sum_{k_1=1}^d \dots \sum_{k_m=1}^d 
  \left( \sum_{i=1}^n c_i (\phi(x_i))_{k_1} \dots (\phi(x_i))_{k_m} \right)
  \left( \sum_{j=1}^n c_j (\phi(x_j))_{k_1} \dots (\phi(x_j))_{k_m} \right)
  \right) \\
  &=& \sum_{m = 1}^d a_m \left( \sum_{k_1=1}^d \dots \sum_{k_m=1}^d 
  \left( \sum_{i=1}^n c_i (\phi(x_i))_{k_1} \dots (\phi(x_i))_{k_m} \right)^2
  \right) \ge 0
\end{eqnarray*}

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel. \\

d) $k(x, \tilde{x}) = \exp (k_1(x, \tilde{x}))$ \\

\begin{eqnarray*}
  \textbf{c}^T \textbf{Kc}
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \exp (k_1(x_i,x_j)) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \exp (\phi_1^T(x_i) \phi_1(x_j)) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j
  \exp \left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right) \\
  &=& \sum_{i=1}^n \sum_{j=1}^n c_i c_j \sum_{m=0}^{+\infty}
  \frac {\left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right)^m}{m!} \\
  &=& \sum_{m=0}^{+\infty} \frac {1}{m!} \sum_{i=1}^n \sum_{j=1}^n c_i c_j
  \left( \sum_{k=1}^d (\phi(x_i))_k (\phi(x_j))_k \right)^m \\
  &=& \sum_{m=0}^{+\infty} \frac {1}{m!} \left(
  \sum_{k_1=1}^d \dots \sum_{k_m=1}^d \left(
  \sum_{i=1}^n c_i (\phi(x_i))_{k_1} \dots (\phi(x_i))_{k_m}
  \right)^2 \right) \ge 0 \\
\end{eqnarray*}

Note that the last equation uses the result from part $(c)$. \\

So \textbf{K} is positive semi-definite. By Mercer's theorem, $k(x, \tilde{x})$ is a Mercer kernel.

\section*{Problem 3}

Let
\begin{equation*}
  \textbf{X} =
  \begin{bmatrix}
    - \phi^T(x_1)- \\
    - \phi^T(x_2)- \\
    \vdots \\
    -\phi^T(x_n)- 
  \end{bmatrix}
\end{equation*}

Rewrite $L(\textbf{w})$
\begin{eqnarray*}
  L(\textbf{w})
  &=& \sum_{i=1}^n (\textbf{w}^T \phi(\textbf{x}_i) - y_i)^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \sum_{i=1}^n (y_i - \textbf{w}^T \phi(\textbf{x}_i))^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert 
  \begin{matrix}
    \begin{bmatrix}
      y_1 \\
      y_2 \\
      \vdots \\
      y_n
    \end{bmatrix}
    -
    \begin{bmatrix}
    - \phi^T(\textbf{x}_1)- \\
    - \phi^T(\textbf{x}_2)- \\
    \vdots \\
    -\phi^T(\textbf{x}_n)- 
    \end{bmatrix}
    \textbf{w}
  \end{matrix}
  \right\rVert^2 + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert \textbf{y} - \textbf{Xw} \right\rVert^2
  + \lambda \textbf{w}^T \textbf{w}
\end{eqnarray*}
Solving gradient $=0$
\begin{eqnarray*}
  \nabla_{\textbf{w}} {L} &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left\lVert \textbf{y} - \textbf{Xw} \right\rVert^2 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  \nabla_{\textbf{w}} {\left( \left(\textbf{y} - \textbf{Xw} \right)^T
      \left(\textbf{y} - \textbf{Xw} \right) 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left(\textbf{y}^T - \textbf{w}^T \textbf{X}^T \right)
      \left(\textbf{y} - \textbf{Xw} \right) 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \textbf{y}^T \textbf{y} - 2 \textbf{y}^T \textbf{X} \textbf{w}
      + \textbf{w}^T \textbf{X}^T \textbf{Xw} 
      + \lambda \textbf{w}^T \textbf{w} \right)} &=& 0 \\
  - 2 \textbf{X}^T \textbf{y} + 2 \textbf{X}^T \textbf{Xw} 
  + 2 \lambda \textbf{w} &=& 0 \\
  \textbf{X}^T \textbf{Xw} + \lambda \textbf{w}
  &=& \textbf{X}^T \textbf{y} \\
  \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right) \textbf{w}
  &=& \textbf{X}^T \textbf{y} \\
  \textbf{w} &=&
  \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right)^{-1}
  \textbf{X}^T \textbf{y}
\end{eqnarray*}
Since
\begin{eqnarray*}
  \textbf{X}^T \textbf{y} &=&
  \begin{bmatrix}
    | & | & \dots  & | \\
    \phi(\textbf{x}_1) & \phi(\textbf{x}_2) & \dots & \phi(\textbf{x}_n) \\
    | & | & \dots  & |
  \end{bmatrix}
  \begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
  \end{bmatrix} \\
  &=& \sum_{i=1}^n \phi(x_i) y_i
\end{eqnarray*}
We have
\begin{eqnarray*}
  \textbf{w}
  &=& \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right)^{-1}
  \textbf{X}^T \textbf{y} \\
  &=& \sum_{i=1}^n
  \left( \textbf{X}^T \textbf{X} + \lambda \textbf{I} \right)^{-1}
  \phi(\textbf{x}_i) y_i \\
  &=& \sum_{i=1}^n y_i
  \left( \left( \textbf{X} \textbf{X}^T \right)^T 
  + \lambda \textbf{I} \right)^{-1} \phi(\textbf{x}_i) \\
  &=& \sum_{i=1}^n y_i 
  \left( \textbf{K}^T + \lambda \textbf{I} \right)^{-1} \phi(\textbf{x}_i) \\ 
  &=& \sum_{i=1}^n y_i 
  \left( \textbf{K} + \lambda \textbf{I} \right)^{-1} \phi(\textbf{x}_i)
\end{eqnarray*}

Intuitively, this shows that \textbf{w} lives in the span of feature maps: $\textbf{w} = \sum_{i=1}^n \alpha_i \phi(\textbf{x}_i)$. \\

Let
\begin{eqnarray*}
  \alpha &=&
  \begin{bmatrix}
    \alpha_1 \\
    \alpha_2 \\
    \vdots \\
    \alpha_n
  \end{bmatrix}
\end{eqnarray*}
Then
\begin{eqnarray*}
  \textbf{w}
  &=& \sum_{i=1}^n \alpha_i \phi(\textbf{x}_i) \\
  &=& \textbf{X}^T \alpha
\end{eqnarray*}
We have
\begin{eqnarray*}
  L(\textbf{w})
  &=& \sum_{i=1}^n \left(\textbf{w}^T \phi(\textbf{x}_i) - y_i \right)^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \sum_{i=1}^n (y_i - \textbf{w}^T \phi(\textbf{x}_i))^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert 
  \begin{matrix}
    \begin{bmatrix}
      y_1 \\
      y_2 \\
      \vdots \\
      y_n
    \end{bmatrix}
    -
    \begin{bmatrix}
    - \phi^T(\textbf{x}_1)- \\
    - \phi^T(\textbf{x}_2)- \\
    \vdots \\
    -\phi^T(\textbf{x}_n)- 
    \end{bmatrix}
    \textbf{w}
  \end{matrix}
  \right\rVert^2 + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert \textbf{y} - \textbf{Xw} \right\rVert^2
  + \lambda \textbf{w}^T \textbf{w} \\
  &=& \left\lVert \textbf{y} - \textbf{X} \textbf{X}^T \alpha \right\rVert^2
  + \lambda \alpha^T \textbf{X}^T \textbf{X} \alpha \\
  &=& \left\lVert \textbf{y} - \textbf{K} \alpha \right\rVert^2
  + \lambda \alpha^T \textbf{K}^T \alpha
\end{eqnarray*}
Solving gradient $=0$
\begin{eqnarray*}
  \nabla_{\textbf{w}} {L}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left\lVert \textbf{y} - \textbf{K} \alpha \right\rVert^2 
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( \left(\textbf{y} - \textbf{K} \alpha \right)^T
      \left(\textbf{y} - \textbf{K} \alpha \right)
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \left(\textbf{y}^T - \alpha^T \textbf{K}^T \right)
      \left(\textbf{y} - \textbf{K} \alpha \right) 
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  \nabla_{\textbf{w}} {\left( 
      \textbf{y}^T \textbf{y} - 2 \textbf{y}^T \textbf{K} \alpha
      + \alpha^T \textbf{K}^T \textbf{K} \alpha 
      + \lambda \alpha^T \textbf{K}^T \alpha \right)}
  &=& 0 \\
  - 2 \textbf{K}^T \textbf{y} + 2 \textbf{K}^T \textbf{K} \alpha 
  + 2 \lambda \textbf{K}^T \alpha
  &=& 0 \\
  \textbf{K}^T \textbf{K} \alpha + \lambda \textbf{K} \alpha
  &=& \textbf{K}^T \textbf{y} \\
  \textbf{K}^{-T} \textbf{K}^T \textbf{K} \alpha
  + \lambda \textbf{K}^{-T} \textbf{K} \alpha
  &=& \textbf{K}^{-T} \textbf{K}^T \textbf{y} \\
  \textbf{K} \alpha + \lambda \alpha
  &=& \textbf{y} \\
  \left( \textbf{K} + \lambda \textbf{I} \right) \alpha
  &=& \textbf{y} \\
  \alpha &=& \left( \textbf{K} + \lambda \textbf{I} \right)^{-1} \textbf{y}
\end{eqnarray*}

\section*{Problem 4}




\end{document}
