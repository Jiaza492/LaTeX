\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{law}[theorem]{Law}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Introduction to Probability and Statistics Note}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\setcounter{tocdepth}{5} %to make it appears in TOC
\setcounter{secnumdepth}{5} %to make it numbered

\maketitle

\tableofcontents

\setlength{\parindent}{0in}

\section{Introduction to Statistics}

A systematic collection of data on the population and the economy was begun in the Italian city-state of Venice and Florence during the Renaissance. The term \emph{statistics}, derived from the word \emph{state}, was used to refer to a collection of facts of interest to the state.

\section{Descriptive Statistics}

\subsection{Describing Data Sets}
\begin{enumerate}
  \item Frequency tables and graphs
  \item Relative frequency tables and graphs
  \item Histograms
  \item Stem and leaf plots
\end{enumerate}

\subsection{Summarizing Data Sets}

\begin{definition}
  The \textbf{sample mean}, designated by $\hat{x}$, is defined by
  \begin{equation*}
    \hat{x} = \sum_{i=1}^n x_i / n
  \end{equation*}
\end{definition}

\begin{definition}
  Order the values of a data set of size $n$ from smallest to largest. If $n$ is odd, the \textbf{sample median} is the value in position $(n+1)/2$; if $n$ is even, it is the average of the values in positions $n/2$ and $n/2+1$.
\end{definition}

\begin{definition}
  The \textbf{sample variance}, call it $s^2$, of the data set $x_1, \dots, x_n$ is defined by
  \begin{equation*}
    s^2 = \sum_{i=1}^n (x_i - \overline{x})^2/(n-1)
  \end{equation*}
\end{definition}

\begin{theorem}
  \begin{equation*}
    \sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n x_i^2 - n \overline{x}^2
  \end{equation*}
\end{theorem}

\begin{definition}
  The quantity $s$, defined by
  \begin{equation*}
    s = \sqrt{\sum_{i=1}^n (x_i - \overline{x})^2 / (n-1)}
  \end{equation*}
  is called the \textbf{sample standard deviation}.
\end{definition}

\begin{definition}
  The \textbf{sample 100p percentile} is that data value such that $100p$ percent of the data are less than or equal to it and $100(1-p)$ percent are greater than or equal to it. if two data values satisfy this condition, then the sample $100p$ percentile is the arithmetic average of these two values.
\end{definition}

\begin{definition}
  The sample $25$ percentile is called the \textbf{first quartile}; the sample 50 percentile is called the sample median or the \textbf{second quartile}; the sample 75 percentile is called the \textbf{third quartile}.
\end{definition}

\subsection{Chebyshev's Inequality}

\begin{theorem}[Chebyshev's Inequality]
  Let $\overline{x}$ and $s$ be the sample mean and sample standard deviation of the data set consisting of the data $x_1, \dots, x_n$, where $s > 0$. Let
  \begin{equation*}
    S_k = \{ i, 1 \le i \le n: |x_i - \overline{x}| < ks \}
  \end{equation*}
  and let $N(S_k)$ be the number of elements in the set $S_k$. Then, for any $k \ge 1$,
  \begin{equation*}
    \frac {N(S_k)}{n} \ge 1 - \frac{n-1}{nk^2} > 1 - \frac {1}{k^2}
  \end{equation*}
\end{theorem}

\begin{theorem}[The One-Sided Chebyshev Inequality]
  For $k>0$, let
  \begin{equation*}
    N(k) = \text{number of } i: x_i - \overline{x} \ge ks
  \end{equation*}
  Then we have
  \begin{equation*}
    \frac{N(k)}{n} \le \frac{1}{1+k^2}
  \end{equation*}
\end{theorem}

\subsection{Normal Data Sets}

\begin{remark} [The Empirical Rule]
  If a data set is approximately normal with sample mean $\overline{x}$ and sample standard deviation $s$, then the following statements are true.
  \begin{enumerate}
    \item Approximately 68 percent of the observations lie within
      \begin{equation*}
        \overline{x} \pm s
      \end{equation*}
    \item Approximately 95 percent of the observations lie within
      \begin{equation*}
        \overline{x} \pm 2s
      \end{equation*}
    \item Approximately 99.7 percent of the observations lie within
      \begin{equation*}
        \overline{x} \pm 3s
      \end{equation*}
  \end{enumerate}
\end{remark}

\subsection{Paired Data Sets and the Sample Correlation Coefficient}

\begin{definition}
  Let $s_x$ and $s_y$ denote, respectively, the sample standard deviations of the $x$ values and the $y$ values. The \textbf{sample correlation coefficient}, call it $r$, of the data pairs $(x_i,y_i),i=1,\dots,n$ is defined by
  \begin{eqnarray*}
    r
    &=& \frac {\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}
    {(n-1) s_x s_y} \\
    &=& \frac {\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}
    { \sqrt{\sum_{i=1}^n (x_i - \overline{x})
        \sum_{i=1}^n (y_i - \overline{y})} } \\
  \end{eqnarray*}
  When $r > 0$ we say that the sample data pairs are \textbf{positively correlated}, when $r<0$ we say that they are \textbf{negatively correlated}.
\end{definition}

Properties of $r$:
\begin{enumerate}
  \item $-1 \le r \le 1$
  \item If for constants $a$ and $b$, with $b>0$,
    \begin{equation*}
      y_i = a + b x_i, \;\; i = 1,\dots,n
    \end{equation*}
    then $r = 1$.
  \item If for constants $a$ and $b$, with $b<0$,
    \begin{equation*}
      y_i = a + b x_i, \;\; i = 1,\dots,n
    \end{equation*}
    then $r = -1$.
  \item If $r$ is the sample correlation coefficient for the data pairs $x_i, y_i,i=1,\dots,n$ then it is also the sample correlation coefficient for the data pairs
    \begin{equation*}
      a + b x_i, \;\; c + d y_i, \;\; i = 1,\dots,n
    \end{equation*}
    provided that $b$ and $d$ are both positive or both negative.
\end{enumerate}

\section{Elements of Probability}

\subsection{Sample Space and Events}

\begin{definition}
  The set of all possible outcomes of an experiment is known as the \textbf{sample space} of the experiment and is denoted by $S$.
\end{definition}

\subsection{Venn Diagrams and the Algebra of Events}

\begin{law}
  \begin{enumerate}
    \item Commutative law
      \begin{eqnarray*}
        E \cup F &=& F \cup E \\
        EF &=& FE
      \end{eqnarray*}
    \item Associative law
      \begin{eqnarray*}
        (E \cup F) \cup G &=& E \cup (F \cup G) \\
        (EF)G &=& E(FG)
      \end{eqnarray*}
    \item Distributive law
      \begin{eqnarray*}
        (E \cup F)G &=& EG \cup FG \\
        EF \cup G &=& (E \cup G)(F \cup G
      \end{eqnarray*}
    \item DeMorgan's law
      \begin{eqnarray*}
        (E \cup F)^C &=& E^CF^C \\
        (EF)^C &=& E^C \cup F^C
      \end{eqnarray*}
  \end{enumerate}
\end{law}

\subsection{Axioms of Probability}

\begin{axiom}
  $0 \le P(E) \le 1$
\end{axiom}

\begin{axiom}
  $P(S) = 1$
\end{axiom}

\begin{axiom}
  For any sequence of mutually exclusive events $E_1, E_2,\dots$(that is, events for which $E_iE_j = \emptyset$ when $i \neq j$),
  \begin{equation*}
    P \left( \bigcup_{i=1}^n E_i \right)
    = \sum_{i=1}^n P(E_i), \;\;\;\; n=1,2,\dots,\infty
  \end{equation*}
  We call $P(E)$ the probability of the event $E$.
\end{axiom}

\begin{proposition}
  \begin{equation*}
    P(E^C) = 1 - P(E)
  \end{equation*}
\end{proposition}

\begin{proposition}
  \begin{equation*}
    P(E \cup F) = P(E) + P(F) - P(EF)
  \end{equation*}
\end{proposition}

\subsection{Sample Spaces Having Equally Likely Outcomes}

\begin{theorem} [Basic Principle of Counting]
  Suppose that two experiments are to be performed. Then if experiment 1 can result in any one of $m$ possible outcomes and if, for each outcome of experiment 1, there are $n$ possible outcomes of experiment 2, then together there are $mn$ possible outcomes of the two experiments.
\end{theorem}

\begin{theorem} [Generalized Basic Principle of Counting]
  If $r$ experiments that are to be performed are such that the first one may result in any of $n_1$ possible outcomes, and if for each of these $n_1$ possible outcomes there are $n_2$ possible outcomes of the second experiment, and if for each of the possible outcomes of the third experiment, and if,..., then there are a total of $n_1 \cdot n_2 \cdots n_r$ possible outcomes of the $r$ experiments.
\end{theorem}

\begin{definition}
  We define $\binom {n}{r}$, for $r \le n$, by
  \begin{equation*}
    \binom {n}{r} = \frac {n!}{(n-r)!r!}
  \end{equation*}
  and call $\binom {n}{r}$ the number of \textbf{combinations} of $n$ objects taken $r$ at a time.
\end{definition}

\subsection{Conditional Probability}

\begin{definition}
  The probability of $E$ given that $F$ has occurred is denoted by
  \begin{equation*}
    P(E|F) = \frac {P(EF)}{P(F)}
  \end{equation*}
\end{definition}

\subsection{Bayes' Formula}

\begin{theorem} [Bayes' Theorem]
  \begin{equation*}
    P(A|B) = \frac {P(B|A)P(A)}{P(B)}
  \end{equation*}
\end{theorem}

\subsection{Independent Events}

\begin{definition}
  Two events $E$ and $F$ are said to be \textbf{independent} if
  \begin{equation*}
    P(EF) = P(E)P(F)
  \end{equation*}
  holds. Two events $E$ and $F$ that are not independent are said to be \textbf{dependent}.
\end{definition}

\begin{proposition}
  If $E$ and $F$ are independent, then so are $E$ and $F^C$.
\end{proposition}

\begin{definition}
  The three events $E,F$ and $G$ are said to be \textbf{independent} if
  \begin{eqnarray*}
    P(EFG) &=& P(E)P(F)P(G) \\
    P(EF) &=& P(E)P(F) \\
    P(EG) &=& P(E)P(G) \\
    P(FG) &=& P(F)P(G)
  \end{eqnarray*}
  holds. Two events $E$ and $F$ that are not independent are said to be \textbf{dependent}.
\end{definition}

It should be noted that if the events $E,F,G$ are independent, then $E$ will be independent of any event formed from $F$ and $G$.

\section{Random Variables and Expectation}

\subsection{Random Variables}

\begin{definition}
  A \textbf{random variable} is a mapping
  \begin{equation*}
    X: \; \Omega \rightarrow \mathbb{R}
  \end{equation*}
  that assigns a real number $X(\omega)$ to each outcome $\omega$.
\end{definition}

\subsection{Types of Random Variables}

A random variable whose set of possible values is a sequence is said to be \textbf{discrete}. For a discrete random variable $X$, we define the \textbf{probability mass function} $p(a)$ of $X$ by
\begin{equation*}
  p(a) = P\{X = a\}
\end{equation*}
Since $X$ must take on one of the values $x_i$, we have
\begin{equation*}
  \sum_{i=1}^{\infty} p(x_i) = 1
\end{equation*}
The cumulative distribution function $F$ can be expressed in terms of $p(x)$ by
\begin{equation*}
  F(a) = \sum_{\text{all } x \le a} p(x)
\end{equation*}
We say that $X$ is a \textbf{continuous} random variable if there exists a nonnegative function $f(x)$, defined for all real $x \in (-\infty, \infty)$, having the property that for any set $B$ of real numbers
\begin{equation*}
  P\{X \in B\} = \int_B f(x) dx
\end{equation*}
The function $f(x)$ is called the \textbf{probability density function} of the random variable $X$. \\

The relationship between the cumulative distribution $F(\cdot)$ and the probability density $f(\cdot)$ is expressed by
\begin{equation*}
  F(a) = P\{X \in (-\infty,a ]\} = \int_{-\infty}^a f(x) dx
\end{equation*}
Differentiating both sides yields
\begin{equation*}
  \frac {d}{da}F(a) = f(a)
\end{equation*}

\subsection{Jointly Distributed Random Variables}

\begin{definition}
  The joint cumulative probability distribution function of $X$ and $Y$ is
  \begin{equation*}
    F(x,y) = P\{ X \le x, Y \le y \}
  \end{equation*}
\end{definition}

The distribution function of $X$, $F_X$, can be obtained from the joint distribution function $F$ of $X$ and $Y$:
\begin{eqnarray*}
  F_X(x) = F(x, \infty)
\end{eqnarray*}

\subsection{Independent Random Variables}

\begin{definition}
  The random variables $X$ and $Y$ are said to be independent if for any two sets of real numbers $A$ and $B$
  \begin{equation*}
    P \{ X \in A, Y \in B \} = P\{ X \in A \} P\{ Y \in B \}
  \end{equation*}
\end{definition}

In terms of joint distribution function $F$ of $X$ and $Y$, we have that $X$ and $Y$ are independent if
\begin{equation*}
  F(a,b) = F_X(a)F_X(b) \;\;\;\; \text{for all } a,b
\end{equation*}

\subsection{Conditional Distributions}

If $X$ and $Y$ are discrete random variables, it is natural to define the conditional probability mass function of $X$ given that $Y=y$, by
\begin{eqnarray*}
  p_{X|Y}(x|y)
  &=& P\{ X = x | Y = y \} \\
  &=& \frac {P\{ X = x, Y = y \}}{P\{ Y = y \}} \\
  &=& \frac {p(x,y)}{p_Y(y)}
\end{eqnarray*}
for all values of $y$ such that $p_Y(y) > 0$.

\subsection{Expectation}

\begin{definition}
  If $X$ is a discrete random variable, then the expectation or expected value of $X$, denoted by $E[X]$, is defined by
  \begin{equation*}
    E[X] = \sum_i x_i P\{ X = x_i \}
  \end{equation*}
  If $X$ is a continuous random variable, then the expectation is defined by
  \begin{equation*}
    E[X] = \int_{-\infty}^{\infty} x f(x) dx
  \end{equation*}
\end{definition}

\subsection{Properties of the Expected Value}

\begin{proposition}
  \begin{itemize}
    \item If $X$ is a discrete random variable with probability mass function $p(x)$, then for any real-valued function $g$,
      \begin{equation*}
        E[g(x)] = \sum_x g(x)p(x)
      \end{equation*}
    \item If $X$ is a continuous random variable with probability density function $f(x)$, then for any real-valued function $g$,
      \begin{equation*}
        E[g(x)] = \inf_{-\infty}^{\infty} g(x)f(x) dx
      \end{equation*}
  \end{itemize}
\end{proposition}

\begin{corollary}
  If $a$ and $b$ are constants, then
  \begin{equation*}
    E[aX + b] = a E[X] + b
  \end{equation*}
\end{corollary}

\subsection{Expected Value of Sums of Random Variables}

If $X$ and $Y$ are random variables and $g$ is a function of two variables, then
\begin{eqnarray*}
  E[g(X,Y)]
  &=& \sum_y \sum_x g(x,y) p(x,y) \\
  &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f(x,y) dx dy
\end{eqnarray*}

\begin{remark}
  For any $n$,
  \begin{equation*}
    E[X_1+X_2+\dots+X_n] = E[X_1] + E[X_2] + \dots + E[X_n]
  \end{equation*}
\end{remark}

\subsection{Variance}

\begin{definition}
  If $X$ is a random variable with mean $\mu$, then the \textbf{variance} of $X$, denoted by $Var(X)$, is defined by
  \begin{equation*}
    Var(X) = E[(X-\mu)^2]
  \end{equation*}
\end{definition}

\begin{remark}
  \begin{equation*}
    Var(X) = E[X^2] - (E[X])^2
  \end{equation*}
\end{remark}

The quantity $\sqrt{Var(X)}$ is called the \textbf{standard deviation} of $X$. The standard deviation has the same units as does the mean.

\subsection{Covariance and Variance of Sums of Random Variables}

\begin{definition}
  The \textbf{covariance} of two random variables $X$ and $Y$, written $Cov(X,Y)$, is defined by
  \begin{equation*}
    Cov(X,Y) = E[(X-\mu_x)(Y-\mu_y)]
  \end{equation*}
  where $\mu_x$ and $\mu_y$ are the means of $X$ and $Y$, respectively.
\end{definition}

\begin{remark}
  \begin{equation*}
    Cov(X,Y) = E[XY] - E[X]E[Y]
  \end{equation*}
\end{remark}

\begin{remark}
  \begin{eqnarray*}
    Cov(X,Y) &=& Cov(Y,X) \\
    Cov(X,X) &=& Var(X) \\
    Cov(aX,Y) &=& a \: Cov(X,Y)
  \end{eqnarray*}
\end{remark}

\begin{lemma}
  \begin{equation*}
    Cov(X+Y,Z) = Cov(X,Z) + Cov(Y,Z)
  \end{equation*}
\end{lemma}

\begin{proposition}
  \begin{equation*}
    Cov \left( \sum_{i=1}^n X_i, \sum_{j=1}^m Y_j \right)
    = \sum_{i=1}^n \sum_{j=1}^m {Cov(X_i, Y_j)}
  \end{equation*}
\end{proposition}

\begin{corollary}
  \begin{equation*}
    Var(\sum_{i=1}^n X_i)
    = \sum_{i=1}^n {Var(X_i)} 
    + \sum_{i=1}^n \sum_{j = 1, j \neq i}^n {Cov(X_i, X_j)}
  \end{equation*}
\end{corollary}

\begin{theorem}
  If $X$ and $Y$ are independent random variables, then
  \begin{equation*}
    Cov(X,Y) = 0
  \end{equation*}
  and so for independent events $X_1,\dots,X_n$,
  \begin{equation*}
    Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n {Var(X_i)}
  \end{equation*}
\end{theorem}

\subsection{Moment Generating Function}

The moment generating function $\phi(t)$ of the random variable $X$ is defined for all values $t$ by
\begin{equation*}
  \phi(t) = E[e^{tX}] =
  \begin{cases}
    \sum_x e^{tx} p(x) & \text{if $X$ is discrete} \\
    \int_{-\infty}^{\infty} e^{tx} f(x) dx & \text{if $X$ is continuous}
  \end{cases}
\end{equation*}

We call $\phi(t)$ the moment generating function because all of the moments of $X$ can be obtained by successively differentiating $\phi(t)$:
\begin{equation*}
  \phi^n(0) = E[X^n], \;\;\;\; n \ge 1
\end{equation*}

\subsection{Chebyshev's Inequality and the Weak Law of Large Numbers}

\begin{proposition} [Markov's Inequality]
  If $X$ is a random variable that takes only nonnegative values, then for any value $a>0$
  \begin{equation*}
    P\{ X \ge a \} \ge \frac {E[X]}{a}
  \end{equation*}
\end{proposition}

\begin{proposition} [Chebshev's Inequality]
  If $X$ is a random variable with mean $\mu$ and variance $\sigma^2$, then for any value $k>0$
  \begin{equation*}
    P\{ |X-\mu| \ge k \} \le \frac {\sigma^2}{k^2}
  \end{equation*}
\end{proposition}

\subsection{The Weak Law of Large Numbers}

\begin{law}
  Let $X_1, X-2,\dots,$be a sequence of independent and identically distributed random variables, each having mean $E[X_i] = \mu$. Then, for any $\epsilon > 0$,
  \begin{equation*}
    P \left\{ \left|\frac{X_1 + \dots + X_n}{n} - \mu \right| 
        > \epsilon \right\} \rightarrow 0 \text{  as } n \rightarrow \infty
  \end{equation*}
\end{law}

\section{Special Random Variables}

\subsection{The Bernoulli and Binomial Random Variables}

Suppose that a trial, or an experiment, whose outcome can be classified as either a ``success'' or as a ``failure'' is performed. If we let $X=1$ when the outcome is a success and $X=0$ when it is a failure, then the probability mass function of $X$ is given by
\begin{eqnarray*}
  P\{X=0\} &=& 1-p \\
  P\{X=1\} &=& p
\end{eqnarray*}
where $p, 0 \le p \le 1$, is the probability that the trial is a ``success''.This random variable $X$ is a Bernoulli random variable. Then we have
\begin{eqnarray*}
  E[X] &=& 1 \cdot P\{X=1\} + 0 \cdot P\{X=0\} = p \\
  Var(X) &=& (1-p)^2 \cdot P\{X=1\} + (0-p)^2 \cdot P\{X=0\} = p(1-p)
\end{eqnarray*}

Suppose now that $n$ independent trials, each of which results in a ``success'' with probability $p$ and in a ``failure'' with probability $1-p$, are to be performed. If $X$ represents the number of successes that occur in the $n$ trials, then $X$ is said to be a \textbf{binomial} random variable with parameters $(n,p)$. \\

The probability mass function of a binomial random variable with parameters $n$ and $p$ is given by
\begin{equation*}
  P\{ X=i \} = \binom{n}{i} p^i (1-p)^{n-i}, \;\;\;\; i =0,1,\dots,n
\end{equation*}

Since a binomial random variable, with parameters $n$ and $p$, represents the number of successes in $n$ independent trials, each having success probability $p$, we can represent $X$ as follows:
\begin{equation*}
  X = \sum_{i=1}^n X_i
\end{equation*}
where
\begin{equation*}
  X_i =
  \begin{cases}
    1 & \text{if the $i$th trial is a success} \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}
Using this representation, we get
\begin{eqnarray*}
  E[X]
  &=& \sum_{i=1}^n E[X_i] \\
  &=& np \\
  Var(X)
  &=& \sum_{i=1}^n Var(X_i) \\
  &=& np(1-p)
\end{eqnarray*}
The distribution function is
\begin{equation*}
  P\{ X \le i \}
  = \sum_{k=0}^i \binom{n}{k} p^k (1-p)^{n-k} \;\;\;\; i = 0,1,\dots,n
\end{equation*}
The relationship between $P\{ X = k+1 \}$ and P\{ X = k \}:
\begin{equation*}
  P\{ X = k+1 \}
  = \frac{p}{1-p} \frac{n-k}{k+1} P\{ X = k \}
\end{equation*}

\subsubsection{Negative Binomial Distribution}

Suppose there is a sequence of independent Bernoulli trials. In each trial the probability of success is $p$ and of failure is $(1-p)$. The negative binomial variable $X$ is
\begin{equation*}
  X = \# \text{ of trials until $k$ success} \;\; X = \{k, k+1, \dots \}
\end{equation*}

The probability mass function is
\begin{eqnarray*}
  P(X = n) 
  &=& \binom {n-1}{k-1} (1-p)^{n-k} p^{k-1} p \\
  &=& \binom {n-1}{k-1} (1-p)^{n-k} p^{k}
\end{eqnarray*}

For geometric random variable $X$ with the probability of success is $p$, we have
\begin{eqnarray*}
  E[X] &=& \frac {1}{p} \\
  Var(X) &=& \frac {1-p}{p^2}
\end{eqnarray*}

Since the negative binomial variable $X$ is the sum of independent geometric random variables, we have
\begin{eqnarray*}
  E[X] &=& \frac{k}{p} \\
  Var(X) &=& \frac{k(1-p)}{p^2}
\end{eqnarray*}

\subsection{The Poisson Random Variables}

A random variable $X$, taking on one of the values 0, 1, 2, ..., is said to be a Poisson random variable with parameter $\lambda$, $\lambda>0$, if its probability mass function is given by
\begin{equation*}
  P\{ X=i \} = e^{-\lambda} \frac{\lambda^i}{i!} \;\;\;\; i = 0,1,\dots
\end{equation*}
Then we get
\begin{eqnarray*}
  E[X] &=& \lambda \\
  Var(X) &=& \lambda
\end{eqnarray*}
If $X$ is Poisson with mean $\lambda$, then
\begin{equation*}
  \frac {P\{X=i+1\}}{P\{X=i\}}
  = \frac {\lambda}{i+1}
\end{equation*}

\subsection{The Hypergeometric Random Variable}

A random variable $X$ is a \textbf{hypergeometric} random variable if its mass function is
\begin{equation*}
  P\{ X = i \} = \frac
  {\binom{N}{i} \binom{M}{n-i}}
  {\binom{N+M}{n}} \;\;\;\; i = 0,1,\dots,\min(N,n)^*
\end{equation*}
Let
\begin{equation*}
  p = \frac {N}{N+M}
\end{equation*}
We get
\begin{eqnarray*}
  E[X] &=& \sum_{i=1}^n E[X_i] = \sum_{i=1}^n P[X_i=1] = \frac{nM}{N+M} = np \\
  Var(X) &=& \sum_{i=1}^n Var(X_i) + 
  2 \sum_{i=1}^n \sum_{j \ge i}^n Cov(X_i, X_j) \\
  &=& \frac{2NM}{(N+M)^2} - \frac{n(n-1)NM}{(N+M)^2(N+M-1)} \\
  &=& \frac{nNM}{(N+M)^2} \left( 1 - \frac{n-1}{N+M-1} \right) \\
  &=& np(1-p) \left[ 1 - \frac{n-1}{N+M-1} \right]
\end{eqnarray*}

\subsection{The Uniform Random Variable}

A random variable $X$ is said to be uniformly distributed over the interval $[\alpha, \beta]$ if its probability density function is given by
\begin{equation*}
  f(x) = 
  \begin{cases}
    \frac{1}{\beta - \alpha} & \text{if } \alpha \le x \le \beta \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}
Then we get
\begin{eqnarray*}
  E[X] &=& \frac{\alpha + \beta}{2} \\
  Var(X) &=& \frac{(\beta - \alpha)^2}{12}
\end{eqnarray*}

\subsection{Normal Random Variable}

A random variable is said to be normally distributed with parameters $\mu$ and $\sigma^2$, and we write $X \sim \mathcal{N}(\mu, \sigma^2)$, if its density is
\begin{equation*}
  f(x) = \frac{1}{\sqrt{2 \pi \sigma}} e^{-(x-\mu)^2/2\sigma^2}
\end{equation*}
Let
\begin{equation*}
  Z = \frac{X-\mu}{\sigma}
\end{equation*}
Then $Z$ is a normal random variable with mean 0 and variance 1. $Z$ is said to have a \textbf{standard normal distribution} $\Phi(\cdot)$:
\begin{equation*}
  \Phi(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x e^{-y^2/2} dy
  \;\;\;\; -\infty < x < \infty
\end{equation*}

\subsection{Exponential Random Variables}

A continuous random variable whose probability density function is given, for some $\lambda > 0$, by
\begin{equation*}
  f(x) =
  \begin{cases}
    \lambda e^{-\lambda x} & \text{if } x \ge 0 \\
    0 & \text{if } x < 0
  \end{cases}
\end{equation*}
is said to be an \textbf{exponential random variable} with parameter $\lambda$. The cumulative distribution function $F(x)$ is given by
\begin{equation*}
  F(X) = 1 - e^{-\lambda x}, \;\;\;\; x \ge 0
\end{equation*}
We get
\begin{eqnarray*}
  \phi(t) &=& \frac{\lambda}{\lambda - t} \;\;\;\; t < \lambda \\
  E[X] &=& \frac{1}{\lambda} \\
  Var(X) &=& \frac{1}{\lambda^2}
\end{eqnarray*}

\begin{remark}
  The key property of an exponential random variable is that it is memoryless, where we say that a nonnegative random variable $X$ is \textbf{memoryless} if
  \begin{equation*}
    P\{ X > s+t | X>t \} = P\{ X >s \} \;\;\;\; \text{for all } s,t \ge 0
  \end{equation*}
\end{remark}

\begin{proposition}
  If $X_1,X_2,\dots,X_n$ are independent exponential random variables having respective parameter $\lambda_1, \lambda_2, \dots, \lambda_n$, then $\min(X_1, X_2,\dots,X_n)$ is exponential with parameter $\sum_{i=1}^n \lambda_i$.
\end{proposition}

\subsubsection{The Poisson Process}

\begin{definition}
  Suppose that ``events'' are occurring at random time points, and let $N(t)$ denote the number of events that occurs in the time interval $[0,t]$. These events are said to constitute a \textbf{Poisson process} having rate $\lambda$, $\lambda > 0$, if
  \begin{enumerate} [(a)]
    \item $N(0) = 0$
    \item The number of events that occur in disjoint time intervals are independent.
    \item The distribution of the number of events that occur in a given interval depends only on the length of the interval and not on its location.
    \item $\lim_{h \rightarrow 0} \frac {P\{ N(h) = 1 \} }{h} = \lambda$
    \item $\lim_{h \rightarrow 0} \frac {P\{ N(h) \ge 2 \} }{h} = 0$
  \end{enumerate}
\end{definition}

Condition $(a)$ states that the process begins at time 0. Condition $(b)$, the \emph{independent increment} assumption, states for instance that the number of events by time $t$ [that is, $N(t)$] is independent of the number of events that occurs between $t$ and $t+s$ [that is, $N(t+s) - N(t)$]. Condition $(c)$,  the \emph{stationary increment} assumption, states that probability distribution of $N(t+s) - N(t)$ is the sample for all values of $t$. Condition $(d)$ and $(e)$ state that in a small interval of length $h$, the probability of one event occurring is approximately $\lambda h$, whereas the probability of 2 or more is approximately 0. \\

\begin{proposition}
  For a Poisson process having rate $\lambda$
  \begin{equation*}
    P\{ N(t)=k \} = e^{-\lambda t} \frac{(\lambda t)^k}{k!}, \;\;\; k=0,1,\dots
  \end{equation*}
  That is, the number of events in any interval of length $t$ has a Poisson distribution with mean $\lambda t$.
\end{proposition}

Suppose we split the interval with length $t$ into $n$ pieces. Let $p = \lambda t/n$. As $n \rightarrow +\infty$, we have
\begin{eqnarray*}
  Pr(X = k)
  &=& \binom {n}{k} p^k (1-p)^{n-k} \\
  &=& \frac{n!}{k! (n-k)!} \left( \frac{\lambda t}{n} \right)^k
  \left( 1 - \frac{\lambda t}{n} \right)^n
  \left( 1 - \frac{\lambda t}{n} \right)^{-k} \\
  &\approx& \frac {n^k}{k!} \left( \frac{\lambda t}{n} \right)^k
  \left( 1 - \frac{\lambda t}{n} \right)^n
  \left( 1 - \frac{\lambda t}{n} \right)^{-k} \\
  &\approx& \frac{(\lambda  t)^k}{k!} \left( 1 + \frac{- \lambda t}{n}
  \right)^n \\
  &=& e^{-\lambda t} \frac{(\lambda t)^k}{k!} 
\end{eqnarray*}

For a Poisson process, let $X_1$ denote the time of the first event. Further, for $n>1$, let $X_n$ denote the elapsed time between $(n-1)$st and the $n$th events. The sequence $\{ X_n, n = 1,2,\dots \}$ is called the \textbf{sequence of interarrival times}. \\

We note that the event $\{ X_1 > t\}$ takes place if and only if no events of the Poisson process occur in the interval $[0,t]$ and thus,
\begin{equation*}
  P\{ X_1 > t \} = P\{ N(t)=0 \} = e^{-\lambda t}
\end{equation*}

Hence, $X_1$ is an exponential distribution with mean $1 / \lambda$.

\begin{proposition}
  $X_1, X_2, \dots$ are independent exponential random variables each with mean $1 / \lambda$.
\end{proposition}

\subsection{The Gamma Distribution}

A random variable is said to have a gamma distribution with parameters $(\lambda, \alpha)$, $\lambda > 0$, $\alpha > 0$, if its density function is given by
\begin{eqnarray*}
  f(x) =
  \begin{cases}
    \frac{\lambda e^{-\lambda x} (\lambda x)^{\alpha - 1}}{\Gamma(\alpha)}
    & x \ge 0 \\
    0 & x < 0
  \end{cases}
\end{eqnarray*}

where
\begin{eqnarray*}
  \Gamma (\alpha)
  &=& \int_0^{\infty} \lambda e^{-\lambda x} (\lambda x)^{\alpha-1} dx \\
  &=& \int_0^{\infty} e^{-y} y^{\alpha - 1} dy \;\;\;\;\;
  \text{(by letting $y = \lambda x$)} \\
  &=& -e^{-y} y^{\alpha - 1} \Big |_{y=0}^{y=\infty}
  + \int_0^{\infty} e^{-y} (\alpha - 1) y^{\alpha - 2} dy \\
  &=& (\alpha - 1) \int_0^{\infty} e^{-y} y^{\alpha-2} dy \\
  &=& (\alpha - 1) \Gamma (\alpha -1)
\end{eqnarray*}

When $\alpha$ is an integer--say, $\alpha = n$--we can iterate the foregoing to obtain that
\begin{eqnarray*}
  \Gamma(n)
  &=& (n-1) \Gamma (n-1) \\
  &=& (n-1) (n-2) \Gamma (n-2) \\
  &&  \vdots \\
  &=& (n-1)!\Gamma (1)
\end{eqnarray*}

Because
\begin{equation*}
  \Gamma (1) = \int_0^{\infty} e^{-y} dy = 1
\end{equation*}

We see that
\begin{equation*}
  \Gamma (n) = (n-1)!
\end{equation*}

The function $\Gamma (n)$ is called the \emph{gamma} function. \\

We can also get
\begin{eqnarray*}
  \phi (t)
  &=& E[e^{tX}] = \left( \frac{\lambda}{\lambda - t} \right)^{\alpha} \\
  E[X]
  &=& \phi '(0) = \frac {\alpha}{\lambda} \\
  Var(X)
  &=& E[X^2] - (E[X])^2 = \phi ''(0) - \left( \phi '(0) \right)^2
  = \frac{\alpha}{\lambda^2}
\end{eqnarray*}

\begin{proposition}
  If $X_i$, $i=1,\dots,n$ are independent gamma random variables with respective parameters $(\alpha_i, \lambda)$, then $\sum_{i=1}^n X_i$ is gamma with parameters $\sum_{i=1}^n \alpha_i, \lambda$.
\end{proposition}

This result easily follows since
\begin{eqnarray*}
  \phi_{X_1+X_2} (t)
  &=& E[e^{t(X_1 + X_2)}] \\
  &=& \phi_{X_1} (t) \phi_{X_2} (t) \\
  &=& \left( \frac{\lambda}{\lambda - t} \right)^{\alpha_1 + \alpha_2}
\end{eqnarray*}

\begin{corollary}
  If $X_1,\dots,X_n$ are independent exponential random variables, each having rate $\lambda$, then $\sum_{i=1}^n X_i$ is a gamma random variable with parameters $(n, \lambda)$.
\end{corollary}

It should be noted that as $\alpha$ becomes large, the density starts to resemble the normal density. This is theoretically explained by the central limit theorem.

\subsection{Distribution Arising From the Normal}

\subsubsection{The Chi-Square Distribution}

\begin{definition}
  If $Z_1, Z_2, \dots, Z_n$ are independent standard normal random variables, then $X$, defined by
  \begin{equation*}
    X = Z_1^2 + Z_2^2 + \cdots + Z_n^2
  \end{equation*}
  is said to have a \textbf{chi-square distribution} with $n$ degrees of freedom. We will use the notation
  \begin{equation*}
    X \sim \chi_n^2
  \end{equation*}
  to signify that $X$ has a chi-square distribution with $n$ degrees of freedom.
\end{definition}

The chi-square distribution has the additive property that if $X_1$ and $X_2$ are independent chi-square random variables with $n_1$ and $n_2$ degrees of freedom, respectively, then $X_1 + X_2$ is chi-square with $n_1+n_2$ degrees of freedom. \\

If $X$ is a chi-square random variable with $n$ degrees of freedom, then for any $\alpha \in (0,1)$, the quantity $\chi_{\alpha,n}^2$ is defined to be such that
\begin{equation*}
  P \{ X \ge \chi_{\alpha,n}^2 \} = \alpha
\end{equation*}

\paragraph{The Relation between Chi-Square and Gamma Random variable}

Let us compute the moment generating function of a chi-square random variable with $n$ degrees of freedom. To begin, we have, when $n=1$, that
\begin{eqnarray*}
  E[e^{tX}]
  &=& E[e^{tZ^2}] \;\;\;\; \text{where} \;\; Z \sim \mathcal{N}(0,1) \\
  &=& \int_{-\infty}^{\infty} e^{tx^2} f_Z(x) dx \\
  &=& \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{tx^2} e^{-x^2/2} dx \\
  &=& \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{x^2(1-2t)/2} dx \\
  &=& \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{-x^2/2\hat{\sigma}^2} dx
  \;\;\;\; \text{where} \;\; \hat{\sigma}^2 = (1-2t)^{-1} \\
  &=& (1-2t)^{-1/2} \frac{1}{\sqrt{2\pi} \hat{\sigma}}
  \int_{-\infty}^{\infty} e^{-x^2/2\hat{\sigma}^2} dx \\
  &=& (1-2t)^{-1/2}
\end{eqnarray*}

Hence, in the general case of $n$ degrees of freedom
\begin{eqnarray*}
  E[e^{tX}]
  &=& E \left[ e^{t \sum_{i=1}^n Z_i^2}  \right] \\
  &=& E \left[ \prod_{i=1}^n e^{tZ_i^2} \right] \\
  &=& \prod_{i=1}^n E \left[ e^{tZ_i^2} \right] \\
  &=& (1-2t)^{-n/2}
\end{eqnarray*}

However, we recognize $[1/(1-2t)]^{n/2}$ as being the moment generating function of a gamma random variable with parameters $(n/2, 1/2)$. Hence, by the uniqueness of moment generating functions, it follows that these two distributions--chi-square with $n$ degrees of freedom and gamma with parameters $n/2$ and $1/2$--are identical, and thus we can conclude that the density of $X$ is given by
\begin{equation*}
  f(x) = \frac{\frac{1}{2} e^{-x/2} \left( \frac{x}{2} \right)^{(n/2)-1} }
  {\Gamma \left( n/2 \right)}, \;\;\;\; x > 0
\end{equation*}

\subsubsection{The t-Distribution}

If $Z$ and $\chi_n^2$ are independent random variables, with $Z$ having a standard normal distribution and $\chi_n^2$ having a chi-square distribution with $n$ degrees of freedom, then the random variable $T_n$ defined by
\begin{equation*}
  T_n = \frac{Z}{\sqrt{\chi_n^2/n}}
\end{equation*}

is said to have a \textbf{t-distribution} with $n$ degrees of freedom. \\

Like the standard normal density, the t-density is symmetric about zero. In addition, as $n$ becomes larger, it becomes more and more like a standard normal density. To understand why, recall that $\chi_n^2$ can be expressed as the sum of the squares of $n$ standard normals, and so
\begin{equation*}
  \frac{\chi_n^2}{n} = \frac{Z_1^2 + \cdots + Z_n^2}{n}
\end{equation*}

where $Z_1, \dots, Z_n$ are independent standard normal random variables. t now follows the weak law of large numbers that, for large $n$, $T_n = Z / \sqrt{\chi_n^2 / n}$ will have approximately the same distribution as $Z$. \\

The mean and variance of $T_n$ can be shown to equal
\begin{eqnarray*}
  E[T_n] &=& 0, \;\;\;\; n > 1 \\
  Var[T_n] &=& \frac{n}{n-2}, \;\;\;\; n > 2
\end{eqnarray*}

Let $t_{\alpha, n}$ be such that
\begin{equation*}
  P\{ T_n \ge t_{\alpha, n} \} = \alpha
\end{equation*}

It follows from the symmetry about zero of the t-density function that $-T_n$ has the same distribution as $T_n$, and so
\begin{eqnarray*}
  \alpha
  &=& P \{ -T_n \ge t_{\alpha, n} \} \\
  &=& P \{ T_n \le -t_{\alpha, n} \} \\
  &=& 1 - P \{ T_n > -t_{\alpha, n} \} \\
  &\Rightarrow& P\{ T_n \ge -t_{\alpha, n} \} = 1 - \alpha \\
  &\Rightarrow& -t_{\alpha, n} = t_{1-\alpha, n}
\end{eqnarray*}

\subsection{The F-Distribution}

If $\chi_n^2$ and $\chi_m^2$ are independent chi-square random variables with $n$ and $m$ degrees of freedom, respectively, then the random variable $F_{n,m}$ defined by
\begin{equation*}
  F_{n,m} = \frac{\chi_n^2/n}{\chi_m^2/m}
\end{equation*}

is said to have an \textbf{F-distribution} with $n$ and $m$ degrees of freedom. \\

For any $\alpha \in (0,1)$, let $F_{\alpha, n, m}$ be such that
\begin{equation*}
  P \{ F_{n,m} > F_{\alpha, n, m} \} = \alpha
\end{equation*}

We can also have
\begin{eqnarray*}
  \alpha
  &=& P \left\{ \frac{\chi_n^2/n}{\chi_m^2/m} > F_{\alpha, n, m} \right\} \\
  &=& P \left\{ \frac{\chi_m^2/m}{\chi_n^2/n}
      < \frac{1}{F_{\alpha, n, m}} \right\} \\
  &=& 1 - P \left\{ \frac{\chi_m^2/m}{\chi_n^2/n}
      \ge \frac{1}{F_{\alpha, n, m}} \right\} \\
  &\Rightarrow& P \left\{ \frac{\chi_m^2/m}{\chi_n^2/n}
      \ge \frac{1}{F_{\alpha, n, m}} \right\} = 1 - \alpha
\end{eqnarray*}

Since
\begin{equation*}
  P \left\{ \frac{\chi_m^2/m}{\chi_n^2/n} 
    \ge F_{1 - \alpha, m, n} \right \} = 1 - \alpha  
\end{equation*}

We get
\begin{equation*}
  \frac{1}{F_{\alpha, n, m}} = F_{1 - \alpha, m, n}
\end{equation*}

\subsection{The Logistic Distribution}

A random variable $X$ is aid to have a \textbf{logistic} distribution with parameters $\mu$ and $\nu > 0$ if its distribution function is
\begin{equation*}
  F(x) = \frac {e^{(x - \mu)/ \ nu}}{1 + e^{(x - \mu)/ \nu}},
  \;\;\;\; -\infty < x < \infty
\end{equation*}

Differentiating $F(x)$ yields the density function
\begin{equation*}
  f(x) = \frac {e^{(x - \mu)/ \nu}}{v(1 + e^{(x - \mu)/ \ nu})^2},
  \;\;\;\; -\infty < x < \infty
\end{equation*}

We also can get
\begin{equation*}
  E[X] = \mu
\end{equation*}

$\mu$ is the mean of the logistic; $\nu$ is called the dispersion parameter.

\section{Distribution of Sampling Statistics}

\subsection{Introduction}

\begin{definition}
  If $X_1,\dots,X_n$ are independent random variables having common distribution $F$, then we say they constitute a \textbf{sample} (sometimes called a \emph{random sample}) from the distribution $F$.
\end{definition}

\subsection{The Sample Mean}

We often suppose that the value associated with any member of the population can be regarded as being the value of a random variable having expectation $\mu$ and variance $\sigma^2$. The quantities $\mu$ and $\sigma^2$ are called the \textbf{population mean} and the \textbf{population variance} respectively. Let $X_1, X_2, \dots, X_n$ be a sample of values from this population. The sample mean is defined by
\begin{equation*}
  \overline{X} = \frac{X_1 + \cdots + X_n}{n}
\end{equation*}
Then its expected value and variance are obtained as follows:
\begin{equation*}
  E[\overline{X}] = E \left[ \frac{X_1 + \cdots + X_n}{n} \right] = \mu
\end{equation*}
and
\begin{equation*}
  Var(\overline{X}) = Var \left( \frac{X_1 + \cdots + X_n}{n} \right)
= \frac{\sigma^2}{n}
\end{equation*}

\subsection{The Central Limit Theorem}

\begin{theorem} [The Central Limit Theorem]
  Let $X_1, X_2,\dots,X_n$ be a sequence of independent and identically distributed random variables each having mean $\mu$ and variance $\sigma^2$. Then for $n$ large, the distribution of
  \begin{equation*}
    X_1 + \cdots + X_n
  \end{equation*}
  is approximately normal with mean $n\mu$ and variance $n\sigma^2$.
\end{theorem}

It follows from the central limit theorem that
\begin{equation*}
  P \left\{ \frac {X_1 + \cdots + X_n - n\mu}{\sigma \sqrt{n}} < x \right\}
  \approx P(Z < x)
\end{equation*}

where $Z$ is a standard random variable.

\subsubsection{Approximately Distribution of the Sample Mean}

Let $X_1, \dots, X_n$ be a sample from a population having mean $\mu$ and variance $\sigma^2$. The central limit theorem can be used to approximate the distribution of the sample mean
\begin{equation*}
  \overline {X} = \sum_{i=1}^n X_i / n
\end{equation*}

Since a constant multiple of a normal random variable is also normal, it follows from the central limit theorem that $\overline{X}$ will be approximately normal when the sample size $n$ is large. Since the sample mean has expected value $\mu$ and standard deviation $\sigma / \sqrt{n}$, it then follows that
\begin{equation*}
  \frac {\overline{X} - \mu}{\sigma / \sqrt{n}}
\end{equation*}

has approximately a standard normal distribution.

\subsubsection{How Large A Sample Is Needed}

A general rule of thumb is that one can be confident of the normal approximation whenever the sample size $n$ is at least 30.

\subsection{The Sample Variance}

\begin{definition}
  The statistic $S^2$, defined by
  \begin{equation*}
    S^2 = \frac {\sum_{i=1}^n (X_i - \overline{X})^2}{n-1}
  \end{equation*}
  is called the \textbf{sample variance}. $S = \sqrt{S^2}$ is called the \textbf{sample standard deviation}.
\end{definition}

Using $E[W^2] = Var(W) + (E[W])^2$, we have
\begin{eqnarray*}
  (n-1)E[S^2]
  &=& E \left[ \sum_{i=1}^n X_i^2 - n \overline{X}^2 \right] \\
  &=& E \left[ \sum_{i=1}^n X_i^2 \right] - n E[\overline{X}^2] \\
  &=& n E[X_1^2] - n E[\overline{X}^2] \\
  &=& n Var(X_1) + n (E[X_1])^2 - n Var(\overline{X})
  - n (E[\overline{X}])^2 \\
  &=& n \sigma^2 + n \mu^2 - n (\sigma^2 / n) - n \mu^2 \\
  &=& (n-1) \sigma^2 \\
  &\Rightarrow& E[S^2] = \sigma^2
\end{eqnarray*}

\subsection{Sampling Distributions from a Normal Population}

Let $X_1, X_2, \dots, X_n$ be a sample from a normal population having mean $\mu$ and variance $\sigma^2$. That is, they are independent and $X_i \sim \mathcal{N}(\mu, \sigma^2), i=1,\dots,n$. Also let
\begin{equation*}
  \overline{X} = \sum_{i=1}^n X_i / n
\end{equation*}

and
\begin{equation*}
  S^2 = \frac {\sum_{i=1}^n (X_i - \overline{X})^2}{n-1}
\end{equation*}

denote the sample mean and sample variance, respectively.

\subsubsection{Distribution of the Sample Mean}

Since the sum of independent normal random variable is normally distributed, it follows that $\overline{X}$ with mean
\begin{equation*}
  E[\overline{X}] = \sum_{i=1}^n \frac {E[X_i]}{n} = \mu
\end{equation*}

and variance
\begin{equation*}
  Var(\overline{X}) = \frac{1}{n^2} \sum_{i=1}^n Var(X_i) = \sigma^2 / n
\end{equation*}

\subsubsection{Joint Distribution of $\overline{X}$ and $S^2$}

\begin{theorem} \label{theorem-6.5.1}
  If $X_1, \dots, X_n$ is a sample from a normal population having mean $\mu$ and variance $\sigma^2$, then $\overline{X}$ and $S^2$ are independent with $\overline{X}$ being normal with mean $\mu$ and variance $\sigma^2/n$ and $(n-1)S^2 / \sigma^2$ having a chi-square distribution with $n-1$ degrees of freedom.
\end{theorem}

For numbers $x_1, \dots, x_n$, let $y_i = x_i - \mu, i = 1, \dots, n$. Then $\overline{y} = \overline{x} - \mu$, it follows from the identity
\begin{equation*}
  \sum_{i=1}^n (y_i - \overline{y})^2 = \sum_{i=1}^n y_i^2 - n \overline{y}^2
\end{equation*}
that
\begin{equation*}
  \sum_{i=1}^n (x_i - \overline{x})^2
  = \sum_{i=1}^n (x_i - \overline{x})^2 - n (\overline{x} - \mu)^2
\end{equation*}

If $X_1, \dots, X_n$ is a sample from a normal population having mean $\mu$ and variance $\sigma^2$, then we obtain from the preceding identity that
\begin{eqnarray*}
  \frac{\sum_{i=1}^n (X_i - \mu)^2}{\sigma^2}
  &=& \frac{\sum_{i=1}^n (X_i - \overline{X})^2}{\sigma^2}
  + \frac{n (\overline{X} - \mu)^2}{\sigma^2} \\
  \sum_{i=1}^n \left( \frac{ (X_i - \mu}{\sigma} \right)^2
  &=& \frac{\sum_{i=1}^n (X_i - \overline{X})^2}{\sigma^2}
  + \left[ \frac{\sqrt{n} (\overline{X} - \mu)}{\sigma} \right]^2
\end{eqnarray*}

The equation above equates a chi-square random variable having $n$ degrees of freedom to the sum of two random variables, one of which is chi-square with 1 degree of freedom. Since it has been established that the sum of two independent chi-square random variables is also chi-square with a degree of freedom equal to the sum of the two degrees of freedom. Thus, it would seem that there is a reasonable possibility that two terms on the right side of the equation are independent, with $(n-1)S^2 / \sigma^2$ having a chi-square distribution with $n-1$ degrees of freedom.

\begin{corollary}
  Let $X_1, \dots, X_n$ be a sample from a normal population with mean $\mu$. If $\overline{X}$ denotes the sample mean and $S$ the sample standard deviation, then
  \begin{equation*}
    \sqrt{n} \frac{(\overline{X} - \mu)}{S} \sim t_{n-1}
  \end{equation*}
\end{corollary}

Recall that a t-random variable with $n$ degrees of freedom is defined as the distribution of
\begin{equation*}
  \frac{Z}{\sqrt{\chi_n^2 / n}}
\end{equation*}

where $Z$ is a standard normal random variable that is independent of $\chi_n^2$, a chi-square random variable with $n$ degrees of freedom. It then follows from the previous theorem that
\begin{equation*}
  \frac{\sqrt{n} (\overline{X} - \mu) / \sigma}{\sqrt{S^2 / \sigma^2}}
  = \sqrt{n} \frac{(\overline{X} - \mu)}{S} \sim t_{n-1}
\end{equation*}

is a t-random variable with $n-1$ degrees of freedom.

\subsection{Sampling from a Finite Population}

A sample of size $n$ from this population is said to be a \textbf{random sample} if it is chosen in such a manner that each of the $\binom {N}{n}$ population subsets of size $n$ is equally likely to be the sample.


\section{Parameter Estimation}

\subsection{Introduction}

In this chapter, we will present the \textbf{maximum likelihood} method for determining estimators of unknown parameters. The estimators so obtained are called \textbf{point estimates}, because they specify a single quantity as an estimate of $\theta$. Also, we will consider the problem of obtaining \textbf{interval estimates}. Additionally, we consider the question of how much confidence we can attach to such an interval estimate. We then consider a variety of interval estimation problems.

\subsubsection{Method of Moments}

The method of moments estimates $E[x^k]$ by $\sum_{i=1}^k x^k/n$ for all $k$. Then find a functional form mapping the parameter to the moment.

\subsection{Maximum Likelihood Estimators}

Let $f(x_1, \dots, x_n | \theta)$ denote the joint probability mass function of the random variables $X_1, X_2, \dots, X_n$ when they are discrete, and let it be their joint probability density function when they are jointly continuous random variables. Because $\theta$ is assumed unknown, we also write $f$ as a function of $\theta$. The \textbf{maximum likelihood} estimator $\hat{\theta}$ is defined to be that value of $\theta$ maximizing $f(x_1, \dots, x_n | \theta)$ where $x_1, \dots, x_n$ are the observed values. The function $f(x_1, \dots, x_n | \theta)$ is often referred to as the \textbf{likelihood} function of $\theta$.

\subsection{Interval Estimates}

Suppose that $X_1, \dots, X_n$ is a sample from a normal population having unknown mean $\mu$ and known variance $\sigma^2$. It has been shown that $\overline{X} = \sum_{i=1}^n X_i / n$ is the maximum likelihood estimator for $\mu$. However, we don't expect the sample mean $\overline{X}$ will exactly equal $\mu$, but rather that it will ``be close''. Since the point estimator $\overline{X}$ is normal with mean $\mu$ and variance $\sigma^2 / n$, it follows that
\begin{eqnarray*}
  \frac {\overline{X} - \mu}{\sigma / \sqrt{n}}
  = \sqrt{n} \frac {\overline{X} - \mu}{\sigma}
\end{eqnarray*}

has a standard normal distribution. For any $\sigma$,
\begin{eqnarray*}
  P \left\{ - z_{\alpha / 2} < Z < z_{\alpha / 2} \right\} = 1 - \alpha
\end{eqnarray*}

when $Z$ is a standard normal random variable. As a result, we have
\begin{eqnarray*}
  P \left\{ - z_{\alpha / 2} < \sqrt{n} \frac {\overline{X} - \mu}{\sigma}
      < z_{\alpha / 2} \right\} &=& 1 - \alpha \\
  P \left\{ \overline{X} - z_{\alpha / 2} \frac {\sigma}{\sqrt{n}} < \mu
      < \overline{X} + z_{\alpha / 2} \frac {\sigma}{\sqrt{n}}
    \right\} &=& 1 - \alpha
\end{eqnarray*}

If we now observe the sample and it turns out that $\overline{X} = \overline{x}$, then the interval
\begin{eqnarray*}
  \left( \overline{x} - z_{\alpha / 2} \frac {\sigma}{\sqrt{n}},
    \overline{x} + z_{\alpha / 2} \frac {\sigma}{\sqrt{n}} \right)
\end{eqnarray*}

is called a \textbf{$100(1 - \alpha)$ two-sided confidence interval estimate} of $\mu$. \\

Similarly, knowing that $Z = \sqrt{n} \frac{\overline{X} - \mu}{\sigma}$ is a standard normal random variable, along with the identities
\begin{eqnarray*}
  P \left\{ Z > z_{\alpha} \right\} = \alpha
\end{eqnarray*}

and
\begin{eqnarray*}
  P \left\{ Z < -z_{\alpha} \right\} = \alpha
\end{eqnarray*}

results in one-sided confidence intervals of any desired level of confidence. Specially, we obtain that
\begin{eqnarray*}
  \left( \overline{x} - z_{\alpha} \frac {\sigma}{\sqrt{n}}, \infty \right)
\end{eqnarray*}

and
\begin{eqnarray*}
  \left( -\infty, \overline{x} + z_{\alpha} \frac {\sigma}{\sqrt{n}} \right)
\end{eqnarray*}

are, respectively, $100(1 - \alpha)$ percent one-sided upper and $100(1 - \alpha)$ percent one-sided lower confidence intervals for $\mu$.

\subsubsection{Confidence Interval for a Normal Mean When the Variance is Unknown}

Suppose that $X_1, \dots, X_n$ is a sample from a normal distribution with unknown mean $\mu$ and unknown variance $\sigma^2$, and that we wish to construct a $100(1 - \sigma)$ percent confidence interval for $\mu$. Since $\sigma$ is unknown, we can no longer base our interval on the fact that $\sqrt{n}(\overline{X} - \mu) / \sigma$ is a standard normal random variable. However, by letting $S^2 = \sum_{i=1}^n (X_i - \overline{X})^2 / (n-1)$ denote the sample variance, we know that
\begin{equation*}
  \sqrt{n} \frac {\overline{X} - \mu}{S}
\end{equation*}

is a $t$-random variable with $n-1$ degrees of freedom. Hence, from the symmetry of the $t$-density function, we have that for any $\sigma \in (0, 1/2)$,
\begin{equation*}
  P \left\{ - t_{\alpha / 2, n-1} < 
    \sqrt{n} \frac {\overline{X} - \mu}{S}
    < t_{\alpha /2, n-1} \right\} = 1 - \alpha
\end{equation*}

or, equivalently
\begin{equation*}
  P \left\{ \overline{X} - t_{\alpha /2, n-1} \frac{S}{\sqrt{n}} < \mu
    < \overline{X} + t_{\alpha /2, n-1} \frac{S}{\sqrt{n}} \right\}
  = 1 - \alpha
\end{equation*}

Thus, if it is observed that $\overline{X} = \overline{x}$ and $S = s$, then we can say that ``with $100(1 - \alpha)$ percent confidence``
\begin{equation*}
  \mu \in \left( \overline{x} - t_{\alpha /2, n-1} \frac{s}{\sqrt{n}}, \;
    \overline{x} + t_{\alpha /2, n-1} \frac{s}{\sqrt{n}} \right)
\end{equation*}

\subsubsection{Confidence Intervals for the Variance of a Normal Distribution}

If $X_1, \dots, X_n$ is a sample from a normal distribution having unknown parameters $\mu$ and $\sigma^2$, then we can construct a confidence interval for $\sigma^2$ by using the fact that
\begin{eqnarray*}
  (n-1) \frac {S^2}{\sigma^2} \sim \chi^2_{n-1}
\end{eqnarray*}

Hence,
\begin{equation*}
  P \left\{ \chi_{1-\alpha / 2, n-1}^2 \le (n-1) \frac {S^2}{\sigma^2}
    \le \chi_{\alpha / 2, n-1}^2 \right\} = 1 - \alpha
\end{equation*}

or, equivalently,
\begin{equation*}
  P \left\{ \frac {(n-1)S^2} {\chi_{\alpha / 2, n-1}^2}
    \le \sigma^2
    \le \frac {(n-1)S^2} {\chi_{1 - \alpha / 2, n-1}^2} \right\}
  = 1 - \alpha
\end{equation*}

\subsection{Estimating the Difference in Means of Two Normal Populations}

If $X_1, \dots, X_n$ is a sample of size $n$ from a normal population having mean $\mu_1$ and variance $\sigma_1^2$ and let $Y_1, \dots, Y_m$ be a sample of size $m$ from a different normal population having mean $\mu_2$ and variance $\sigma_2^2$ and suppose that the two samples are independent of each other. We are interested estimating $\mu_1 - \mu_2$. \\

Since $\overline{X} = \sum_{i=1}^n X_i / n$ and $\overline{Y} = \sum_{i=1}^m Y_i / m$ are the maximum likelihood estimators of $\mu_1$ and $\mu_2$, it seems intuitive (and can be proven) that $\overline{X} - \overline{Y}$ is the maximum likelihood estimator of $\mu_1$ and $\mu_2$. \\

To obtain a confidence interval estimator, we need the distribution of $\overline{X} - \overline{Y}$. Because
\begin{eqnarray*}
  \overline{X} \sim \mathcal{N} (\mu_1, \sigma_1^2 / n) \\
  \overline{Y} \sim \mathcal{N} (\mu_2, \sigma_2^2 / m)
\end{eqnarray*}

It follows from the fact that the sum of independent normal random variables is also normal that
\begin{eqnarray*}
  \overline{X} - \overline{Y} \sim \mathcal{N} (\mu_1 - \mu_2,
  \frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m})
\end{eqnarray*}

Hence, assuming $\sigma_1^2$ and $\sigma_2^2$ are known, we have that
\begin{eqnarray*}
  \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
  {\sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}}
  \sim \mathcal{N} (0,1)
\end{eqnarray*}

and so
\begin{eqnarray*}
  P \left\{ -z_{\alpha/2} <
      \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
      {\sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}}
      < z_{\alpha/2} \right\} = 1 - \alpha
\end{eqnarray*}

Or equivalent,
\begin{eqnarray*}
  P \left\{ \overline{X} - \overline{Y}
    - z_{\alpha/2} \sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}
    < \mu_1 - \mu_2
    < \overline{X} - \overline{Y}
      + z_{\alpha/2} \sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}
    \right\} = 1 - \alpha
\end{eqnarray*}

Hence, if $\overline{X}$ and $\overline{Y}$ are observed to equal $\overline{x}$ and $\overline{y}$, respectively, then a $100(1 - \alpha)$ two-sided confidence interval estimate for $\mu_1 - \mu_2$ is
\begin{eqnarray*}
  \mu_1 - \mu_2 \in
  \left( \overline{x} - \overline{y}
    - z_{\alpha/2} \sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}, \;
    \overline{x} - \overline{y}
      + z_{\alpha/2} \sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}
    \right)
\end{eqnarray*}

One-sided confidence intervals for $\mu_1 - \mu_2$ are obtained in a similar fashion, we have
\begin{eqnarray*}
  \mu_1 - \mu_2 \in
  \left( - \infty, \; \overline{x} - \overline{y}
      + z_{\alpha/2} \sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}
    \right)
\end{eqnarray*}

Let us suppose now that we again desire an interval estimator of $\mu_1 - \mu_2$ but that the population variances $\sigma_1^2$ and $\sigma_2^2$ are unknown. In this case, it is natural to try to replace $\sigma_1^2$ and $\sigma_2^2$ by sample variances
\begin{eqnarray*}
  S_1^2 &=& \sum_{i=1}^n \frac {(X_i - \overline{X})^2}{n-1} \\
  S_2^2 &=& \sum_{i=1}^n \frac {(Y_i - \overline{Y})^2}{m-1}
\end{eqnarray*}

That is, it is natural to base our interval estimate on something like
\begin{eqnarray*}
  \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
  {\sqrt {\frac{S_1^2}{n} + \frac{S_2^2}{m}}}
\end{eqnarray*}

However, to utilize the foregoing to obtain a confidence interval, we need its distribution and it must not depend on any of the unknown parameters $\sigma_1^2$ and $\sigma_2^2$. Unfortunately, this distribution is both complicated and does indeed depend on the unknown parameters $\sigma_1^2$ and $\sigma_2^2$. In fact, it is only in the special case when $\sigma_1^2 = \sigma_2^2$ that we will be able to obtain an interval estimator. So let us suppose that the population variances, thought unknown, are equal and let $\sigma^2$ denote their common value. Now, from Theorem~\ref{theorem-6.5.1} it follows that
\begin{eqnarray*}
  (n-1) \frac {S_1^2}{\sigma^2} \sim \chi_{n-1}^2
\end{eqnarray*}

and
\begin{eqnarray*}
  (m-1) \frac {S_2^2}{\sigma^2} \sim \chi_{m-1}^2
\end{eqnarray*}

Also, because the samples are independent, it follows that these two chi-square random variables are independent. Hence, from the additive property of chi-square random variables, which states that the sum of independent chi-square random variables is also chi-square with a degree of freedom equal to the sum of their degrees of freedom, it follows that
\begin{eqnarray*}
    (n-1) \frac {S_1^2}{\sigma^2} + (m-1) \frac {S_2^2}{\sigma^2}
    \sim \chi_{n+m-2}^2
\end{eqnarray*}

Also, since
\begin{eqnarray*}
  \overline{X} - \overline{Y} \sim \mathcal{N} (\mu_1 - \mu_2,
  \frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m})
\end{eqnarray*}

we see that
\begin{eqnarray*}
  \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
  {\sqrt {\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{m}}}
  \sim \mathcal{N} (0,1)
\end{eqnarray*}

Now it follows from the fundamental result that in normal sampling $\overline{X}$ and $S^2$ are independent (Theorem~\ref{theorem-6.5.1}), that $\overline{X}_1, S_1^2, \overline{X}_2, S_2^2$ are independent random variables. Hence, using the definition of a $t$-random variable (as the ratio of two independent random variables, the numerator being a standard normal and the denominator being the square root of a chi-square random variable divided by its degree of freedom parameter), it follows that if we let
\begin{eqnarray*}
  S_p^2 = \frac {(n-1)S_1^2 + (m-1)S_2^2}{n+m-2}
\end{eqnarray*}

then
\begin{eqnarray*}
  \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
  {\sqrt {\sigma^2 (1 / n + 1 / m)}}
  \div \sqrt{S_p^2 / \sigma^2}
  = \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
  {S_p \sqrt { (1 / n + 1 / m)}}
\end{eqnarray*}

has a $t$-distribution with $n+m-2$ degrees of freedom. Consequently,
\begin{eqnarray*}
  P \left\{ - t_{\alpha / 2, n+m-2} \le
    \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
    {S_p \sqrt {1 / n + 1 / m}}
    \le t_{\alpha / 2, n+m-2} \right\} = 1 - \alpha
\end{eqnarray*}

Therefore, when the data result in the values $\overline{X} = \overline{x}, \overline{Y} = \overline{y}, S_p = s_p$, we obtain the following $100(1 - \alpha)$ percent confidence interval for $\mu_1 - \mu_2$:
\begin{eqnarray*}
  \left( \overline{x} - \overline{y} - t_{\alpha / 2, n+m-2}
    s_p \sqrt{1/n + 1/m}, \overline{x} - \overline{y} + t_{\alpha / 2, n+m-2}
    s_p \sqrt{1/n + 1/m} \right)
\end{eqnarray*}

One sided confidence intervals are similarly obtained. \\

\begin{remark}
  The confidence interval given above was obtained under the assumption that the population variances are equal; with $\sigma^2$ as their common value , it follows that
  \begin{eqnarray*}
      \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
      {\sqrt {\sigma^2 / n + \sigma^2 / m}}
      = \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
      {\sigma \sqrt {1 / n + 1 / m}}
  \end{eqnarray*}
  has a standard normal distribution. However, since $\sigma^2$ is unknown this result cannot be immediately applied to obtain a confidence interval; $\sigma^2$ must first be estimated. To do so, note that both sample variances are estimators o $\sigma^2$; moreover, since $S_1^2$ has $n-1$ degrees of freedom and $S_2^2$ has $m-1$, the appropriate estimator is to take a weighted average of the two sample variances, with the weights proportional to these degrees of freedom. That is, the estimator of $\sigma^2$ is the \textbf{pooled estimator}
  \begin{eqnarray*}
    S_p^2 = \frac {(n-1)S_1^2 + (m-1)S_2^2}{n+m-2}
  \end{eqnarray*}
  and the confidence interval is then based on the statistic
  \begin{eqnarray*}
    \frac {\overline{X} - \overline{Y} - (\mu_1 - \mu_2)}
    {S_p \sqrt {1 / n + 1 / m}}
  \end{eqnarray*}
  which, by our previous analysis, has a $t$-distribution with $n+m-2$ degrees of freedom.
\end{remark}

\subsection{Approximate Confidence Interval for the Mean of a Bernoulli Random Variable}

Consider a population of items, each of which independently meets certain standards with some unknown probability $p$. If $n$ of these items are tested to determine whether they meet the standards, how can we use the resulting data to obtain a confidence interval for $p$? \\

If we let $X$ denote the number of the $n$ items that meet the standards, then $X$ is a binomial random variable with parameters $n$ and $p$. Thus, when $n$ is large, it follows by the normal approximation to the binomial that $X$ is approximately normally distributed with mean $np$ and variance $np(1-p)$. Hence,
\begin{eqnarray*}
  \frac {X - np}{\sqrt {np(1-p)}} \sim \mathcal{N}(0,1)
\end{eqnarray*}

where $\sim$ means ``is approximately distributed as''. Therefore, for any $\alpha \in (0,1)$,
\begin{eqnarray*}
  P \left\{ - z_{\alpha /2} < \frac {X - np}{\sqrt {np(1-p)}}
    < z_{\alpha / 2} \right\} \approx 1 - \alpha
\end{eqnarray*}

and so if $X$ is observed to equal $x$, then an approximate $100(1 - \alpha)$ percent confidence region for $p$ is
\begin{eqnarray*}
  \left\{ p: - z_{\alpha /2} < \frac {x - np}{\sqrt {np(1-p)}}
    < z_{\alpha / 2} \right\}
\end{eqnarray*}

The foregoing region, however, is not an interval. To obtain a confidence interval for $p$, let $\hat{p} = X / n$ be the fraction of the items that meet the standards. And $\hat{p}$ is the maximum likelihood estimator of $p$, and so should be approximately equal to $p$. As a result, $\sqrt{n \hat{p}(1 - \hat{p})}$ will be approximately equal to $\sqrt{np(1-p)}$ and so we have
\begin{eqnarray*}
  \frac {X - n\hat{p}}{\sqrt {n\hat{p}(1-\hat{p})}} \sim \mathcal{N}(0,1)
\end{eqnarray*}

Hence, for any $\alpha \in (0,1)$ we have that
\begin{eqnarray*}
  P \left\{ - z_{\alpha /2} <
    \frac {X - n\hat{p}}{\sqrt {n\hat{p}(1-\hat{p})}}
    < z_{\alpha / 2} \right\} \approx 1 - \alpha
\end{eqnarray*}

or, equivalently,
\begin{eqnarray*}
  P \left\{ - z_{\alpha /2} \sqrt {n\hat{p}(1-\hat{p})} <
    np - X < z_{\alpha /2} \sqrt {n\hat{p}(1-\hat{p})}
  \right\} \approx 1 - \alpha
\end{eqnarray*}

Since $\hat{p} = X/n$, the preceding can be written as
\begin{eqnarray*}
  P \left\{\hat{p} - z_{\alpha /2} \sqrt {\hat{p}(1-\hat{p}) / n} <
    p < \hat{p} + z_{\alpha /2} \sqrt {\hat{p}(1-\hat{p}) / n}
  \right\} \approx 1 - \alpha
\end{eqnarray*}

We often want to specify an approximate $100(1 - \alpha)$ percent confidence interval for $p$ that is no greater than some given length, say $b$. The problem is to determine the appropriate sample size $n$ to obtain such an interval. To do so, note that the length of the approximate $100(1 - \alpha)$ percent confidence interval for $p$ from a sample of size $n$ is
\begin{eqnarray*}
  2 z_{\alpha / 2} \sqrt {\hat{p} (1 - \hat{p}) / n}
\end{eqnarray*}

which is approximately equal to $2 z_{\alpha / 2} \sqrt {p(1-p)/n}$. Unfortunately, $p$ is not known in advance, and so we cannot just set $2 z_{\alpha / 2} \sqrt {p(1-p)/n}$ equal to $b$ to determine the necessary sample size $n$. What we can do, however, is to first take a preliminary sample to obtain a rough estimate of $p$, and then use this estimate to determine $n$. That is, e use $p^*$, the proportion of the preliminary sample that meets the standards, as a preliminary estimate of $p$; we then determine the total sample size $n$ by solving the equation
\begin{equation*}
  2 z_{\alpha / 2} \sqrt {p^* (1 - p^*) / n} = b
\end{equation*}

Squaring both sides of the preceding yields that
\begin{equation*}
  (2 z_{\alpha / 2})^2 p^* (1 - p^*) / n = b^2
\end{equation*}

Or
\begin{equation*}
  n = \frac {(2 z_{\alpha / 2})^2 p^* (1 - p^*)}{b^2}
\end{equation*}

That is, if $k$ items were initially sampled to obtain the preliminary estimate of $p$, then an additional $n-k$ (or 0 if $n \le k$) items should be sampled.

\begin{remark}
  As shown, a $100(1-\alpha)$ percent confidence interval for $p$ will be of approximate length $b$ when the sample size is
  \begin{equation*}
    n = \frac {(2 z_{\alpha / 2})^2 p^* (1 - p^*)}{b^2}
  \end{equation*}
  Now it is easily shown that the function $g(p) = p(1-p)$ attains its maximum value of $1/4$, in the interval $0 \le p \le 1$, when $p = 1/2$. Thus, an upper bound on $n$ is
  \begin{equation*}
    n \le \frac {(z_{\alpha / 2})^2}{b^2}
  \end{equation*}
  and so by choosing a sample whose size is at least as large as $(z_{\alpha / 2})^2/b^2$, one can be assured of obtaining a confidence interval of length no greater than $b$ without need of any additional sampling.
\end{remark}

\subsection{Confidence Interval of the Mean of the Exponential Distribution}

If $X_1, X_2, \dots, X_n$ are independent exponential random variables each having mean $\theta$, then it can be shown that the maximum likelihood estimator of $\theta$ is the sample mean $\sum_{i=1}^n X_i / n$. To obtain a confidence interval estimator of $\theta$, recall that $\sum_{i=1}^n X_i$ has a gamma distribution with parameters $n$, $1 / \theta$. This in turn implies (from the relationship between the gamma and chi-square distribution) that
\begin{eqnarray*}
  \frac {2}{\theta} \sum_{i=1}^n X_i \sim \chi_{2n}^2
\end{eqnarray*}

Hence, for any $\sigma \in (0,1)$
\begin{eqnarray*}
  P \left\{ \chi_{1-\alpha / 2, 2n}^2
    < \frac {2}{\theta} \sum_{i=1}^n X_i
    < \chi_{\alpha / 2, 2n}^2 \right\} = 1 - \alpha
\end{eqnarray*}

or, equivalently,
\begin{eqnarray*}
  P \left\{ \frac {2 \sum_{i=1}^n X_i}{\chi_{\alpha / 2, 2n}^2}
    < \theta <\frac {2 \sum_{i=1}^n X_i}{\chi_{1-\alpha / 2, 2n}^2}
  \right\} = 1 - \alpha
\end{eqnarray*}

Hence, a $100(1 - \alpha)$ percent confidence interval for $\theta$ is
\begin{eqnarray*}
  \theta \in \left(
    \frac {2 \sum_{i=1}^n X_i}{\chi_{\alpha / 2, 2n}^2}
    , \; frac{2 \sum_{i=1}^n X_i}{\chi_{1-\alpha / 2, 2n}^2} \right)
\end{eqnarray*}

\subsection{Evaluating a Point Estimator}

Let $\textbf{X} = (X_1, \dots, X_n)$ be a sample from a population whose distribution is specified up to an unknown parameter $\theta$, and let $d = d(\textbf{X})$ be an estimator of $\theta$. How are we to determine its worth as an estimator of $\theta$? One way is to consider the square of the difference between $d(\textbf{X})$ and $\theta$. However, since $(d(\textbf{X}) - \theta)^2$ is a random variable, let us agree to consider $r(d, \theta)$, the \textbf{mean square error} of the estimator $d$, which is defined by
\begin{equation*}
  r(d, \theta) = E[(d(\textbf{X}) - \theta)^2]
\end{equation*}

as an indication of the worth of $d$ as an estimator of $\theta$.

\begin{definition}
  let $d = d(\textbf{X})$ be an estimator of the parameter $\theta$. Then
  \begin{equation*}
    b_{\theta}(d) = E[d(\textbf{X})] - \theta
  \end{equation*}
  is called the \textbf{bias} of $d$ as an estimator of $\theta$. If $b_{\theta}(d) = 0$ for all $\theta$, then we say that $d$ is an \textbf{unbiased} estimator of $\theta$.
\end{definition}

If $d(X_1, \dots, X_n)$ is an unbiased estimator, then its mean square error is given by
\begin{eqnarray*}
  r(d, \theta)
  &=& E[(d(\textbf{X}) - \theta)^2] \\
  &=& E[(d(\textbf{X}) - E[d(\textbf{X})])^2] \\
  &=& Var(d(\textbf{X}))
\end{eqnarray*}

Thus the mean square error of an unbiased estimator is equal to its variance. \\

A generalization of the result that the mean square error of an unbiased estimator is equal to its variance is that the mean square error of any estimator is equal to its variance plus the square of its bias. This follows since
\begin{eqnarray*}
  r(d, \theta)
  &=& E[(d(\textbf{X}) - \theta)^2] \\
  &=& E[(d - E[d] + E[d] - \theta)^2] \\
  &=& E[ (d - E[d])^2 + (E[d] - \theta)^2 + 2(E[d] - \theta)(d - E[d]) ] \\
  &=& E[(d - E[d])^2] + E[(E[d] - \theta)^2] \\
  &&  + 2E[(E[d] - \theta)(d - E[d])] \\
  &=& E[(d - E[d])^2] + (E[d] - \theta)^2 
  + 2(E[d] - \theta)E[d - E[d]] \\
  &=& E[(d - E[d])^2] + (E[d] - \theta)^2
\end{eqnarray*}

The last equality follows since
\begin{equation*}
  E[d - E[d]] = 0
\end{equation*}

Hence
\begin{equation*}
  r(d, \theta) = Var(d) + b_{\theta}^2 (d)
\end{equation*}

\subsection{The Bayes Estimator}

In certain situations it seems reasonable to regard an unknown parameter $\theta$ as being the value of a random variable from a given probability distribution. This usually arises when, prior to the observance of the outcomes of the data $X_1, \dots, X_n$, we have some information about the value of $\theta$ and this information is expressible in terms of a probability distribution (called appropriately the \textbf{prior} distribution of $\theta$). \\

Suppose now that our prior feelings about $\theta$ are that it can be regarded as being the value of a continuous random variable having probability density function $p(\theta)$; and suppose that we are about to observe the value of a sample whose distribution depends on $\theta$. Specially, suppose that $f(x | \theta)$ represents the likelihood--that ism it is the probability mass function in the discrete case or the probability density function in the continuous case-- that a data value is equal to $x$ when $\theta$ is the value of the parameter. If the observed data values are $X_i = x_i, i =1,\dots,n$, then the updated, or conditional, probability density function of $\theta$ is as follows:
\begin{eqnarray*}
  f(\theta | x_1, \dots, x_n)
  &=& \frac {f(\theta, x_1, \dots, x_n)}{f(x_1, \dots, x_n)} \\
  &=& \frac {p(\theta) f(x_1, \dots, x_n | \theta)}
  {\int f(x_1, \dots, x_n | \theta) p(\theta) \; d \theta}
\end{eqnarray*}

The condition density function $f(\theta | x_1, \dots, x_n)$ is called the \textbf{posterior} density function. (Thus, before observing the data, one's feelings about $\theta$ are expressed in terms of the prior distribution, whereas once the data are observed, this prior distribution is updated to yield the posterior distribution.) \\

Now we have shown that whenever we are given the probability distribution of a random variable, the best estimate of the value of the random variable, in the sense of minimizing the expected squared error, is its mean. Therefore, it follows that the best estimate of $\theta$, given the data values $X_i = x_i, i= 1, \dots, n$, is the mean of the posterior distribution $f(\theta | x_1, \dots, x_n)$. This estimator, called the \textbf{Bayes estimator}, is written as $E[\theta | X_1, \dots, X_n]$. That is, if $X_i = x_i, i = 1, \dots, n$, then the value of the Bayes estimator is
\begin{eqnarray*}
  E[\theta | X_1 = x_1, \dots, X_n = x_n]
  = \int \theta f(\theta | x_1, \dots, x_n) \; d \theta
\end{eqnarray*}





\end{document}

