\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{law}[theorem]{Law}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Introduction to Probability and Statistics Note}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\maketitle

\tableofcontents

\setlength{\parindent}{0in}

\section{Introduction to Statistics}

A systematic collection of data on the population and the economy was begun in the Italian city-state of Venice and Florence during the Renaissance. The term \emph{statistics}, derived from the word \emph{state}, was used to refer to a collection of facts of interest to the state.

\section{Descriptive Statistics}

\subsection{Describing Data Sets}
\begin{enumerate}
  \item Frequency tables and graphs
  \item Relative frequency tables and graphs
  \item Histograms
  \item Stem and leaf plots
\end{enumerate}

\subsection{Summarizing Data Sets}

\begin{definition}
  The \textbf{sample mean}, designated by $\hat{x}$, is defined by
  \begin{equation*}
    \hat{x} = \sum_{i=1}^n x_i / n
  \end{equation*}
\end{definition}

\begin{definition}
  Order the values of a data set of size $n$ from smallest to largest. If $n$ is odd, the \textbf{sample median} is the value in position $(n+1)/2$; if $n$ is even, it is the average of the values in positions $n/2$ and $n/2+1$.
\end{definition}

\begin{definition}
  The \textbf{sample variance}, call it $s^2$, of the data set $x_1, \dots, x_n$ is defined by
  \begin{equation*}
    s^2 = \sum_{i=1}^n (x_i - \bar{x})^2/(n-1)
  \end{equation*}
\end{definition}

\begin{theorem}
  \begin{equation*}
    \sum_{i=1}^n (x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - n \bar{x}^2
  \end{equation*}
\end{theorem}

\begin{definition}
  The quantity $s$, defined by
  \begin{equation*}
    s = \sqrt{\sum_{i=1}^n (x_i - \bar{x})^2 / (n-1)}
  \end{equation*}
  is called the \textbf{sample standard deviation}.
\end{definition}

\begin{definition}
  The \textbf{sample 100p percentile} is that data value such that $100p$ percent of the data are less than or equal to it and $100(1-p)$ percent are greater than or equal to it. if two data values satisfy this condition, then the sample $100p$ percentile is the arithmetic average of these two values.
\end{definition}

\begin{definition}
  The sample $25$ percentile is called the \textbf{first quartile}; the sample 50 percentile is called the sample median or the \textbf{second quartile}; the sample 75 percentile is called the \textbf{third quartile}.
\end{definition}

\subsection{Chebyshev's Inequality}

\begin{theorem}[Chebyshev's Inequality]
  Let $\bar{x}$ and $s$ be the sample mean and sample standard deviation of the data set consisting of the data $x_1, \dots, x_n$, where $s > 0$. Let
  \begin{equation*}
    S_k = \{ i, 1 \le i \le n: |x_i - \bar{x}| < ks \}
  \end{equation*}
  and let $N(S_k)$ be the number of elements in the set $S_k$. Then, for any $k \ge 1$,
  \begin{equation*}
    \frac {N(S_k)}{n} \ge 1 - \frac{n-1}{nk^2} > 1 - \frac {1}{k^2}
  \end{equation*}
\end{theorem}

\begin{theorem}[The One-Sided Chebyshev Inequality]
  For $k>0$, let
  \begin{equation*}
    N(k) = \text{number of } i: x_i - \bar{x} \ge ks
  \end{equation*}
  Then we have
  \begin{equation*}
    \frac{N(k)}{n} \le \frac{1}{1+k^2}
  \end{equation*}
\end{theorem}

\subsection{Normal Data Sets}

\begin{remark} [The Empirical Rule]
  If a data set is approximately normal with sample mean $\bar{x}$ and sample standard deviation $s$, then the following statements are true.
  \begin{enumerate}
    \item Approximately 68 percent of the observations lie within
      \begin{equation*}
        \bar{x} \pm s
      \end{equation*}
    \item Approximately 95 percent of the observations lie within
      \begin{equation*}
        \bar{x} \pm 2s
      \end{equation*}
    \item Approximately 99.7 percent of the observations lie within
      \begin{equation*}
        \bar{x} \pm 3s
      \end{equation*}
  \end{enumerate}
\end{remark}

\subsection{Paired Data Sets and the Sample Correlation Coefficient}

\begin{definition}
  Let $s_x$ and $s_y$ denote, respectively, the sample standard deviations of the $x$ values and the $y$ values. The \textbf{sample correlation coefficient}, call it $r$, of the data pairs $(x_i,y_i),i=1,\dots,n$ is defined by
  \begin{eqnarray*}
    r
    &=& \frac {\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{(n-1) s_x s_y} \\
    &=& \frac {\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}
    { \sqrt{\sum_{i=1}^n (x_i - \bar{x}) \sum_{i=1}^n (y_i - \bar{y})} } \\
  \end{eqnarray*}
  When $r > 0$ we say that the sample data pairs are \textbf{positively correlated}, when $r<0$ we say that they are \textbf{negatively correlated}.
\end{definition}

Properties of $r$:
\begin{enumerate}
  \item $-1 \le r \le 1$
  \item If for constants $a$ and $b$, with $b>0$,
    \begin{equation*}
      y_i = a + b x_i, \;\; i = 1,\dots,n
    \end{equation*}
    then $r = 1$.
  \item If for constants $a$ and $b$, with $b<0$,
    \begin{equation*}
      y_i = a + b x_i, \;\; i = 1,\dots,n
    \end{equation*}
    then $r = -1$.
  \item If $r$ is the sample correlation coefficient for the data pairs $x_i, y_i,i=1,\dots,n$ then it is also the sample correlation coefficient for the data pairs
    \begin{equation*}
      a + b x_i, \;\; c + d y_i, \;\; i = 1,\dots,n
    \end{equation*}
    provided that $b$ and $d$ are both positive or both negative.
\end{enumerate}

\section{Elements of Probability}

\subsection{Sample Space and Events}

\begin{definition}
  The set of all possible outcomes of an experiment is known as the \textbf{sample space} of the experiment and is denoted by $S$.
\end{definition}

\subsection{Venn Diagrams and the Algebra of Events}

\begin{law}
  \begin{enumerate}
    \item Commutative law
      \begin{eqnarray*}
        E \cup F &=& F \cup E \\
        EF &=& FE
      \end{eqnarray*}
    \item Associative law
      \begin{eqnarray*}
        (E \cup F) \cup G &=& E \cup (F \cup G) \\
        (EF)G &=& E(FG)
      \end{eqnarray*}
    \item Distributive law
      \begin{eqnarray*}
        (E \cup F)G &=& EG \cup FG \\
        EF \cup G &=& (E \cup G)(F \cup G
      \end{eqnarray*}
    \item DeMorgan's law
      \begin{eqnarray*}
        (E \cup F)^C &=& E^CF^C \\
        (EF)^C &=& E^C \cup F^C
      \end{eqnarray*}
  \end{enumerate}
\end{law}

\subsection{Axioms of Probability}

\begin{axiom}
  $0 \le P(E) \le 1$
\end{axiom}

\begin{axiom}
  $P(S) = 1$
\end{axiom}

\begin{axiom}
  For any sequence of mutually exclusive events $E_1, E_2,\dots$(that is, events for which $E_iE_j = \emptyset$ when $i \neq j$),
  \begin{equation*}
    P \left( \bigcup_{i=1}^n E_i \right)
    = \sum_{i=1}^n P(E_i), \;\;\;\; n=1,2,\dots,\infty
  \end{equation*}
  We call $P(E)$ the probability of the event $E$.
\end{axiom}

\begin{proposition}
  \begin{equation*}
    P(E^C) = 1 - P(E)
  \end{equation*}
\end{proposition}

\begin{proposition}
  \begin{equation*}
    P(E \cup F) = P(E) + P(F) - P(EF)
  \end{equation*}
\end{proposition}

\subsection{Sample Spaces Having Equally Likely Outcomes}

\begin{theorem} [Basic Principle of Counting]
  Suppose that two experiments are to be performed. Then if experiment 1 can result in any one of $m$ possible outcomes and if, for each outcome of experiment 1, there are $n$ possible outcomes of experiment 2, then together there are $mn$ possible outcomes of the two experiments.
\end{theorem}

\begin{theorem} [Generalized Basic Principle of Counting]
  If $r$ experiments that are to be performed are such that the first one may result in any of $n_1$ possible outcomes, and if for each of these $n_1$ possible outcomes there are $n_2$ possible outcomes of the second experiment, and if for each of the possible outcomes of the third experiment, and if,..., then there are a total of $n_1 \cdot n_2 \cdots n_r$ possible outcomes of the $r$ experiments.
\end{theorem}

\begin{definition}
  We define $\binom {n}{r}$, for $r \le n$, by
  \begin{equation*}
    \binom {n}{r} = \frac {n!}{(n-r)!r!}
  \end{equation*}
  and call $\binom {n}{r}$ the number of \textbf{combinations} of $n$ objects taken $r$ at a time.
\end{definition}

\subsection{Conditional Probability}

\begin{definition}
  The probability of $E$ given that $F$ has occurred is denoted by
  \begin{equation*}
    P(E|F) = \frac {P(EF)}{P(F)}
  \end{equation*}
\end{definition}

\subsection{Bayes' Formula}

\begin{theorem} [Bayes' Theorem]
  \begin{equation*}
    P(A|B) = \frac {P(B|A)P(A)}{P(B)}
  \end{equation*}
\end{theorem}

\subsection{Independent Events}

\begin{definition}
  Two events $E$ and $F$ are said to be \textbf{independent} if
  \begin{equation*}
    P(EF) = P(E)P(F)
  \end{equation*}
  holds. Two events $E$ and $F$ that are not independent are said to be \textbf{dependent}.
\end{definition}

\begin{proposition}
  If $E$ and $F$ are independent, then so are $E$ and $F^C$.
\end{proposition}

\begin{definition}
  The three events $E,F$ and $G$ are said to be \textbf{independent} if
  \begin{eqnarray*}
    P(EFG) &=& P(E)P(F)P(G) \\
    P(EF) &=& P(E)P(F) \\
    P(EG) &=& P(E)P(G) \\
    P(FG) &=& P(F)P(G)
  \end{eqnarray*}
  holds. Two events $E$ and $F$ that are not independent are said to be \textbf{dependent}.
\end{definition}

It should be noted that if the events $E,F,G$ are independent, then $E$ will be independent of any event formed from $F$ and $G$.

\section{Random Variables and Expectation}

\subsection{Random Variables}

\begin{definition}
  A \textbf{random variable} is a mapping
  \begin{equation*}
    X: \; \Omega \rightarrow \mathbb{R}
  \end{equation*}
  that assigns a real number $X(\omega)$ to each outcome $\omega$.
\end{definition}

\subsection{Types of Random Variables}

A random variable whose set of possible values is a sequence is said to be \textbf{discrete}. For a discrete random variable $X$, we define the \textbf{probability mass function} $p(a)$ of $X$ by
\begin{equation*}
  p(a) = P\{X = a\}
\end{equation*}
Since $X$ must take on one of the values $x_i$, we have
\begin{equation*}
  \sum_{i=1}^{\infty} p(x_i) = 1
\end{equation*}
The cumulative distribution function $F$ can be expressed in terms of $p(x)$ by
\begin{equation*}
  F(a) = \sum_{\text{all } x \le a} p(x)
\end{equation*}
We say that $X$ is a \textbf{continuous} random variable if there exists a nonnegative function $f(x)$, defined for all real $x \in (-\infty, \infty)$, having the property that for any set $B$ of real numbers
\begin{equation*}
  P\{X \in B\} = \int_B f(x) dx
\end{equation*}
The function $f(x)$ is called the \textbf{probability density function} of the random variable $X$. \\

The relationship between the cumulative distribution $F(\cdot)$ and the probability density $f(\cdot)$ is expressed by
\begin{equation*}
  F(a) = P\{X \in (-\infty,a ]\} = \int_{-\infty}^a f(x) dx
\end{equation*}
Differentiating both sides yields
\begin{equation*}
  \frac {d}{da}F(a) = f(a)
\end{equation*}

\subsection{Jointly Distributed Random Variables}

\begin{definition}
  The joint cumulative probability distribution function of $X$ and $Y$ is
  \begin{equation*}
    F(x,y) = P\{ X \le x, Y \le y \}
  \end{equation*}
\end{definition}

The distribution function of $X$, $F_X$, can be obtained from the joint distribution function $F$ of $X$ and $Y$:
\begin{eqnarray*}
  F_X(x) = F(x, \infty)
\end{eqnarray*}

\subsection{Independent Random Variables}

\begin{definition}
  The random variables $X$ and $Y$ are said to be independent if for any two sets of real numbers $A$ and $B$
  \begin{equation*}
    P \{ X \in A, Y \in B \} = P\{ X \in A \} P\{ Y \in B \}
  \end{equation*}
\end{definition}

In terms of joint distribution function $F$ of $X$ and $Y$, we have that $X$ and $Y$ are independent if
\begin{equation*}
  F(a,b) = F_X(a)F_X(b) \;\;\;\; \text{for all } a,b
\end{equation*}

\subsection{Conditional Distributions}

If $X$ and $Y$ are discrete random variables, it is natural to define the conditional probability mass function of $X$ given that $Y=y$, by
\begin{eqnarray*}
  p_{X|Y}(x|y)
  &=& P\{ X = x | Y = y \} \\
  &=& \frac {P\{ X = x, Y = y \}}{P\{ Y = y \}} \\
  &=& \frac {p(x,y)}{p_Y(y)}
\end{eqnarray*}
for all values of $y$ such that $p_Y(y) > 0$.

\subsection{Expectation}

\begin{definition}
  If $X$ is a discrete random variable, then the expectation or expected value of $X$, denoted by $E[X]$, is defined by
  \begin{equation*}
    E[X] = \sum_i x_i P\{ X = x_i \}
  \end{equation*}
  If $X$ is a continuous random variable, then the expectation is defined by
  \begin{equation*}
    E[X] = \int_{-\infty}^{\infty} x f(x) dx
  \end{equation*}
\end{definition}

\subsection{Properties of the Expected Value}

\begin{proposition}
  \begin{itemize}
    \item If $X$ is a discrete random variable with probability mass function $p(x)$, then for any real-valued function $g$,
      \begin{equation*}
        E[g(x)] = \sum_x g(x)p(x)
      \end{equation*}
    \item If $X$ is a continuous random variable with probability density function $f(x)$, then for any real-valued function $g$,
      \begin{equation*}
        E[g(x)] = \inf_{-\infty}^{\infty} g(x)f(x) dx
      \end{equation*}
  \end{itemize}
\end{proposition}

\begin{corollary}
  If $a$ and $b$ are constants, then
  \begin{equation*}
    E[aX + b] = a E[X] + b
  \end{equation*}
\end{corollary}

\subsection{Expected Value of Sums of Random Variables}

If $X$ and $Y$ are random variables and $g$ is a function of two variables, then
\begin{eqnarray*}
  E[g(X,Y)]
  &=& \sum_y \sum_x g(x,y) p(x,y) \\
  &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f(x,y) dx dy
\end{eqnarray*}

\begin{remark}
  For any $n$,
  \begin{equation*}
    E[X_1+X_2+\dots+X_n] = E[X_1] + E[X_2] + \dots + E[X_n]
  \end{equation*}
\end{remark}

\subsection{Variance}

\begin{definition}
  If $X$ is a random variable with mean $\mu$, then the \textbf{variance} of $X$, denoted by $Var(X)$, is defined by
  \begin{equation*}
    Var(X) = E[(X-\mu)^2]
  \end{equation*}
\end{definition}

\begin{remark}
  \begin{equation*}
    Var(X) = E[X^2] - (E[X])^2
  \end{equation*}
\end{remark}

The quantity $\sqrt{Var(X)}$ is called the \textbf{standard deviation} of $X$. The standard deviation has the same units as does the mean.

\subsection{Covariance and Variance of Sums of Random Variables}

\begin{definition}
  The \textbf{covariance} of two random variables $X$ and $Y$, written $Cov(X,Y)$, is defined by
  \begin{equation*}
    Cov(X,Y) = E[(X-\mu_x)(Y-\mu_y)]
  \end{equation*}
  where $\mu_x$ and $\mu_y$ are the means of $X$ and $Y$, respectively.
\end{definition}

\begin{remark}
  \begin{equation*}
    Cov(X,Y) = E[XY] - E[X]E[Y]
  \end{equation*}
\end{remark}

\begin{remark}
  \begin{eqnarray*}
    Cov(X,Y) &=& Cov(Y,X) \\
    Cov(X,X) &=& Var(X) \\
    Cov(aX,Y) &=& a \: Cov(X,Y)
  \end{eqnarray*}
\end{remark}

\begin{lemma}
  \begin{equation*}
    Cov(X+Y,Z) = Cov(X,Z) + Cov(Y,Z)
  \end{equation*}
\end{lemma}

\begin{proposition}
  \begin{equation*}
    Cov \left( \sum_{i=1}^n X_i, \sum_{j=1}^m Y_j \right)
    = \sum_{i=1}^n \sum_{j=1}^m {Cov(X_i, Y_j)}
  \end{equation*}
\end{proposition}

\begin{corollary}
  \begin{equation*}
    Var(\sum_{i=1}^n X_i)
    = \sum_{i=1}^n {Var(X_i)} 
    + \sum_{i=1}^n \sum_{j = 1, j \neq i}^n {Cov(X_i, X_j)}
  \end{equation*}
\end{corollary}

\begin{theorem}
  If $X$ and $Y$ are independent random variables, then
  \begin{equation*}
    Cov(X,Y) = 0
  \end{equation*}
  and so for independent events $X_1,\dots,X_n$,
  \begin{equation*}
    Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n {Var(X_i)}
  \end{equation*}
\end{theorem}

\subsection{Moment Generating Function}

The moment generating function $\phi(t)$ of the random variable $X$ is defined for all values $t$ by
\begin{equation*}
  \phi(t) = E[e^{tX}] =
  \begin{cases}
    \sum_x e^{tx} p(x) & \text{if $X$ is discrete} \\
    \int_{-\infty}^{\infty} e^{tx} f(x) dx & \text{if $X$ is continuous}
  \end{cases}
\end{equation*}

We call $\phi(t)$ the moment generating function because all of the moments of $X$ can be obtained by successively differentiating $\phi(t)$:
\begin{equation*}
  \phi^n(0) = E[X^n], \;\;\;\; n \ge 1
\end{equation*}

\subsection{Chebyshev's Inequality and the Weak Law of Large Numbers}

\begin{proposition} [Markov's Inequality]
  If $X$ is a random variable that takes only nonnegative values, then for any value $a>0$
  \begin{equation*}
    P\{ X \ge a \} \ge \frac {E[X]}{a}
  \end{equation*}
\end{proposition}

\begin{proposition} [Chebshev's Inequality]
  If $X$ is a random variable with mean $\mu$ and variance $\sigma^2$, then for any value $k>0$
  \begin{equation*}
    P\{ |X-\mu| \ge k \} \le \frac {\sigma^2}{k^2}
  \end{equation*}
\end{proposition}

\subsection{The Weak Law of Large Numbers}

\begin{law}
  Let $X_1, X-2,\dots,$be a sequence of independent and identically distributed random variables, each having mean $E[X_i] = \mu$. Then, for any $\epsilon > 0$,
  \begin{equation*}
    P \left\{ \left|\frac{X_1 + \dots + X_n}{n} - \mu \right| 
        > \epsilon \right\} \rightarrow 0 \text{  as } n \rightarrow \infty
  \end{equation*}
\end{law}

\section{Special Random Variables}

\subsection{The Bernoulli and Binomial Random Variables}

Suppose that a trial, or an experiment, whose outcome can be classified as either a ``success'' or as a ``failure'' is performed. If we let $X=1$ when the outcome is a success and $X=0$ when it is a failure, then the probability mass function of $X$ is given by
\begin{eqnarray*}
  P\{X=0\} &=& 1-p \\
  P\{X=1\} &=& p \\
\end{eqnarray*}
where $p, 0 \le p \le 1$, is the probability that the trial is a ``success''.This random variable $X$ is a Bernoulli random variable. Then we have
\begin{eqnarray*}
  E[X] &=& 1 \cdot P\{X=1\} + 0 \cdot P\{X=0\} = p \\
  Var(X) &=& (1-p)^2 \cdot P\{X=1\} + (0-p)^2 \cdot P\{X=0\} = p(1-p)
\end{eqnarray*}

Suppose now that $n$ independent trials, each of which results in a ``success'' with probability $p$ and in a ``failure'' with probability $1-p$, are to be performed. If $X$ represents the number of successes that occur in the $n$ trials, then $X$ is said to be a \textbf{binomial} random variable with parameters $(n,p)$. \\

The probability mass function of a binomial random variable with parameters $n$ and $p$ is given by
\begin{equation*}
  P\{ X=i \} = \binom{n}{i} p^i (1-p)^{n-i}, \;\;\;\; i =0,1,\dots,n
\end{equation*}

Since a binomial random variable, with parameters $n$ and $p$, represents the number of successes in $n$ independent trials, each having success probability $p$, we can represent $X$ as follows:
\begin{equation*}
  X = \sum_{i=1}^n X_i
\end{equation*}
where
\begin{equation*}
  X_i =
  \begin{cases}
    1 & \text{if the $i$th trial is a success} \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}
Using this representation, we get
\begin{eqnarray*}
  E[X]
  &=& \sum_{i=1}^n E[X_i] \\
  &=& np \\
  Var(X)
  &=& \sum_{i=1}^n Var(X_i) \\
  &=& np(1-p)
\end{eqnarray*}
The distribution function is
\begin{equation*}
  P\{ X \le i \}
  = \sum_{k=0}^i \binom{n}{k} p^k (1-p)^{n-k} \;\;\;\; i = 0,1,\dots,n
\end{equation*}
The relationship between $P\{ X = k+1 \}$ and P\{ X = k \}:
\begin{equation*}
  P\{ X = k+1 \}
  = \frac{p}{1-p} \frac{n-k}{k+1} P\{ X = k \}
\end{equation*}

\subsection{The Poisson Random Variables}

A random variable $X$, taking on one of one of the values 0, 1, 2, ..., is said to be a Poisson random variable with parameter $\lambda$, $\lambda>0$, if its probability mass function is given by
\begin{equation*}
  P\{ X=i \} = e^{-\lambda} \frac{\lambda^i}{i!} \;\;\;\; i = 0,1,\dots
\end{equation*}
Then we get
\begin{eqnarray*}
  E[X] &=& \lambda \\
  Var(X) &=& \lambda
\end{eqnarray*}
If $X$ is Poisson with mean $\lambda$, then
\begin{equation*}
  \frac {P\{X=i+1\}}{P\{X=i\}}
  = \frac {\lambda}{i+1}
\end{equation*}

\subsection{The Hypergeometric Random Variable}

A random variable $X$ is a \textbf{hypergeometric} random variable if its mass function is
\begin{equation*}
  P\{ X = i \} = \frac
  {\binom{N}{i} \binom{M}{n-i}}
  {\binom{N+M}{n}} \;\;\;\; i = 0,1,\dots,\min(N,n)^*
\end{equation*}
Let
\begin{equation*}
  p = \frac {N}{N+M}
\end{equation*}
We get
\begin{eqnarray*}
  E[X] &=& \sum_{i=1}^n E[X_i] = \sum_{i=1}^n P[X_i=1] = \frac{nM}{N+M} = np \\
  Var(X) &=& \sum_{i=1}^n Var(X_i) + 
  2 \sum_{i=1}^n \sum_{j \ge i}^n Cov(X_i, X_j) \\
  &=& \frac{2NM}{(N+M)^2} - \frac{n(n-1)NM}{(N+M)^2(N+M-1)} \\
  &=& \frac{nNM}{(N+M)^2} \left( 1 - \frac{n-1}{N+M-1} \right) \\
  &=& np(1-p) \left[ 1 - \frac{n-1}{N+M-1} \right]
\end{eqnarray*}

\subsection{The Uniform Random Variable}

A random variable $X$ is said to be uniformly distributed over the interval $[\lambda, \beta]$ if its probability density function is given by
\begin{equation*}
  f(x) = 
  \begin{cases}
    \frac{1}{\beta - \lambda} & \text{if } \alpha \le x \le \beta \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}
Then we get
\begin{eqnarray*}
  E[X] &=& \frac{\lambda + \beta}{2} \\
  Var(X) = \frac{(\beta - \lambda)^2}{12}
\end{eqnarray*}

\subsection{Normal Random Variable}

A random variable is said to be normally distributed with parameters $\mu$ and $\sigma^2$, and we write $X ~ \mathcal{N}(\mu, \sigma^2)$, if its density is
\begin{equation*}
  f(x) = \frac{1}{sqrt{2 \pi \sigma}} e^{-(x-\mu)^2/2\sigma^2}
\end{equation*}
Let
\begin{equation*}
  Z = \frac{X-\mu}{\sigma}
\end{equation*}
Then $Z$ is a normal random variable with mean 0 and variance 1. $Z$ is said to have a \textbf{standard normal distribution} $\Phi(\cdot)$:
\begin{equation*}
  \Phi(x) = \frac{1}{sqrt{2 \pi}} \int_{-\infty}^x e^{-y^2/2} dy
  \;\;\;\; -\infty < x < \infty
\end{equation*}

\subsection{Exponential Random Variables}

A continuous random variable whose probability density function is given, for some $\lambda > 0$, by
\begin{equation*}
  f(x) =
  \begin{cases}
    \lambda e^{-\lambda x} & \text{if } x \ge 0 \\
    0 & \text{if } x < 0
  \end{cases}
\end{equation*}
is said to be an \textbf{exponential random variable} with parameter $\lambda$. The cumulative distribution function $F(x)$ is given by
\begin{equation*}
  F(X) = 1 - e^{-\lambda x}, \;\;\;\; x \ge 0
\end{equation*}
We get
\begin{eqnarray*}
  \phi(t) &=& \frac{\lambda}{\lambda - t} \;\;\;\; t < \lambda \\
  E[X] = \frac{1}{\lambda} \\
  Var(X) = \frac{1}{\lambda^2}
\end{eqnarray*}

\begin{remark}
  The key property of an exponential random variable is that it is memoryless, where we say that a nonnegative random variable $X$ is \textbf{memoryless} if
  \begin{equation*}
    P\{ X > s+t | X>t \} = P\{ X >s \} \;\;\;\; \text{for all } s,t \ge 0
  \end{equation*}
\end{remark}

\begin{proposition}
  If $X_1,X_2,\dots,X_n$ are independent exponential random variables having respective parameter $\lambda_1, \lambda_2, \dots, \lambda_n$, then $\min(X_1, X_2,\dots,X_n)$ is exponential with parameter $\sum_{i=1}^n \lambda_i$.
\end{proposition}


\section{Distribution of Sampling Statistics}

\subsection{Introduction}

\begin{definition}
  If $X_1,\dots,X_n$ are independent random variables having common distribution $F$, then we say they constitute a \textbf{sample} (sometimes called a \emph{random sample}) from the distribution $F$.
\end{definition}

\subsection{The Sample Mean}

We often suppose that the value associated with any member of the population can be regarded as being the value of a random variable having expectation $\mu$ and variance $\sigma^2$. The quantities $\mu$ and $\sigma^2$ are called the \textbf{population mean} and the \textbf{population variance} respectively. Let $X_1, X_2, \dots, X_n$ be a sample of values from this population. The sample mean is defined by
\begin{equation*}
  \bar{X} = \frac{X_1 + \cdots + X_n}{n}
\end{equation*}
Then its expected value and variance are obtained as follows:
\begin{equation*}
  E[\bar{X}] = E \left[ \frac{X_1 + \cdots + X_n}{n} \right] = \mu
\end{equation*}
and
\begin{equation*}
  Var(\bar{X}) = Var \left( \frac{X_1 + \cdots + X_n}{n} \right)
= \frac{\sigma^2}{n}
\end{equation*}

\subsection{The Central Limit Theorem}

\begin{theorem} [The Central Limit Theorem]
  Let $X_1, X_2,\dots,X_n$ be a sequence of independent and identically distributed random variables each having mean $\mu$ and variance $\sigma^2$. Then for $n$ large, the distribution of
  \begin{equation*}
    X_1 + \cdots + X_n
  \end{equation*}
  is approximately normal with mean $n\mu$ and variance $n\sigma^2$.
\end{theorem}

It follows from the central limit theorem that
\begin{equation*}
  \frac {X_1 + \cdots + X_n - n\mu}{\sigma \sqrt{n} < x} \approx P(Z < x)
\end{equation*}


\end{document}



