\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Introduction to Algorithms Note}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\maketitle

\tableofcontents

\setlength{\parindent}{0in}

\section{The Role of Algorithms in Computing}

Informally, an \textbf {algorithm} is any well-defined computational procedure that takes some value, or set of values, as \textbf {input} and produces some value, or set of values, as \textbf {output}. An algorithm is thus a sequence of computational steps that transform the input into the output.

\section{Getting Started}

\subsection{Insertion sort}

INSERTION-SORT(A)
\begin{algorithmic}[1]
\For {$j = 2 \textbf { to } A.legnth$}
	\State $key = A[j]$
        \State // Insert $A[j]$ into the sorted sequence $A[1..j-1]$.
        \State $i = j - 1$
        \While {$i > 0$ and $A[i] > key$}
        	\State $A[i+1] = A[i]$
                \State $i = i - 1$
        \EndWhile
        \State $A[i+1] = key$
\EndFor
\end{algorithmic}

\subsection{Merge sort}

MERGE($A, p, q, r$)
\begin{algorithmic}[1]
\State $n_1 = q - p + 1$
\State $n_2 = r - q$
\State let $L[1..n_1+1]$ and $R[1..n_2+1]$ be new arrays
\For {$i = 1 \textbf { to } n_1$}
	\State $L[i] = A[p+i-1]$
\EndFor
\For {$j = 1 \textbf { to } n_2$}
	\State $R[j] = A[q+j]$
\EndFor
\State $L[n_1 + 1] = \infty$
\State $R[n_2 + 1] = \infty$
\State $i = 1$
\State $j = 1$
\For {$k = p \textbf { to } r$}
	\If {$L[i] \le R[j]$}
        	\State $A[k] = L[i]$
                \State $i = i + 1$
        \Else
        	\State $A[k] = R[j]$
        	\State $j = j + 1$
        \EndIf
\EndFor
\end{algorithmic}

MERGE-SORT($A, p, r$)
\begin{algorithmic}[1]
\If {$p < r$}
	\State $q = \lfloor (p+r)/2 \rfloor$
        \State MERGE-SORT($A, p, q$)
        \State MERGE-SORT($A, q+1, r$)
        \State MERGE($A, p, q, r$)
\EndIf
\end{algorithmic}

\section{Growth of Functions}

\subsection{Asymptotic notation}

\begin{eqnarray*}
  \Theta (g(n)) = \{ f(n): && \text{there exist positive constants
    $c_1$, $c_2$, and $n_0$ such that} \\
  && 0 \le c_1 g(n) \le f(n) \text{ for all } n \ge n_0 \} \\
  O(g(n)) = \{ f(n): && \text{there exist positive constants
    $c$ and $n_0$ such that} \\
  && 0 \le f(n) \le cg(n) \text{ for all } n \ge n_0 \} \\
  \Omega (g(n)) = \{ f(n): && \text{there exist positive constants
    $c$ and $n_0$ such that} \\
  && 0 \le cg(n) \le f(n) \text{ for all } n \ge n_0 \}
\end{eqnarray*}

\subsection{Common functions}

\begin{eqnarray*}
  e^x &=& \sum_0^{\infty} \frac {x^i}{i!} \\
  e^x &=& \lim_{n \rightarrow \infty} (1 + \frac {x}{n})^n \\
  n!  &=& \sqrt {2 \pi n} \left(\frac {n}{e} \right)^n 
          \left( 1 + \Theta \left( \frac {1}{n} \right) \right)
\end{eqnarray*}

\section{Divide-and-Conquer}

\subsection{The master method}

\begin{theorem}
  Let $a \ge 1$ and $b \ge 1$ be constants, let $f(n)$ be a function, and let $T(n)$ be defined on the nonnegative integers by recurrence
  \begin{equation*}
    T(n) = aT(n/b) + f(n),
  \end{equation*}
  where we interpret $n/b$ to mean either $\lfloor n/b \rfloor$ or $\lceil n/b \rceil$. Then $T(n)$ can be bounded asymptotically as follows.
  \begin{enumerate}
  \item If $f(n) = O(n^{\log_b^a - \epsilon})$ for some constant $\epsilon > 0$, then $T(n) = \Theta (n^{\log_b a})$.
  \item If $f(n) = O(n^{\log_b^a})$, then $T(n) = \Theta (n^{\log_b a} \lg n)$.
  \item If $f(n) = O(n^{\log_b^a + \epsilon})$ for some constant $\epsilon > 0$, and if $af(n/b) \le cf(n)$ for some constant $c < 1$ and all sufficiently large $n$, then $T(n) = \Theta \left( f(n) \right)$.
  \end{enumerate}
\end{theorem}

\section{Probabilistic Analysis and Randomized Algorithms}

\subsection{Indicator random variables}

Suppose we are given a sample space $S$ and an event $A$. Then the \textbf {indicator random variable} $I\{A\}$ associated with event A is defined as
\begin{equation*}
  I\{A\} =
  \begin{cases}
    1 & \text{if $A$ occurs,} \\
    0 & \text{if $A$ does not occur.}
  \end{cases}
\end{equation*}

\begin{lemma}
  Given a sample space $S$ and an event $A$ in the sample space $S$, let $X_A = I\{ A \}$. Then $E[X_A] = Pr \{ A \}$.
\end{lemma}

\subsection{The hiring problem}

HIRE-ASSISTANT($n$)
\begin{algorithmic}[1]
\State best = 0
\Comment candidate $0$ is a least-qualified dummy candidate
\For {$i = 1 \textbf { to } n$}
	\State interview candidate $i$
        \If {candidate $i$ is better than candidate $best$}
        	\State $best = i$
                \State hire candidate $i$
        \EndIf
\EndFor
\end{algorithmic}

\begin{lemma}
  Assuming that the candidates are presented in a random order, algorithm HIRE-ASSISTANT has a total hiring cost of $O(c_h \ln n)$.
\end{lemma}

Let $X$ be the random variable whose value equals the number of times we hire a new office assistant. Now we can compute $E[X]$:
\begin{eqnarray*}
  E[X]
  &=& E \left[ \sum_{i=1}^n X_i \right] \\
  &=& \sum_{i=1}^n E[X_i] \\
  &=& \sum_{i=1}^n 1/i \\
  &=& \ln n + O(1)
\end{eqnarray*}

\subsection{Probabilistic analysis and further uses of indicator random variables}

\subsubsection{The birthday paradox}

How many people must be in a room before there is a $50 \%$ chance that two of them were born on the same day of the year? \\

The event that $k$ people have distinct birthday is
\begin{equation*}
  B_k = \bigcap_{i=1}^k A_i,
\end{equation*}
where $A_i$ is the event that person $i$'s birthday is different from person $j$'s for all $j < i$. Since we can write $B_k = A_k \cap B_{k-1}$, we get
\begin{eqnarray*}
  Pr \{ B_k \} 
  &=& Pr \{ B_{k-1} \} Pr \{ A_k | B_{k-1} \} \\
  &=& Pr \{ B_{k-2} \} Pr \{ A_{k-1} | B_{k-2} \} Pr \{ A_{k} | B_{k-1} \} \\
  &\vdots& \\
  &=& Pr \{ B_{1} \} Pr \{ A_{2} | B_{1} \} Pr \{ A_{3} | B_{2} \} \cdots
      Pr \{ A_{k} | B_{k-1} \} \\
  &=& 1 \cdot \left( \frac {n-1}{n} \right) \left( \frac {n-2}{n} \right)
      \cdots \left( \frac {n-k+1}{n} \right) \\
  &=& 1 \cdot \left( 1 - \frac {1}{n} \right) \left( 1 - \frac {2}{n} \right)
      \cdots \left( 1 - \frac {k-1}{n} \right)
\end{eqnarray*}
Inequality $1 + x \le e^x$ gives us
\begin{eqnarray*}
  Pr \{ B_k \} 
  &\le& e^{-1/n} e^{-2/n} \cdots e^{-(k-1)/n} \\
  &=& e^{- \sum_{i=1}^{k-1} i/n} \\
  &=& e^{-k(k-1)/2n} \\
  &\le& 1/2
\end{eqnarray*}
when $-k(k-1)/2n \le \ln(1/2)$. we get $k \ge (1 + \sqrt {1 + (8 \ln 2)n})/2$. For $n = 365$, we must have $k \ge 23$.

\subsubsection{Balls and bins}

Consider the process of randomly tossing identical balls into $b$ bins, numbered $1, 2, \dots, b$. The tosses are independent, and on each toss the ball is equally likely to end up in any bin. \\

\emph{How many balls must one toss until every bin contains at least one ball?} Let us call a toss in which a ball falls into an empty bin a ``hit''. We want to know the expected number $n$ of tosses required to get $b$ hits. \\

The hits can be used to partition the $n$ tosses into stages. The $i$th stage consists of the tosses after the $(i-1)$st hit until the $i$th hit. Let $n_i$ denote the number of tosses in the $i$th stage. We get
\begin{equation*}
  E[n_i] = \frac {b}{b - i + 1}
\end{equation*}
By linearity of expectation,
\begin{eqnarray*}
  E[n]
  &=& E \left[ \sum_{i=1}^b n_i  \right] \\
  &=& \sum_{i=1}^b [n_i] \\
  &=& \sum_{i=1}^b \frac {b}{b-i+1} \\
  &=& \sum_{i=1}^b \frac {1}{i} \\
  &=& b( \ln b + O(1)).
\end{eqnarray*}
The problem is also known as the \textbf {coupon collector's problem}.

\subsubsection{Streaks}

Suppose you flip a fair coin $n$ times. What is the longest streak of consecutive heads that you expect to see? The answer is $\Theta (\lg n)$. \\

Let $A_{i,k}$ be the event that a streak of heads of length at least $k$ begins with the $i$th coin flip or, more precisely, the event that the k consecutive coin flips $i, i + 1, \dots, i + k -1$ yield only heads, where $1 \le k \le n$ and $1 \le i \le n - k + 1$. Since coin flips are mutually independent, for any given event $A_{ik}$, the probability that all $k$ flips are heads is
\begin{equation*}
  Pr \{ A_{ik} \} = 1 / 2^k.
\end{equation*}
For $k = 2 \lceil \lg n \rceil$,
\begin{eqnarray*}
  Pr \{ A_{i, 2 \lceil \lg n \rceil} \}
  &=& 1 / 2^{2 \lceil \lg n \rceil} \\
  &\le& 1 / 2^{2 \lg n} \\
  &=& 1 / n^2
\end{eqnarray*}
There are at most $n - 2 \lceil \lg n \rceil + 1$ positions where such a streak can begin. The probability that a streak of heads of length at least $2 \lceil \lg n \rceil$ begins anywhere is therefore
\begin{eqnarray*}
  Pr \left\{ \bigcup_{i=1}^{n - 2 \lceil \lg n \rceil + 1} A_{i,2 \lceil \lg n \rceil} \right\}
  &\le&  \sum_{i=1}^{n - 2 \lceil \lg n \rceil + 1} 1 / n^2 \\
  &<& \sum_{i=1}^n 1 / n^2 \\
  &=& 1/n,
\end{eqnarray*}
since Boole's inequality,
\begin{equation*}
  Pr \{ A_1 \cup A_2 \cup \cdots \} \le Pr \{ A_1 \} + Pr \{ A_2 \} + \cdots,
\end{equation*}
the probability of a union of events is at most the sum of the probabilities of the individual events. \\

By the definition of expected value,
\begin{eqnarray*}
  E[L]
  &=& \sum_{j=0}^{n} j Pr \{ L_j \} \\
  &=& \sum_{j=0}^{2 \lceil \lg n \rceil - 1} j Pr \{ L_j \}
      + \sum_{j = 2 \lceil \lg n \rceil - 1}^{n} j Pr \{ L_j \} \\
  &<& \sum_{j=0}^{2 \lceil \lg n \rceil - 1} (2 \lceil \lg n \rceil - 1)
      Pr \{ L_j \} + \sum_{j = 2 \lceil \lg n \rceil - 1}^{n} n Pr \{ L_j \} \\
  &=& 2 \lceil \lg n \rceil - 1 \sum_{j=0}^{2 \lceil \lg n \rceil - 1} 
      Pr \{ L_j \} + n \sum_{j = 2 \lceil \lg n \rceil - 1}^{n} Pr \{ L_j \} \\
  &<& 2 \lceil \lg n \rceil - 1 \cdot 1 + n \cdot (1/n) \\
  &=& O(\lg n)
\end{eqnarray*}
The chances that a streak of heads exceeds $r \lceil \lg n \rceil$ flips diminish quickly with $r$. For $r \ge 1$, the probability that a streak of $r \lceil \lg n \rceil$ heads starts in position $i$ is
\begin{eqnarray*}
  Pr \{ A_{i, r \lceil \lg n \rceil} \}
  &=& 1 / 2^{r \lceil \lg n \rceil} \\
  &\le& 1 / n^r
\end{eqnarray*}
Thus, the probability is at most $n / n^r = 1 / n^{r-1}$ that the longest streak is at least $r \lceil \lg n \rceil$. \\

We now prove a complementary lower bound: the expected length of the longest streak of heads in $n$ coin flips is $\Omega (\lg n)$. To prove this bound, we look for streaks of length $s$ by partitioning the $n$ flips into approximately $n/s$ groups of $s$ flips each. If we choose $s = \lfloor (\lg n)/2 \rfloor$, we can show that it is likely that at least one of these groups comes up all heads, and hence it is likely that the longest streak has length at least $s = \Omega (\lg n)$. We then show that the longest streak has expected length $\Omega (\lg n)$.
\begin{eqnarray*}
  Pr \{ A_{i, \lfloor (\lg n)/2 \rfloor} \}
  &=& 1 / 2^{\lfloor (\lg n)/2 \rfloor} \\
  &\ge& 1 / \sqrt n.
\end{eqnarray*}
The probability that a streak of heads of length at least $\lfloor (\lg n)/2 \rfloor$ does not begin in position $i$ is therefore at most $1 - 1 / \sqrt n$. Since the $\lfloor n / \lfloor (\lg n)/2 \rfloor \rfloor$ groups are formed from mutually exclusive, independent coin flips, the probability that every one of these groups fails to be a streak of length $\lfloor (\lg n)/2 \rfloor$ is at most
\begin{eqnarray*}
  (1 - 1 / \sqrt n)^{\lfloor n/ {\lfloor (\lg n)/2 \rfloor} \rfloor}
  &\le& (1 - 1 / \sqrt n)^{n/ {\lfloor (\lg n)/2 \rfloor} - 1} \\
  &\le& (1 - 1 / \sqrt n)^{2n/ {\lg n} - 1} \\
  &\le& e^{-(2n/ {\lg n} -1)/{\sqrt n}} \\
  &=& O(e^{- \lg n}) \\
  &=& O(1/n)
\end{eqnarray*}
For this argument, we used $(2n / {\lg n} - 1) / {\sqrt n} \ge \lg n$ for sufficiently large $n$. \\

Thus , the probability that the longest streak exceeds $\lfloor (\lg n)/2 \rfloor$ is
\begin{equation*}
  \sum_{j = \lfloor (\lg n)/2 \rfloor + 1}^{n} Pr \{ L_j \} \ge 1 - O(1/n)
\end{equation*}
We can now calculate a lower bound on the expected length of the longest streak:
\begin{eqnarray*}
  E[L]
  &=& \sum_{j=0}^{n} j Pr \{ L_j \} \\
  &=& \sum_{j=0}^{\lfloor (\lg n)/2 \rfloor} j Pr \{ L_j \}
      + \sum_{j = \lfloor (\lg n)/2 \rfloor + 1}^{n} j Pr \{ L_j \} \\
  &\ge& \sum_{j=0}^{\lfloor (\lg n)/2 \rfloor} 0 \cdot Pr \{ L_j \}
      + \sum_{j = \lfloor (\lg n)/2 \rfloor + 1}^{n} 
      \lfloor (\lg n)/2 \rfloor Pr \{ L_j \} \\
  &=& 0 \cdot \sum_{j=0}^{\lfloor (\lg n)/2 \rfloor} \cdot Pr \{ L_j \}
      + \lfloor (\lg n)/2 \rfloor \sum_{j = \lfloor (\lg n)/2 \rfloor + 1}^{n}
      Pr \{ L_j \} \\
  &\ge& 0 + \lfloor (\lg n)/2 \rfloor (1 - O(1/n)) \\
  &=& \Omega (\lg n).
\end{eqnarray*}
As with the birthday paradox, we can obtain a simpler but approximate analysis using indicator random variables. We let $X_{ik} = I \{ A_{ik} \}$ be the indicator random variable associated with a streak of heads of length at least $k$ beginning with the $i$th coin flip. To count the total number of such streaks, we define
\begin{equation*}
  X = \sum_{i=1}^{n-k+1} X_{ik}.
\end{equation*}
Taking expectations and using linearity of expectation, we have
\begin{eqnarray*}
  E[X]
  &=& E \left[ \sum_{i=1}^{n-k+1} X_{ik} \right] \\
  &=& \sum_{i=1}^{n-k+1} E[X_{ik}] \\
  &=& \sum_{i=1}^{n-k+1} Pr \{ X_{ik} \} \\
  &=& \sum_{i=1}^{n-k+1} 1 / 2^k \\
  &=& \frac {n-k+1}{2^k}
\end{eqnarray*}
By plugging in various values for $k$, we can calculate the expected number of streaks of length $k$. If $k = c \lg n$, for some positive constant $c$, we obtain
\begin{eqnarray*}
  E[X]
  &=& \frac {n - c \lg n + 1}{2^{c \lg n}} \\
  &=& \frac {n - c \lg n + 1}{n^c} \\
  &=& \frac {1}{n^c - 1} - \frac {(c \lg n - 1)/n}{n^{c-1}} \\
  &=& \Theta(1 / n^{c-1}).
\end{eqnarray*}

\section{Heapsort}

Like merge sort, but unlike insertion sort, heapsort's running time is $O(n \lg n)$. Like insertion sort, but unlike merge sort, heapsort sorts in place: only a constant number of array elements are stored outside the input array at any time.

\subsection{Heaps}

PARENT($i$)
\begin{algorithmic}[1]
\State \textbf {return} $\lfloor i/2 \rfloor$
\end{algorithmic}

LEFT($i$)
\begin{algorithmic}[1]
\State \textbf {return} $2i$
\end{algorithmic}

RIGHT($i$)
\begin{algorithmic}[1]
\State \textbf {return} $2i+1$
\end{algorithmic}

There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds, the values in the nodes satisfy a {\bf heap property}, the specifics of which depends on the kind of heap. In a {\bf max-heap}, the {\bf max-heap perperty} is that for every node $i$ other than the root,
\begin{equation*}
  A[\text{PARENT}(i)] \ge A[i],
\end{equation*}
that is, the value of a node is at most the value of its parent. \\

Viewing a heap as a tree, we define the {\bf height} of a node in a heap to be the number of edges on the longest simple downward path from the node to a leaf, and we define the height of the heap to be the height of its root.

\subsection{Maintaining the heap property}

 MAX-HEAPIFY($A,i$)
\begin{algorithmic}[1]
\State $l = \text {LEFT}(i)$
\State $r = \text {RIGHT}(i)$
\If {$l \le A.\text{heap-size}$ and $A[l] > A[i]$}
	\State $largest = l$
\Else
	\State $largest = i$
\EndIf
\If {$r \le A.\text{heap-size}$ and $A[r] > A[largest]$}
	\State $largest = r$
\EndIf
\If {$largest \neq i$}
	\State exchange $A[i]$ with $A[largest]$
        \State MAX-HEAPIFY($A, largest$)
\EndIf
\end{algorithmic}

We can describe the running time of MAX-HEAPIFY by recurrence
\begin{equation*}
  T(n) \le T(2n/3) + \Theta(1).
\end{equation*}
The solution to this recurrence is $T(n) = O(\lg n)$. Alternatively, we can characterize the running time of MAX-HEAPIFY on a node of height $h$ as $O(h)$.

\subsection{Building a heap}

BUILD-MAX-HEAP(A)
\begin{algorithmic}[1]
\State $A.\text{heap-size} = A.length$
\For {$i = \lfloor A.length/2 \rfloor \text {{ \bf downto} } 1$}
	\State MAX-HEAPIFY($A, i$)
\EndFor
\end{algorithmic}

The time required by MAX-HEAPIFY when called on a node of height $h$ is $O(h)$, so we can express the total cost of BUILD-MAX-HEAP as
\begin{eqnarray*}
  \sum_{h=0}^{\lfloor \lg n \rfloor} \lceil \frac {n}{2^{h+1}} \rceil O(h)
  &=& O(n \sum_{h=0}^{\lfloor \lg n \rfloor} \frac {h}{2^h}) \\
  &=& O(n \sum_{h=0}^{\infty} \frac {h}{2^h}) \\
  &=& O(2n) \\
  &=& O(n)
\end{eqnarray*}

\subsection{The heapsort algorithm}

HEAPSORT(A)
\begin{algorithmic}[1]
\State BUILD-MAX-HEAP(A)
\For {$i = A.length \text {{ \bf downto} } 2$}
	\State exchange $A[1]$ with $A[i]$
        \State $A.\text{heap-size} = A.\text{heap-size} - 1$
	\State MAX-HEAPIFY($A, 1$)
\EndFor
\end{algorithmic}

The HEAPSORT procedure takes time $O(n \lg n)$, since the call to BUILD-MAX-HEAP takes time O(n) and each of the $n-1$ calls to MAX-HEAPIFY takes time $O(\lg n)$.

\subsection{Priority queues}

HEAP-MAXIMUM(A)
\begin{algorithmic}[1]
\State \textbf {return} $A[1]$
\end{algorithmic}

HEAP-EXTRACT-MAX(A)
\begin{algorithmic}[1]
\If {$A.\text{heap-size} < 1$}
	\State \textbf {error} ``heap underflow''
\EndIf
\State $max$ = A[1]
\State $A[1] = A[A.\text{heap-size}]$
\State $A.\text{heap-size} = A.\text{heap-size} - 1$
\State MAX-HEAPIFY($A,1$)
\State \textbf {return} $max$
\end{algorithmic}

The running time of HEAP-EXTRACT-MAX is $O(\lg n)$, since it performs only a constant amount of work on top of the $O(\lg n)$ time for MAX-HEAPIFY. \\

HEAP-INCREASE-KEY($A, i, key$)
\begin{algorithmic}[1]
\If {$key < A[i]$}
	\State \textbf {error} ``new key is smaller than current key''
\EndIf
\State $A[i] = key$
\While {$i > 1$ and $A[\text {PARENT}(i)] < A[i]$}
	\State exchange $A[i]$ with $A[\text {PARENT}(i)]$
        \State $i = \text {PARENT}(i)$
\EndWhile
\end{algorithmic}

The running time of HEAP-INCREASE-KEY on an $n-$element heap is $O(\lg n)$. \\

MAX-HEAP-INSERT($A, key$)
\begin{algorithmic}[1]
\State $A.\text{heap-size} = A.\text{heap-size} + 1$
\State $A[A.\text{heap-size}] = -\infty$
\State HEAP-INCREASE-KEY($A, A.\text{heap-size},key$)
\end{algorithmic}

The running time of MAX-HEAP-INSERT on an $n-$element heap is $O(\lg n)$. \\

In summary, a heap can support any priority-queue operation on a set of size $n$ in $O(\lg n)$ time.

\section{Quicksort}

\subsection{Description of quicksort}

QUICKSORT($A, p, r$)
\begin{algorithmic}[1]
\If {$p < r$}
	\State $q = \text {PARTITION}(A, p, r)$
        \State QUICKSORT($A, p, q-1$)
        \State QUICKSORT($A, q+1, r$)
\EndIf
\end{algorithmic}

To sort an array $A$, the initial call is QUICKSORT($A, 1, A.length$). \\

PARTITION($A, p, r$)
\begin{algorithmic}[1]
\State $x = A[r]$
\State $i = p - 1$
\For {$j = p \textbf { to } r-1$}
	\If {$A[j] \le x$}
        	\State $i = i + 1$
                \State exchange $A[i]$ with $A[j]$
        \EndIf
\EndFor
\State exchange $A[i+1]$ with $A[r]$
\State \textbf{return } $i+1$
\end{algorithmic}

\subsection{Performance of quicksort}

\begin{itemize}
  \item \textbf{Worst-case partitioning}
    \begin{eqnarray*}
      T(n)
      &=& T(n-1) + T(0) + \Theta(n) \\
      &=& T(n-1) + \Theta(n).
    \end{eqnarray*}
    We get $T(n) = \Theta (n^2)$. This occurs when the input array is already completely sorted.
  \item \textbf {Best-case partitioning}
    \begin{eqnarray*}
      T(n)
      &=& 2T(n/2) + \Theta(n)
    \end{eqnarray*}
    We get $T(n) = \Theta (n \lg n)$.
  \item \textbf {Balanced partitioning}
    \begin{eqnarray*}
      T(n)
      &=& T(9n/10) + T(n/10) + cn
    \end{eqnarray*}    
    We get $T(n) = \Theta (n \lg n)$.
\end{itemize}

\subsection{A randomized version of quicksort}

RANDOMIZED-PARTITION($A, p, r$)
\begin{algorithmic}[1]
\State $i = \text{RANDOM}(p,r)$
\State exchange $A[r]$ with $A[i]$
\State \textbf{return } PARTITION($A, p, r$)
\end{algorithmic}

RANDOMIZED-QUICKSORT($A, p, r$)
\begin{algorithmic}[1]
\If {$p < r$}
	\State $q = \text {RANDOMIZED-PARTITION}(A, p, r)$
        \State RANDOMIZED-QUICKSORT($A, p, q-1$)
        \State RANDOMIZED-QUICKSORT($A, q+1, r$)
\EndIf
\end{algorithmic}

The expected running time of RANDOMIZED-QUICKSORT is $O(n \lg n)$.

\section{Sorting in Linear Time}

\subsection{Lower bounds for sorting}

\begin{theorem}
  Any comparison sort algorithm requires $\Omega (n \lg n)$ comparisons in the worst case.
\end{theorem}

\begin{corollary}
  Heapsort and merge sort are asymptotically optimal comparison sorts.
\end{corollary}

\subsection{Counting sort}

COUNTING-SORT($A, B, k$)
\begin{algorithmic}[1]
\State let $C[0..k]$ be a new array
\For {$i = 0 \textbf { to } k$}
	\State $C[i] = 0$
\EndFor
\For {$j = 1 \textbf { to } A.length$}
	\State $C[A[j]] = C[A[j]] + 1$
\EndFor
\State // $C[i]$ now contains the number of elements equal to $i$.
\For {i = 1 \textbf { to } k}
	\State $C[i] = C[i] + C[i-1]$
\EndFor
\State // $C[i]$ now contains the number of elements less than or equal to $i$.
\For {j = A.length \textbf { downto } 1}
	\State $B[C[A[j]]] = A[j]$
        \State $C[A[j]] = C[A[j]] - 1$
\EndFor
\end{algorithmic}

\subsection{Radix sort}

RADIX-SORT($A, d$)
\begin{algorithmic}[1]
\For {$i = 1 \textbf { to } d$}
	\State use a stable sort to sort array $A$ on digit $i$
\EndFor
\end{algorithmic}

\begin{lemma}
  Given $n$ $d-$digit numbers in which each digit can take on up to $k$ possible values, RADIX-SORT correctly sorts these numbers in $\Theta(d(n+k))$ time if the stable sort it uses takes $\Theta (n+k)$.
\end{lemma}

When $d$ is constant and $k = O(n)$, we can make radix sort run in linear time. More generally, we have some flexibility in how to break each key into digits.

\begin{lemma}
  Given $n$ $b$-bit numbers and any positive integer $r \le b$, RADIX-SORT correctly sorts these numbers in $\Theta ((b/r)(n + 2^r))$ time.
\end{lemma}

For a value $r \le b$, we view each key as having $d = \lceil b/r \rceil$ digits of $r$ bits each. Each digit is an integer in the range $0$ to $2^r-1$, so that we can use counting sort with $k = 2^r -1$.

\subsection{Bucket sort}

\textbf {Bucket sort} assumes that the input is drawn from a uniform distribution and has an average-case running time of $O(n)$. Bucket sort divides the interval $[0,1)$ into $n$ equal-sized subintervals, or \textbf {buckets}, and then distributes the $n$ input numbers into the buckets. \\

Our code for bucket sort assumes that the input is an $n$-element array $A$ and that each element $A[i]$ in the array satisfies $0 \le A[i] < 1$. The code requires an auxiliary array $B[0...n-1]$ of linked lists (buckets) and assumes that there is a mechanism for maintaining such lists. \\

BUCKET-SORT($A$)
\begin{algorithmic}[1]
\State let $B[0..n-1]$ be a new array
\State $n = A.length$
\For {$i = 0 \textbf { to } n-1$}
	\State make $B[i]$ an empty list
\EndFor
\For {$i = 1 \textbf { to } n$}
	\State insert $A[i]$ into list $B[\lfloor nA[i] \rfloor ]$
\EndFor
\For {$i = 0 \textbf { to } n-1$}
	\State sort list $B[i]$ with insertion sort
\EndFor
\State concatenate the lists $B[0], B[1], \dots, B[n-1]$ together in order
\end{algorithmic}

Since insertion sort runs in quadratic time, the running time of bucket sort is
\begin{equation*}
  T(n) = \Theta(n) + \sum_{i=0}^{n-1} O(n_i^2).
\end{equation*}
We now analyze the average-case running time of bucket sort, by computing the expected value of the running time, where we take the expectation over the input distribution. We have
\begin{eqnarray*}
  E[T(n)]
  &=& E \left[ \Theta (n) + \sum_{i=0}^{n-1} O(n_i)^2 \right] \\
  &=& \Theta (n) + \sum_{i=0}^{n-1} E \left[ O(n_i)^2 \right] \\
  &=& \Theta (n) + \sum_{i=0}^{n-1} E \left( O[n_i]^2 \right) \\
  &=& \Theta (n) + \sum_{i=0}^{n-1} (2 - 1/n) \\
  &=& \Theta (n)
\end{eqnarray*}

\section{Medians and Order Statistics}

The $i$th \textbf {order statistic} of a set of $n$ elements is the $i$th smallest element.

\subsection{Selection in expected linear time}

RANDOMIZED-SELECT($A, p, r, i$)
\begin{algorithmic}[1]
\If {$p == r$}
	\State \textbf {return } $A[p]$
\EndIf
\State $q = \text{RANDOMIZED-PARTITION}(A, p, r)$
\State $k = q - p + 1$
\If {$i == k$}
\Comment the pivot value is the answer
	\State \textbf {return} $A[q]$
\ElsIf {$i < k$}
	\State \textbf {return} RANDOMIZED-SELECT($A, p, q-1, i$)
\Else
	\State \textbf {return} RANDOMIZED-SELECT($A, q+1, r, i-k$)
\EndIf
\end{algorithmic}

\subsection{Selection in worst-case linear time}

Like RANDOMIZED-SELECT, the algorithm SELECT finds the desired element by recursively partitioning the input array. Here, however, we guarantee a good split upon partitioning the array. SELECT uses the deterministic partitioning algorithm PARTITION from quicksort, but modified to take the element to partition around as an input parameter. \\

The SELECT algorithm determines the $i$th smallest of an input array of $n>1$ distinct elements by executing the following steps. (If $n = 1$, then SELECT merely returns its only input value as the $i$th smallest.)

\begin{enumerate}
  \item Divide the $n$ elements of the input array into $\lfloor n/5 \rfloor$ groups of $5$ elements each and at most one group made up of remaining $n \mod 5$ elements.
  \item Find the median of each of the $\lceil n/5 \rceil$ groups by first insertion-sorting the elements of each group (of which there are at most $5$) and then picking the median from the sorted list of group elements.
  \item Use SELECT recursively to find the median $x$ of the $\lceil n/5 \rceil$ medians found in step $2$. (If there are an even number of medians, then by our convention, $x$ is the lower median.)
  \item Partition the input array around the median-of-medians $x$ using the modified version of PARTITION. Let $k$ be one more than the number of elements on low side of the partition, so that $x$ is the $k$th smallest element and there are $n-k$ elements on the high side of the partition.
  \item If $i = k$, then return $x$. Otherwise, use SELECT recursively to find the $i$th smallest element on the low side if $i < k$, or the $(i-k)$th smallest element on the high side if $i>k$.
\end{enumerate}

\section{Elementary Data Structure}

\begin{enumerate}
  \item Stacks and queues
  \item Linked lists
  \item Implementing pointers and objects
  \item Representing rooted trees
\end{enumerate}

\section{Hash Tables}

\subsection{Direct-address tables}

DIRECT-ADDRESS-SEARCH($T, k$)
\begin{algorithmic}[1]
\State \textbf {return} $T[k]$
\end{algorithmic}

DIRECT-ADDRESS-INSERT($T, x$)
\begin{algorithmic}[1]
\State $T[x.key] = x$
\end{algorithmic}

DIRECT-ADDRESS-DELETE($T, x$)
\begin{algorithmic}[1]
\State $T[x.key] = NIL$
\end{algorithmic}

Each of these operations takes only $O(1)$ time.

\subsection{Hash tables}

With hashing, an element with key $k$ is stored in slot $h(k)$; that is, we use a \textbf {hash function} $h$ to compute the slot from the key $k$. here, $h$ maps the universe $U$ of keys into the slots of a \textbf {hash table} $T[0..m-1]$:
\begin{equation*}
  h: U \rightarrow \{ 0,1, \dots, m-1 \},
\end{equation*}
where the size $m$ of the hash table is typically much less than $|U|$.

\subsubsection*{Collision resolution by chaining}

In chaining, we place all elements that hash to the same slot into the same linked list.

CHAINED-HASH-INSERT($T, x$)
\begin{algorithmic}[1]
\State insert $x$ at the head of list $T[h(x.key)]$
\end{algorithmic}

CHAINED-HASH-SEARCH($T, k$)
\begin{algorithmic}[1]
\State search for an element with key $k$ in list $T[h(k)]$
\end{algorithmic}

CHAINED-HASH-DELETE($T, x$)
\begin{algorithmic}[1]
\State delete $x$ from the list $T[h(x.key)]$
\end{algorithmic}

\subsubsection*{Analysis of hashing with chaining}

Given a hash table $T$ with $m$ slots that stores $n$ elements, we define the \textbf {load factor} $\alpha$ for $T$ as $n/m$, that is, the average number of elements stored in a chain. \\

The average-case performance of hashing depends on how well the hash function $h$ distributes the set of keys to be stored among the $m$ slots, on the average. Now we shall assume that any given element is equally likely to hash into any of the $m$ slots, independently of where any other element has hashed to. We call this the assumption of \textbf {simple uniform hashing}.

\begin{theorem}
  In a hash table in which collisions are resolved by chaining, an unsuccessful search takes average-case time $\Theta (1 + \alpha)$, under the assumption of simple uniform hashing.
\end{theorem}

\begin{theorem}
  In a hash table in which collisions are resolved by chaining, a successful search takes average-case time $\Theta (1 + \alpha)$, under the assumption of simple uniform hashing.
\end{theorem}

\subsection{Hash functions}

A good hash function satisfies (approximately) the assumption of simple uniform hashing: each key is equally likely to hash to any of the $m$ slots, independently of where any other key has hashed to. Unfortunately, we typically have no way to check this condition, since we rarely know the probability distribution from which the keys are drawn. Moreover, the keys might not be drawn independently.

\subsubsection*{Interpreting keys as natural numbers}

Most hash functions assume that the universe of keys is the set $\mathbb {N} = \{0,1,2,\dots \}$ of natural numbers.

\subsubsection{The division method}

In the \textbf {division method} for creating hash functions, we map a key $k$ into one of $m$ slots by taking the remainder of $k$ divided by $m$. That is, the hash function is
\begin{equation*}
  h(k) = k \mod m.
\end{equation*}
A prime not too close to an exact power of $2$ is often a good choice for $m$.

\subsubsection{The multiplication method}

The \textbf {multiplication method} for creating hash functions operates in two steps. First, we multiply the key $k$ by a constant $A$ in the range $0 < A < 1$ and extract the fractional part of $kA$. Then, we multiply this value by $m$ and take the floor o the result. In short, the hash function is
\begin{equation*}
  h(k) = \lfloor m (kA \mod 1) \rfloor,
\end{equation*}
where ``$kA \mod 1$'' means the fractional part of $kA$, that is, $kA - \lfloor kA \rfloor$. \\

An advantage of the multiplication method is that the value of m is not critical. We typically choose it to be a power of $2$ ($m = 2^p$ for some integer $p$), since we can then easily implement the function on most computers as follows. Suppose that the word size of the machine is $w$ bits and that $k$ fits into a single word. We restrict $A$ to be a fraction of the form $s/2^w$, where $s$ is an integer in the range $0 < s < 2^w$. We first multiply $k$ by the $w$-bit integer $s = A \cdot 2^w$. The result is a $2w$-bit value $r_12^w+r_0$, where $r_1$ is the high-order word of the product and $r_0$ is the low-order word of the product. The desired $p$-bit hash value consists of the $p$ most significant bits of $r_0$.

\subsubsection{Universal hashing}

If a malicious adversary chooses the keys to be hashed by some fixed hash function, then the adversary can choose $n$ keys that all hash to the same slot, yielding an average retrieval time of $\Theta(n)$. Any fixed hash function is vulnerable to such terrible worst-case behavior; the only effective way to improve the situation is to choose the hash function randomly in a way that is independent of the keys that are actually going to be stored. This approach, called \textbf {universal hashing}, can yield provably good performance on average, no matter which keys the adversary chooses. \\

In universal hashing, at the beginning of execution we select the hash function at random from a carefully designed class of functions. Because we randomly select the hash function, the algorithm can behave differently on each execution, even for the same input, guaranteeing good average-case performance for any input. \\

Let $\mathcal {H}$ be a finite collection of hash functions that map a given universe $U$ of keys into the range $\{ 0,1,\dots,m-1 \}$. Such a collection is said to be \textbf {universal} if for each pair of distinct keys $k, l \in U$, the number of hash functions $h \in \mathcal {H}$ for which $h(k) = h(l)$ is at most $|\mathcal {H}| / m$. In other words, with a hash function randomly chosen from $\mathcal {H}$, the chance of a collision between distinct keys $k$ and $l$ is no more than the chance $1/m$ of a collision if $h(k)$ and $h(l)$ were randomly and independently chosen from the set $\{ 0,1,\dots, m-1\}$. \\

Recall that $n_i$ denotes the length of list $T[i]$.

\begin{theorem}
  Suppose that a hash function $h$ is chosen randomly from a universal collection of hash functions and has been used to hash $n$ keys into a table $T$ of size $m$, using chaining to resolve collisions. If key $k$ is not in the table, then the expected length $E[n_{h(k)}]$ of the list that key $k$ hashes to is at most the load factor $\alpha = m/m$. If key $k$ is in the table, then the expected length $E[n_{h(k)}]$ of the list containing key $k$ is at most $1 + \alpha$.
\end{theorem}

\begin{corollary}
  Using universal hashing and collision resolution by chaining in an initially empty table with $m$ slots, it takes expected time $\Theta (n)$ to handle any sequence of $n$ INSERT, SEARCH, and DELETE operations containing $O(m)$ INSERT operations.
\end{corollary}

\subsection{Open addressing}

In \textbf {open addressing}, all elements occupy the hash table itself. That is, each table entry contains either an element of the dynamic set or NIL. \\

To perform insertion using open addressing, we successively examine, or \textbf {probe}, the hash table until we find an empty slot in which to put the key. To determine which slot to probe, we extend the hash function to include the probe number (starting from 0) as a second input. Thus, the hash function becomes
\begin{equation*}
  h: U \times \{ 0,1,\dots,m-1 \} \rightarrow \{ 0,1,\dots,m-1 \}.
\end{equation*}
With open addressing, we require that for every key $k$, the \textbf {probe sequence}

\begin{equation*}
  <h(k,0), h(k,1), \dots, h(k, m-1)>
\end{equation*}
be a permutation of $<0,1,\dots,m-1>$, so that every hash-table position is eventually considered as a slot for a new key as the table fills up. \\

HASH-INSERT($T, k$)
\begin{algorithmic}[1]
\State i = 0
\Repeat
	\State $j = h(k,i)$
        \If {$T[j] == \text {NIL}$}
        	\State {$T[j] = k$}
                \State \textbf {return} $j$
        \Else
        	\State $i = i + 1$
        \EndIf
\Until {$i == m$}
\State \textbf {error} ``hash table overflow''
\end{algorithmic}

HASH-SEARCH($T, k$)
\begin{algorithmic}[1]
\State i = 0
\Repeat
	\State $j = h(k,i)$
        \If {$T[j] == k$}
                \State \textbf {return} $j$
        \EndIf
        \State $i = i + 1$
\Until {$T[j] == \text {NIL or } i == m$}
\State \textbf {return} NIL
\end{algorithmic}

\subsubsection*{Linear probing}

Given an ordinary hash function $h': U \rightarrow \{ 0,1,\dots,m-1 \}$, which we refer to as an \textbf {auxiliary hash function}, the method of \textbf {linear probing} uses the hash function
\begin{equation*}
  h(k,i) = (h'(k) + i) \mod m
\end{equation*}
for $i = 0,1,\dots,m-1$. Linear probing is easy to implement, but it suffers from a problem known as \textbf {primary clustering}.

\subsubsection*{Quadratic probing}

\textbf {Quadratic probing} uses a hash function of the form
\begin{equation*}
  h(k,i) = (h'(k) + c_1 i + c_2 i^2) \mod m,
\end{equation*}
where $h'$ is an auxiliary hash function, $c_1$ and $c_2$ are positive auxiliary constants and $i = 0,1,\dots,m-1$. This methods works much better than linear probing, but to make full use of the hash table, the values of $c_1, c_2$ and $m$ are constrained. Also, if two keys have the same initial probe position, then their probe sequences are the same. This property leads to a milder from of clustering, called \textbf {secondary clustering}.

\subsubsection*{Double hashing}

Double hashing offers one of the best methods available for open addressing because the permutations produced have many of the characteristics of randomly chosen permutations. \textbf {Double hashing} uses a hash function of the form
\begin{equation*}
  h(k,i) = (h_1(k) + ih_2(k)) \mod m,
\end{equation*}
where both $h_1$ and $h_2$ are auxiliary hash functions.

\subsubsection*{Analysis of open-address hashing}

\begin{theorem}
  Given an open-address hash table with load factor $\alpha =n/m < 1$, the expected number of probes in an unsuccessful search is at most $1/(1-\alpha)$, assuming uniform hashing.
\end{theorem}

\begin{corollary}
  Inserting an element into an open-address hash table with load factor $\alpha$ requires at most $1/(1-\alpha)$ probes on average, assuming uniform hashing.
\end{corollary}

\begin{theorem}
  Given an open-address hash table with load factor $\alpha < 1$, the expected number of probes in a successful search is at most
  \begin{equation*}
    \frac {1}{\alpha} \ln \frac {1}{1 - \alpha},
  \end{equation*}
  assuming uniform hashing and assuming that each key in the table is equally likely to be searched for.
\end{theorem}

\subsection{Perfect hashing}

Hashing can provide excellent worst-case performance when the set of keys is \textbf {static}: once the keys are stored in the table, the set of keys never changes. We call a hashing technique \textbf {perfect hashing} if $O(1)$ memory accesses are required to perform a search in the worst case. \\

To create a perfect hashing scheme, we use two levels of hashing, with universal hashing at each level.

\section{Binary Search Trees}

\subsection{What is a binary search tree}

The keys in a binary search tree are always stored in such a way as to satisfy the \textbf {binary-search-tree property}:
\begin{quote}
  Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree of $x$, then $y.key \le x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \ge x.key$.
\end{quote}
\textbf {Inorder tree walk} prints the key of the root of a subtree between printing the values in its left subtree and printing those in its right subtree. Similarly, a \textbf {preorder tree walk} prints the root before the values in either subtree, and a \textbf {postorder tree walk} prints the root after the values in its subtrees. \\

INORDER-TREE-WALK($x$)
\begin{algorithmic}[1]
\If {$x \neq \text {NIL}$}
\State INORDER-TREE-WALK($x.\text{left}$)
\State print $x.\text{key}$
\State INORDER-TREE-WALK($x.\text{right}$)
\EndIf
\end{algorithmic}

\subsection{Querying a binary search tree}

\subsubsection*{Searching}

TREE-SEARCH($x, k$)
\begin{algorithmic}[1]
\If {$x == \text {NIL}$ or $k == x.\text{key}$}
	\State \textbf {return} $x$
\EndIf
\If {$k < x.\text{key}$}
	\State \textbf {return} TREE-SEARCH($x.\text{left}, k$)
\Else
	\State \textbf {return} TREE-SEARCH($x.\text{right}, k$)
\EndIf
\end{algorithmic}

ITERATIVE-TREE-SEARCH($x, k$)
\begin{algorithmic}[1]
\While {$x \neq \text {NIL}$ and $x \neq x.\text{key}$}
	\If {$k < x.\text{key}$}
		\State $x = x.\text{left}$
        \Else
		\State $x = x.\text{right}$
	\EndIf
\EndWhile
\State \textbf {return} $x$
\end{algorithmic}

TREE-MINIMUM($x$)
\begin{algorithmic}[1]
\While {$x.\text{left} \neq \text {NIL}$}
	\State $x = x.\text{left}$
\EndWhile
\State \textbf {return} $x$
\end{algorithmic}

TREE-MAXIMUM($x$)
\begin{algorithmic}[1]
\While {$x.\text{right} \neq \text {NIL}$}
	\State $x = x.\text{right}$
\EndWhile
\State \textbf {return} $x$
\end{algorithmic}

TREE-SUCCESSOR($x$)
\begin{algorithmic}[1]
\If {$x.\text{right} \neq \text {NIL}$}
	\State \textbf {return} TREE-MINIMUM($x.\text{right}$)
\EndIf
\State $y = x.p$
\While {$y \neq \text {NIL}$ and $x == y.\text{right}$}
	\State $x = y$
        \State $y = y.p$
\EndWhile
\State \textbf {return} $y$
\end{algorithmic}

\begin{theorem}
  We can implement the dynamic-set operations SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR so that each one runs in $O(h)$ time on a binary search tree of height $h$.
\end{theorem}

\subsection{Insertion and deletion}

\subsubsection*{Insertion}

TREE-INSERT($T,z$)
\begin{algorithmic}[1]
\State $y = \text {NIL}$
\State $x = T.\text{root}$
\While {$x \neq NIL$}
	\State $y = x$
        \If {$z.\text{key} < x.\text{key}$}
        	\State $x = x.\text{left}$
        \Else
        	\State $x = x.\text{right}$
        \EndIf
\EndWhile
\State $z.p = y$
\If {$y == NIL$}
	\State $T.\text{root} = z$
        \Comment tree $T$ was empty
\ElsIf {$z.\text{key} < y.\text{key}$}
	\State $y.\text{left} = z$
\Else
	\State $y.\text{right} = z$
\EndIf
\end{algorithmic}

\subsubsection{Deletion}

TRANSPLANT replaces one subtree ($u$) as a child of its parent ($u.p$) with another subtree ($v$). \\

TRANSPLANT($T,u,v$)
\begin{algorithmic}[1]
\If {$u.p == \text {NIL}$}
	\State $T.\text{root} = v$
\ElsIf {$u == u.p.\text{left}$}
	\State $u.p.\text{left} = v$
\Else
	\State $u.p.\text{right} = v$
\EndIf
\If {$v \neq \text {NIL}$}
	\State $v.p = u.p$
\EndIf
\end{algorithmic}

Deletion: Replace $z$ with its successor $y$. And $y$ does not have a left subtree. \\

TREE-DELETE($T,z$)
\begin{algorithmic}[1]
\If {$z.\text{left} == \text {NIL}$}
	\State TRANSPLANT($T,z,z.\text{right}$)
\ElsIf {$z.\text{right} == \text {NIL}$}
	\State TRANSPLANT($T,z,z.\text{left}$)
\Else
	\State $y = \text {TREE-MINIMUM}(z.\text{right})$
        \If {$y.p \neq z$}
		\State TRANSPLANT($T,y,y.\text{right}$)
                \State $y.\text{right} = z.\text{right}$
                \State $y.\text{right}.p = y$
        \EndIf
        \State TRANSPLANT($T,z,y$)
        \State $y.\text{left} = z.\text{left}$
        \State $y.\text{left}.p = y$
\EndIf
\end{algorithmic}

\begin{theorem}
  We can implement the dynamic-set operations INSERT and DELETE so that each one runs in $O(h)$ time on a binary search tree of height $h$.
\end{theorem}

\subsection{Randomly built binary search tree}

A \textbf {randomly built binary search tree} on $n$ keys as one that arises from inserting the keys in random order into an initially empty tree, where each of the $n!$ permutations of the input keys is equally likely. 

\begin{theorem}
  The expected height of a randomly built binary search tree on $n$ distinct keys is $O(\lg n)$.
\end{theorem}

\section{Red-Black Tree}

Red-black trees are one of many search-tree schemes that are ``balanced'' in order to guarantee that basic dynamic-set operations take $O(\lg n)$ time in the worst case.

\subsection{Properties of red-black trees}

A \textbf {red-black tree} is a binary search tree with one extra bit of storage per node: its \textbf {color}, which can be either RED or BLACK. By constraining the node colors on any simple path from the root to a leaf, red-black trees ensure that no such path is more than twice as long as any other, so that the tree is approximately \textbf {balanced}. \\

A red-black tree is a binary tree that satisfies the following \textbf {red-black properties}:
\begin{enumerate}
  \item Every node is either red or black.
  \item The root is black.
  \item Every leaf (NIL) is black.
  \item Of a node is red, then both its children are black.
  \item For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.
\end{enumerate}

We call the number of black nodes on any simple path from, but not including, a node $x$ down to a leaf the \textbf {black-height} of the node, denoted $bh(x)$. We define the black-height of a red-black tree to be the black-height of its root.

\begin{lemma}
  A red-black tree with $n$ internal nodes has height at most $2 \lg (n+1)$.
\end{lemma}

As an immediate consequence of this lemma, we can implement the dynamic-set operations SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR in $O(\lg n)$ time on red-black tree.

\subsection{Rotations}

We change the pointer structure through \textbf {rotation}, which is a local operation in a search tree that preserves the binary-search-tree property. \\

The pseudocode for LEFT-ROTATE assumes that $x.right \neq T.nil$ and that root's parent is $T.nil$. \\

LEFT-ROTATE($T,x$)
\begin{algorithmic}[1]
\State $y = x.\text{right}$
\Comment set $y$
\State $x.\text{right} = y.\text{left}$
\Comment turn $y$'s left subtree into $x$'s right subtree
\If {$y.\text{left} \neq T.\text{nil}$}
	\State $y.\text{left}.p = x$
\EndIf
\State $y.p = x.p$
\Comment link $x$'s parent to $y$
\If {$x.p == T.ni$}
	\State $T.\text{root} = y$
\ElsIf {$x == x.p.\text{left}$}
	\State $x.p.left = y$
\Else
	\State $x.p.right = y$
\EndIf
\State $y.left = x$
\Comment put $x$ on $y$'s left
\State $x.p = y$
\end{algorithmic}

Both LEFT-ROTATE and RIGHT-ROTATE run in $O(1)$ time. Only pointers are changed by a rotation; all other attributes in a node remain the same.

\subsection{Insertion}

RB-INSERT($T,z$)
\begin{algorithmic}[1]
\State $y = T.nil$
\State $x = T.root$
\While {$x \neq T.nil$}
	\State $y = x$
        \If {$z.key < x.key$}
        	\State $x = x.left$
        \Else
        	\State $x = x.right$
        \EndIf
\EndWhile
\State $z.p = y$
\If {$y == T.nil$}
	\State $T.root = z$
\ElsIf {$z.key < y.key$}
	\State $y.left = z$
\Else
	\State $y.right = z$
\EndIf
\State $z.left = T.nil$
\State $z.right = T.nil$
\State $z.color = RED$
\State RB-INSERT-FIXUP($T,z$)
\end{algorithmic}

RB-INSERT-FIXUP($T,z$)
\begin{algorithmic}[1]
\While {$z.p.color == RED$}
        \If {$z.p == z.p.p.left$}
        	\State $y = z.p.p.right$
                \If $y.color == RED$
                	\State $z.p.color = BLACK$
                        \State $y.color = BLACK$
                        \State $z.p.p.color = RED$
                        \State $z = z.p.p$
                \Else
                	\If {$z == z.p.right$}
                		\State $z = z.p$
                        	\State LEFT-ROTATE($T,z$)
                        \EndIf
                        \State $z.p.color = BLACK$
                        \State $z.p.p.color = RED$
                        \State RIGHT-ROTATE($T,z.p.p$)
                \EndIf
        \Else
        	\State (same as \textbf {then} clause with ``right'' and ``left'' exchanged)
        \EndIf
\EndWhile
\State $T.root.color = BLACK$
\end{algorithmic}

\section{Augmenting Data Structures}

\subsection{Dynamic order statistics}

Recall that the $i$th order statistic of a set of $n$ elements, where $i \in \{ 1,2,\dots,n \}$, is simply the element in the set with the $i$th smallest key. The \textbf{rank} of an element is its position in the linear order of the set. \\

An \textbf{order-statistic tree} T is simply a red-black tree with additional information sorted in each node. Besides the usual red-black tree attributes $x.key$, $x.color$, $x.p$, $x.left$ and $x.right$ in a node x, we have another attribute, $x.size$. This attribute contains the number of (internal) nodes in the subtree rooted at $x$ (including $x$ itself), that is, the size of the subtree. \\

\subsubsection*{Retrieving an element with a given rank}

OS-SELECT($x,i$)
\begin{algorithmic}[1]
\State $r = x.left.size + 1$
\If {$i == r$}
	\State \textbf{return} $x$
\ElsIf {$i < r$}
	\State \textbf{return} OS-SELECT($x.left, i$)
\Else
	\State \textbf{return} OS-SELECT($x.right, i-r$)
\EndIf
\end{algorithmic}

The running time o $OS-SELECT$ is $O(\lg n)$ for a dynamic set of $n$ elements.

\subsection*{Determining the rank of an element}

OS-RANK($T, x$)
\begin{algorithmic}[1]
\State $r = x.left.size + 1$
\State $y = x$
\While {$y \neq T.root$}
	\If {$y == y.p.right$}
        	\State $r = r + y.p.left.size + 1$
        \EndIf
        \State $y = y.p$
\EndWhile
\State \textbf{return} $r$
\end{algorithmic}

The running time of OS-RANK is at worst proportional to the height of the tree: $O(\lg n)$ on an $n$-node order-statistic tree.








\end{document}

