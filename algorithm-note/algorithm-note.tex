\documentclass[12pt]{article}
\parindent=.25in

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{440pt}
\setlength{\topmargin}{0in}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Introduction to Algorithms Note}
\author{Mengqi Zong $<mz2326@columbia.edu>$}

\begin{document}

\maketitle

\setlength{\parindent}{0in}

\section{Getting Started}

\subsection{Insertion sort}

INSERTION-SORT(A)
\begin{algorithmic}[1]
\For {$j = 2 \textbf { to } A.legnth$}
	\State $key = A[j]$
        \State // Insert $A[j]$ into the sorted sequence $A[1..j-1]$.
        \State $i = j - 1$
        \While {$i > 0$ and $A[i] > key$}
        	\State $A[i+1] = A[i]$
                \State $i = i - 1$
        \EndWhile
        \State $A[i+1] = key$
\EndFor
\end{algorithmic}

\subsection{Merge sort}

MERGE($A, p, q, r$)
\begin{algorithmic}[1]
\State $n_1 = q - p + 1$
\State $n_2 = r - q$
\State let $L[1..n_1+1]$ and $R[1..n_2+1]$ be new arrays
\For {$i = 1 \textbf { to } n_1$}
	\State $L[i] = A[p+i-1]$
\EndFor
\For {$j = 1 \textbf { to } n_2$}
	\State $R[j] = A[q+j]$
\EndFor
\State $L[n_1 + 1] = \infty$
\State $R[n_2 + 1] = \infty$
\State $i = 1$
\State $j = 1$
\For {$k = p \textbf { to } r$}
	\If {$L[i] \le R[j]$}
        	\State $A[k] = L[i]$
                \State $i = i + 1$
        \Else
        	\State $A[k] = R[j]$
        	\State $j = j + 1$
        \EndIf
\EndFor
\end{algorithmic}

MERGE-SORT($A, p, r$)
\begin{algorithmic}[1]
\If {$p < r$}
	\State $q = \lfloor (p+r)/2 \rfloor$
        \State MERGE-SORT($A, p, q$)
        \State MERGE-SORT($A, q+1, r$)
        \State MERGE($A, p, q, r$)
\EndIf
\end{algorithmic}

\section{Growth of Functions}

\subsection{Asymptotic notation}

\begin{eqnarray*}
  \Theta (g(n)) = \{ f(n): && \text{there exist positive constants
    $c_1$, $c_2$, and $n_0$ such that} \\
  && 0 \le c_1 g(n) \le f(n) \text{ for all } n \ge n_0 \} \\
  O(g(n)) = \{ f(n): && \text{there exist positive constants
    $c$ and $n_0$ such that} \\
  && 0 \le f(n) \le cg(n) \text{ for all } n \ge n_0 \} \\
  \Omega (g(n)) = \{ f(n): && \text{there exist positive constants
    $c$ and $n_0$ such that} \\
  && 0 \le cg(n) \le f(n) \text{ for all } n \ge n_0 \}
\end{eqnarray*}

\subsection{Common functions}

\begin{eqnarray*}
  e^x &=& \sum_0^{\infty} \frac {x^i}{i!} \\
  e^x &=& \lim_{n \rightarrow \infty} (1 + \frac {x}{n})^n \\
  n!  &=& \sqrt {2 \pi n} \left(\frac {n}{e} \right)^n 
          \left( 1 + \Theta \left( \frac {1}{n} \right) \right)
\end{eqnarray*}

\section{Divide-and-Conquer}

\subsection{The master method}

Let $a \ge 1$ and $b \ge 1$ be constants, let $f(n)$ be a function, and let $T(n)$ be defined on the nonnegative integers by recurrence
\begin{equation*}
  T(n) = aT(n/b) + f(n),
\end{equation*}
where we interpret $n/b$ to mean either $\lfloor n/b \rfloor$ or $\lceil n/b \rceil$. Then $T(n)$ can be bounded asymptotically as follows.
\begin{enumerate}
  \item If $f(n) = O(n^{\log_b^a - \epsilon})$ for some constant $\epsilon > 0$, then $T(n) = \Theta (n^{\log_b a})$.
  \item If $f(n) = O(n^{\log_b^a})$, then $T(n) = \Theta (n^{\log_b a} \lg n)$.
  \item If $f(n) = O(n^{\log_b^a + \epsilon})$ for some constant $\epsilon > 0$, and if $af(n/b) \le cf(n)$ for some constant $c < 1$ and all sufficiently large $n$, then $T(n) = \Theta \left( f(n) \right)$.
\end{enumerate}

\section{Probabilistic Analysis and Randomized Algorithms}

left blank


\section{Heapsort}

Like merge sort, but unlike insertion sort, heapsort's running time is $O(n \lg n)$. Like insertion sort, but unlike merge sort, heapsort sorts in place: only a constant number of array elements are stored outside the input array at any time.

\subsection{Heaps}

PARENT($i$)
\begin{algorithmic}[1]
\State \textbf {return} $\lfloor i/2 \rfloor$
\end{algorithmic}

LEFT($i$)
\begin{algorithmic}[1]
\State \textbf {return} $2i$
\end{algorithmic}

RIGHT($i$)
\begin{algorithmic}[1]
\State \textbf {return} $2i+1$
\end{algorithmic}

There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds, the values in the nodes satisfy a {\bf heap property}, the specifics of which depends on the kind of heap. In a {\bf max-heap}, the {\bf max-heap perperty} is that for every node $i$ other than the root,
\begin{equation*}
  A[\text{PARENT}(i)] \ge A[i],
\end{equation*}
that is, the value of a node is at most the value of its parent.

Viewing a heap as a tree, we define the {\bf height} of a node in a heap to be the number of edges on the longest simple downward path from the node to a leaf, and we define the height of the heap to be the height of its root.

\subsection{Maintaining the heap property}

 MAX-HEAPIFY($A,i$)
\begin{algorithmic}[1]
\State $l = \text {LEFT}(i)$
\State $r = \text {RIGHTT}(i)$
\If {$l \le A.heap-size$ and $A[l] > A[i]$}
	\State $largest = 1$
\Else
	\State $largest = i$
\EndIf
\If {$r \le A.heap-size$ and $A[r] > A[largest]$}
	\State $largest = r$
\EndIf
\If {$largest \neq i$}
	\State exchange $A[i]$ with $A[largest]$
        \State MAX-HEAPIFY($A, largest$)
\EndIf
\end{algorithmic}

We can describe the running time of MAX-HEAPIFY by recurrence
\begin{equation*}
  T(n) \le T(2n/3) + \Theta(1).
\end{equation*}
The solution to this recurrence is $T(n) = O(\lg n)$. Alternatively, we can characterize the running time of MAX-HEAPIFY on a node of height $h$ as $O(h)$.

\subsection{Building a heap}

BUILD-MAX-HEAP(A)
\begin{algorithmic}[1]
\State $A.heap-size = A.length$
\For {$i = \lfloor A.length/2 \rfloor \text {{ \bf downto} } 1$}
	\State MAX-HEAPIFY($A, i$)
\EndFor
\end{algorithmic}

The time required by MAX-HEAPIFY when called on a node of height $h$ is $O(h)$, so we can express the total cost of BUILD-MAX-HEAP as
\begin{eqnarray*}
  \sum_{h=0}^{\lfloor \lg n \rfloor} \lceil \frac {n}{2^{h+1}} \rceil O(h)
  &=& O(n \sum_{h=0}^{\lfloor \lg n \rfloor} \frac {h}{2^h}) \\
  &=& O(n \sum_{h=0}^{\infty} \frac {h}{2^h}) \\
  &=& O(2n) \\
  &=& O(n)
\end{eqnarray*}

\subsection{The heapsort algorithm}

HEAPSORT(A)
\begin{algorithmic}[1]
\State BUILD-MAX-HEAP(A)
\For {$i = A.length \text {{ \bf downto} } 2$}
	\State exchange $A[1]$ with $A[i]$
        \State $A.heap-size = A.heap-size - 1$
	\State MAX-HEAPIFY($A, 1$)
\EndFor
\end{algorithmic}

The HEAPSORT procedure takes time $O(n \lg n)$, since the call to BUILD-MAX-HEAP takes time O(n) and each of the $n-1$ calls to MAX-HEAPIFY takes time $O(\lg n)$.

\subsection{Priority queues}

HEAP-MAXIMUM(A)
\begin{algorithmic}[1]
\State \textbf {return} $A[1]$
\end{algorithmic}

HEAP-EXTRACT-MAX(A)
\begin{algorithmic}[1]
\If {$A.heap-size < 1$}
	\State \textbf {error} ``heap underflow''
\EndIf
\State $max$ = A[1]
\State $A[1] = A[A.heap-size]$
\State $A.heap-size = A.heap-size - 1$
\State MAX-HEAPIFY($A,1$)
\State \textbf {return} $max$
\end{algorithmic}

The running time of HEAP-EXTRACT-MAX is $O(\lg n)$, since it performs only a constant amount of work on top of the $O(\lg n)$ time for MAX-HEAPIFY. \\

HEAP-INCREASE-KEY($A, i, key$)
\begin{algorithmic}[1]
\If {$key < A[i]$}
	\State \textbf {error} ``new key is smaller than current key''
\EndIf
\State $A[i] = key$
\While {$i > 1$ and $A[\text {PARENT}(i)] < A[i]$}
	\State exchange $A[i]$ with $A[\text {PARENT}(i)]$
        \State $i = \text {PARENT}(i)$
\EndWhile
\end{algorithmic}

The running time of HEAP-INCREASE-KEY on an $n-$element heap is $O(\lg n)$. \\

MAX-HEAP-INSERT($A, key$)
\begin{algorithmic}[1]
\State $A.heap-size = A.heap-size + 1$
\State $A[A.heap-size] = -\infty$
\State HEAP-INCREASE-KEY($A, A.heap-size,key$)
\end{algorithmic}

The running time of MAX-HEAP-INSERT on an $n-$element heap is $O(\lg n)$. \\

In summary, a heap can support any priority-queue operation on a set of size $n$ in $O(\lg n)$ time.

\section{Quicksort}

\subsection{Description of quicksort}

QUICKSORT($A, p, r$)
\begin{algorithmic}[1]
\If {$p < r$}
	\State $q = \text {PARTITION}(A, p, r)$
        \State QUICKSORT($A, p, q-1$)
        \State QUICKSORT($A, q+1, r$)
\EndIf
\end{algorithmic}

To sort an array $A$, the initial call is QUICKSORT($A, 1, A.length$). \\

PARTITION($A, p, r$)
\begin{algorithmic}[1]
\State $x = A[r]$
\State $i = p - 1$
\For {$j = p \textbf { to } r-1$}
	\If {$A[j] \le x$}
        	\State $i = i + 1$
                \State exchange $A[i]$ with $A[j]$
        \EndIf
\EndFor
\State exchange $A[i+1]$ with $A[r]$
\State \textbf{return } $i+1$
\end{algorithmic}

\subsection{Performance of quicksort}

\begin{itemize}
  \item \textbf{Worst-case partitioning}
    \begin{eqnarray*}
      T(n)
      &=& T(n-1) + T(0) + \Theta(n) \\
      &=& T(n-1) + \Theta(n).
    \end{eqnarray*}
    We get $T(n) = \Theta (n^2)$. This occurs when the input array is already completely sorted.
  \item \textbf {Best-case partitioning}
    \begin{eqnarray*}
      T(n)
      &=& 2T(n/2) + \Theta(n)
    \end{eqnarray*}
    We get $T(n) = \Theta (n \lg n)$.
  \item \textbf {Balanced partitioning}
    \begin{eqnarray*}
      T(n)
      &=& T(9n/10) + T(n/10) + cn.
    \end{eqnarray*}    
    We get $T(n) = \Theta (n \lg n)$.
\end{itemize}

\subsection{A randomized version of quicksort}

RANDOMIZED-PARTITION($A, p, r$)
\begin{algorithmic}[1]
\State $i = \text{RANDOM}(p,r)$
\State exchange $A[r]$ with $A[i]$
\State \textbf{return } PARTITION($A, p, r$)
\end{algorithmic}

RANDOMIZED-QUICKSORT($A, p, r$)
\begin{algorithmic}[1]
\If {$p < r$}
	\State $q = \text {RANDOMIZED-PARTITION}(A, p, r)$
        \State RANDOMIZED-QUICKSORT($A, p, q-1$)
        \State RANDOMIZED-QUICKSORT($A, q+1, r$)
\EndIf
\end{algorithmic}

The expected running time of RANDOMIZED-QUICKSORT is $O(n \lg n)$.


\end{document}